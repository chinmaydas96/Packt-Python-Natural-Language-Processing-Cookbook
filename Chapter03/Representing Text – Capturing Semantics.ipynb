{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef1d27f",
   "metadata": {},
   "source": [
    "#### Related links\n",
    "\n",
    "* https://www.kaggle.com/currie32/project-gutenbergs-top-20-books\n",
    "* https://www.kaggle.com/PromptCloudHQ/imdb-data\n",
    "* https://www.yelp.com/dataset\n",
    "* http://vectors.nlpl.eu/repository/20/40.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c971e",
   "metadata": {},
   "source": [
    "# Putting documents into a bag of words\n",
    "\n",
    "* A bag of words is the simplest way of representing text. We treat our text as a collection of documents, where documents are anything from sentences to book chapters to whole books. \n",
    "\n",
    "\n",
    "* Since we usually compare different documents to each other or use them in a larger context of other documents, typically, we work with a collection of documents, not just a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895bc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb26d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from Chapter01.dividing_into_sentences import read_text_file,\\\n",
    "preprocess_text, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2356d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Read in the text file, preprocess the text, \n",
    "# and divide it into sentences:\n",
    "\n",
    "def get_sentences(filename):\n",
    "    sherlock_holmes_text = read_text_file(filename)\n",
    "    sherlock_holmes_text = preprocess_text(sherlock_holmes_text)\n",
    "    senteces = divide_into_sentences_nltk(sherlock_holmes_text)\n",
    "    return senteces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0d60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will return the vectorizer and final matrix\n",
    "\n",
    "def create_vectorizer(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return vectorizer, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abcea698",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('../Chapter01/sherlock_holmes_1.txt')\n",
    "vectorizer, X = create_vectorizer(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acfe66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 114)\t1\n",
      "  (0, 99)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 124)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 95)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 64)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 116)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 79)\t1\n",
      "  (1, 70)\t1\n",
      "  (2, 98)\t1\n",
      "  (2, 42)\t1\n",
      "  (2, 48)\t1\n",
      "  (2, 46)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 25)\t1\n",
      "  (2, 11)\t1\n",
      "  :\t:\n",
      "  (9, 57)\t1\n",
      "  (9, 15)\t1\n",
      "  (9, 67)\t1\n",
      "  (9, 21)\t1\n",
      "  (9, 107)\t1\n",
      "  (9, 103)\t1\n",
      "  (9, 71)\t1\n",
      "  (10, 114)\t1\n",
      "  (10, 124)\t2\n",
      "  (10, 44)\t1\n",
      "  (10, 11)\t3\n",
      "  (10, 109)\t1\n",
      "  (10, 76)\t1\n",
      "  (10, 119)\t2\n",
      "  (10, 108)\t1\n",
      "  (10, 53)\t1\n",
      "  (10, 4)\t1\n",
      "  (10, 77)\t1\n",
      "  (10, 16)\t1\n",
      "  (10, 127)\t1\n",
      "  (10, 110)\t1\n",
      "  (10, 56)\t1\n",
      "  (10, 24)\t1\n",
      "  (10, 89)\t1\n",
      "  (10, 61)\t1\n"
     ]
    }
   ],
   "source": [
    "# he resulting matrix is a scipy.sparse.csr.csr_matrix object, and the beginning of its printout looks like this:\n",
    "print(X)\n",
    "\n",
    "# first number document number, word number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dfbb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# It can also be turned into a numpy.matrixlib.defmatrix.matrix object, where each sentence is a vector. These sentence vectors can be used our machine learning algorithms later:\n",
    "denseX = X.todense()\n",
    "print(denseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14600729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(11, 128)\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(denseX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4ef3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'all', 'always', 'and', 'any', 'as', 'balanced', 'be', 'but', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'for', 'from', 'gibe', 'grit', 'has', 'have', 'he', 'heard', 'her', 'high', 'him', 'himself', 'his', 'holmes', 'in', 'instrument', 'into', 'introduce', 'intrusions', 'irene', 'is', 'it', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'might', 'mind', 'more', 'most', 'motives', 'name', 'nature', 'never', 'not', 'observer', 'observing', 'of', 'one', 'or', 'other', 'own', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'she', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'such', 'take', 'temperament', 'than', 'that', 'the', 'there', 'they', 'things', 'throw', 'to', 'trained', 'under', 'upon', 'veil', 'was', 'were', 'which', 'whole', 'with', 'woman', 'world', 'would', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a94689e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I had seen little of Holmes lately.\"\n",
    "new_sentece_vector = vectorizer.transform([new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c29c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 94)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentece_vector)\n",
    "print(new_sentece_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1165613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'observing'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0d8f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 114,\n",
       " 'sherlock': 99,\n",
       " 'holmes': 47,\n",
       " 'she': 98,\n",
       " 'is': 54,\n",
       " 'always': 10,\n",
       " '_the_': 0,\n",
       " 'woman': 124,\n",
       " 'have': 39,\n",
       " 'seldom': 95,\n",
       " 'heard': 41,\n",
       " 'him': 44,\n",
       " 'mention': 64,\n",
       " 'her': 42,\n",
       " 'under': 116,\n",
       " 'any': 12,\n",
       " 'other': 79,\n",
       " 'name': 70,\n",
       " 'in': 48,\n",
       " 'his': 46,\n",
       " 'eyes': 29,\n",
       " 'eclipses': 25,\n",
       " 'and': 11,\n",
       " 'predominates': 88,\n",
       " 'the': 109,\n",
       " 'whole': 122,\n",
       " 'of': 76,\n",
       " 'sex': 97,\n",
       " 'it': 55,\n",
       " 'was': 119,\n",
       " 'not': 73,\n",
       " 'that': 108,\n",
       " 'he': 40,\n",
       " 'felt': 32,\n",
       " 'emotion': 26,\n",
       " 'akin': 8,\n",
       " 'love': 58,\n",
       " 'for': 34,\n",
       " 'irene': 53,\n",
       " 'adler': 4,\n",
       " 'all': 9,\n",
       " 'emotions': 27,\n",
       " 'one': 77,\n",
       " 'particularly': 81,\n",
       " 'were': 120,\n",
       " 'abhorrent': 1,\n",
       " 'cold': 17,\n",
       " 'precise': 87,\n",
       " 'but': 16,\n",
       " 'admirably': 6,\n",
       " 'balanced': 14,\n",
       " 'mind': 66,\n",
       " 'take': 105,\n",
       " 'most': 68,\n",
       " 'perfect': 83,\n",
       " 'reasoning': 91,\n",
       " 'observing': 75,\n",
       " 'machine': 60,\n",
       " 'world': 125,\n",
       " 'has': 38,\n",
       " 'seen': 94,\n",
       " 'as': 13,\n",
       " 'lover': 59,\n",
       " 'would': 126,\n",
       " 'placed': 84,\n",
       " 'himself': 45,\n",
       " 'false': 31,\n",
       " 'position': 85,\n",
       " 'never': 72,\n",
       " 'spoke': 102,\n",
       " 'softer': 101,\n",
       " 'passions': 82,\n",
       " 'save': 93,\n",
       " 'with': 123,\n",
       " 'gibe': 36,\n",
       " 'sneer': 100,\n",
       " 'they': 111,\n",
       " 'admirable': 5,\n",
       " 'things': 112,\n",
       " 'observer': 74,\n",
       " 'excellent': 28,\n",
       " 'drawing': 23,\n",
       " 'veil': 118,\n",
       " 'from': 35,\n",
       " 'men': 62,\n",
       " 'motives': 69,\n",
       " 'actions': 2,\n",
       " 'trained': 115,\n",
       " 'reasoner': 90,\n",
       " 'admit': 7,\n",
       " 'such': 104,\n",
       " 'intrusions': 52,\n",
       " 'into': 50,\n",
       " 'own': 80,\n",
       " 'delicate': 19,\n",
       " 'finely': 33,\n",
       " 'adjusted': 3,\n",
       " 'temperament': 106,\n",
       " 'introduce': 51,\n",
       " 'distracting': 20,\n",
       " 'factor': 30,\n",
       " 'which': 121,\n",
       " 'might': 65,\n",
       " 'throw': 113,\n",
       " 'doubt': 22,\n",
       " 'upon': 117,\n",
       " 'mental': 63,\n",
       " 'results': 92,\n",
       " 'grit': 37,\n",
       " 'sensitive': 96,\n",
       " 'instrument': 49,\n",
       " 'or': 78,\n",
       " 'crack': 18,\n",
       " 'high': 43,\n",
       " 'power': 86,\n",
       " 'lenses': 57,\n",
       " 'be': 15,\n",
       " 'more': 67,\n",
       " 'disturbing': 21,\n",
       " 'than': 107,\n",
       " 'strong': 103,\n",
       " 'nature': 71,\n",
       " 'yet': 127,\n",
       " 'there': 110,\n",
       " 'late': 56,\n",
       " 'dubious': 24,\n",
       " 'questionable': 89,\n",
       " 'memory': 61}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89faddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizer with stopwords removal\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17d5084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f9ee3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "['_the_', 'abhorrent', 'actions', 'adjusted', 'adler', 'admirable', 'admirably', 'admit', 'akin', 'balanced', 'cold', 'crack', 'delicate', 'distracting', 'disturbing', 'doubt', 'drawing', 'dubious', 'eclipses', 'emotion', 'emotions', 'excellent', 'eyes', 'factor', 'false', 'felt', 'finely', 'gibe', 'grit', 'heard', 'high', 'holmes', 'instrument', 'introduce', 'intrusions', 'irene', 'late', 'lenses', 'love', 'lover', 'machine', 'memory', 'men', 'mental', 'mention', 'mind', 'motives', 'nature', 'observer', 'observing', 'particularly', 'passions', 'perfect', 'placed', 'position', 'power', 'precise', 'predominates', 'questionable', 'reasoner', 'reasoning', 'results', 'save', 'seen', 'seldom', 'sensitive', 'sex', 'sherlock', 'sneer', 'softer', 'spoke', 'strong', 'temperament', 'things', 'throw', 'trained', 'veil', 'woman', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d4ca2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf9fe8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'woman', 'late', 'irene', 'adler', 'dubious', 'questionable', 'memory']\n"
     ]
    }
   ],
   "source": [
    "new_sentence_vector = vectorizer.transform([new_sentence])\n",
    "analyse = vectorizer.build_analyzer()\n",
    "print(analyse(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eccb9fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 41)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 77)\t2\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d49f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any word is present more than 80%  on the document we will ignore\n",
    "vectorizer = CountVectorizer(max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53b2d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence_vector = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1af99603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd08fc",
   "metadata": {},
   "source": [
    "# Constructing the N-gram model\n",
    "\n",
    "* Representing a document as a bag of words is useful, but semantics is about more than just words in isolation. \n",
    "\n",
    "\n",
    "* To capture word combinations, an n-gram model is useful. Its vocabulary consists not just of words, but word sequences, or n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03f01a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from Chapter01.dividing_into_sentences import read_text_file, preprocess_text, divide_into_sentences_nltk\n",
    "from Chapter03.bag_of_words_1 import get_sentences, get_new_sentences_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1a5dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('../Chapter01/sherlock_holmes_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28da7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "421a8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 269)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 226)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 299)\t1\n",
      "  (0, 275)\t1\n",
      "  (0, 230)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 137)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 93)\t1\n",
      "  (1, 221)\t1\n",
      "  (1, 101)\t1\n",
      "  (1, 108)\t1\n",
      "  (1, 156)\t1\n",
      "  (1, 103)\t1\n",
      "  (1, 278)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 190)\t1\n",
      "  (1, 167)\t1\n",
      "  :\t:\n",
      "  (10, 307)\t1\n",
      "  (10, 261)\t1\n",
      "  (10, 141)\t1\n",
      "  (10, 60)\t1\n",
      "  (10, 210)\t1\n",
      "  (10, 151)\t1\n",
      "  (10, 30)\t1\n",
      "  (10, 308)\t1\n",
      "  (10, 262)\t1\n",
      "  (10, 285)\t1\n",
      "  (10, 45)\t1\n",
      "  (10, 187)\t1\n",
      "  (10, 300)\t1\n",
      "  (10, 271)\t1\n",
      "  (10, 109)\t1\n",
      "  (10, 251)\t1\n",
      "  (10, 301)\t1\n",
      "  (10, 288)\t1\n",
      "  (10, 253)\t1\n",
      "  (10, 142)\t1\n",
      "  (10, 8)\t1\n",
      "  (10, 180)\t1\n",
      "  (10, 61)\t1\n",
      "  (10, 27)\t1\n",
      "  (10, 211)\t1\n"
     ]
    }
   ],
   "source": [
    "X = bigram_vectorizer.fit_transform(sentences)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b152bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "denseX = X.todense()\n",
    "print(denseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "399e4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', '_the_ woman', 'abhorrent', 'abhorrent to', 'actions', 'adjusted', 'adjusted temperament', 'adler', 'adler of', 'admirable', 'admirable things', 'admirably', 'admirably balanced', 'admit', 'admit such', 'akin', 'akin to', 'all', 'all emotions', 'all his', 'always', 'always _the_', 'and', 'and actions', 'and finely', 'and observing', 'and predominates', 'and questionable', 'and sneer', 'and that', 'and yet', 'any', 'any emotion', 'any other', 'as', 'as his', 'as lover', 'balanced', 'balanced mind', 'be', 'be more', 'but', 'but admirably', 'but as', 'but for', 'but one', 'cold', 'cold precise', 'crack', 'crack in', 'delicate', 'delicate and', 'distracting', 'distracting factor', 'disturbing', 'disturbing than', 'doubt', 'doubt upon', 'drawing', 'drawing the', 'dubious', 'dubious and', 'eclipses', 'eclipses and', 'emotion', 'emotion akin', 'emotion in', 'emotions', 'emotions and', 'excellent', 'excellent for', 'eyes', 'eyes she', 'factor', 'factor which', 'false', 'false position', 'felt', 'felt any', 'finely', 'finely adjusted', 'for', 'for drawing', 'for irene', 'for the', 'from', 'from men', 'gibe', 'gibe and', 'grit', 'grit in', 'has', 'has seen', 'have', 'have placed', 'have seldom', 'he', 'he felt', 'he never', 'he was', 'he would', 'heard', 'heard him', 'her', 'her sex', 'her under', 'high', 'high power', 'him', 'him and', 'him mention', 'himself', 'himself in', 'his', 'his cold', 'his eyes', 'his mental', 'his own', 'holmes', 'holmes she', 'in', 'in false', 'in his', 'in nature', 'in one', 'in sensitive', 'instrument', 'instrument or', 'into', 'into his', 'introduce', 'introduce distracting', 'intrusions', 'intrusions into', 'irene', 'irene adler', 'is', 'is always', 'it', 'it the', 'it was', 'late', 'late irene', 'lenses', 'lenses would', 'love', 'love for', 'lover', 'lover he', 'machine', 'machine that', 'memory', 'men', 'men motives', 'mental', 'mental results', 'mention', 'mention her', 'might', 'might throw', 'mind', 'more', 'more disturbing', 'most', 'most perfect', 'motives', 'motives and', 'name', 'nature', 'nature such', 'never', 'never spoke', 'not', 'not be', 'not that', 'observer', 'observer excellent', 'observing', 'observing machine', 'of', 'of dubious', 'of her', 'of his', 'of the', 'one', 'one of', 'one particularly', 'one woman', 'or', 'or crack', 'other', 'other name', 'own', 'own delicate', 'own high', 'particularly', 'particularly were', 'passions', 'passions save', 'perfect', 'perfect reasoning', 'placed', 'placed himself', 'position', 'power', 'power lenses', 'precise', 'precise but', 'predominates', 'predominates the', 'questionable', 'questionable memory', 'reasoner', 'reasoner to', 'reasoning', 'reasoning and', 'results', 'save', 'save with', 'seen', 'seen but', 'seldom', 'seldom heard', 'sensitive', 'sensitive instrument', 'sex', 'she', 'she eclipses', 'she is', 'sherlock', 'sherlock holmes', 'sneer', 'softer', 'softer passions', 'spoke', 'spoke of', 'strong', 'strong emotion', 'such', 'such as', 'such intrusions', 'take', 'take it', 'temperament', 'temperament was', 'than', 'than strong', 'that', 'that he', 'that one', 'that the', 'that woman', 'the', 'the late', 'the most', 'the observer', 'the softer', 'the trained', 'the veil', 'the whole', 'the world', 'there', 'there was', 'they', 'they were', 'things', 'things for', 'throw', 'throw doubt', 'to', 'to admit', 'to him', 'to his', 'to introduce', 'to love', 'to sherlock', 'trained', 'trained reasoner', 'under', 'under any', 'upon', 'upon all', 'veil', 'veil from', 'was', 'was but', 'was not', 'was take', 'was the', 'was to', 'were', 'were abhorrent', 'were admirable', 'which', 'which might', 'whole', 'whole of', 'with', 'with gibe', 'woman', 'woman to', 'woman was', 'world', 'world has', 'would', 'would have', 'would not', 'yet', 'yet there']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52ee99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "print(len(bigram_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ced400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I had seen little of Holmes lately.\"\n",
    "new_sentece_vector = bigram_vectorizer.transform([new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cef755cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 114)\t1\n",
      "  (0, 99)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 124)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 95)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 64)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 116)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 79)\t1\n",
      "  (1, 70)\t1\n",
      "  (2, 98)\t1\n",
      "  (2, 42)\t1\n",
      "  (2, 48)\t1\n",
      "  (2, 46)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 25)\t1\n",
      "  (2, 11)\t1\n",
      "  :\t:\n",
      "  (9, 57)\t1\n",
      "  (9, 15)\t1\n",
      "  (9, 67)\t1\n",
      "  (9, 21)\t1\n",
      "  (9, 107)\t1\n",
      "  (9, 103)\t1\n",
      "  (9, 71)\t1\n",
      "  (10, 114)\t1\n",
      "  (10, 124)\t2\n",
      "  (10, 44)\t1\n",
      "  (10, 11)\t3\n",
      "  (10, 109)\t1\n",
      "  (10, 76)\t1\n",
      "  (10, 119)\t2\n",
      "  (10, 108)\t1\n",
      "  (10, 53)\t1\n",
      "  (10, 4)\t1\n",
      "  (10, 77)\t1\n",
      "  (10, 16)\t1\n",
      "  (10, 127)\t1\n",
      "  (10, 110)\t1\n",
      "  (10, 56)\t1\n",
      "  (10, 24)\t1\n",
      "  (10, 89)\t1\n",
      "  (10, 61)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector)\n",
    "print(new_sentece_vector.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2c843d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence1 = \" And yet there was but one woman to him, and that woman was the late Irene Adler, of dubious and questionable memory.\"\n",
    "new_sentence_vector1 = vectorizer.transform([new_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4aaecc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 94)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_sentence_vector1)\n",
    "print(new_sentence_vector1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6dbd4",
   "metadata": {},
   "source": [
    "# Representing texts with TF-IDF\n",
    "\n",
    "* TF-IDF stands for term frequency-inverse document frequency and gives more weight to words that are unique to a document than to words that are frequent,but repeated throughout most documents. \n",
    "\n",
    "\n",
    "* This allows us to give more weight to words uniquely characteristic to particular documents. \n",
    "\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting.\n",
    "\n",
    "\n",
    "* The TfidfVectorizer class allows for all the functionality of CountVectorizer, except that it uses the TF-IDF algorithm to count the words instead of direct counts. The other features of the class should be familiar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3556296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69c7e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59c01be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences('../Chapter01/sherlock_holmes_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fcecf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84088cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "645e280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentences):\n",
    "    tokens = nltk.word_tokenize(sentences)\n",
    "    filtered_token = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_token]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b757352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stop_words + [tokenize_and_stem(stop_word)[0] for stop_word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8c373b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.9, max_features = 200000,\n",
    "                                  min_df = 0.05, stop_words=all_stopwords,\n",
    "                                  use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                  ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "83b57363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", \"n't\"] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = tfidf_vectorizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b7cf877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 221)\t0.24956605786128022\n",
      "  (0, 187)\t0.2919708551400885\n",
      "  (0, 186)\t0.2919708551400885\n",
      "  (0, 185)\t0.2919708551400885\n",
      "  (0, 88)\t0.2919708551400885\n",
      "  (0, 87)\t0.2919708551400885\n",
      "  (0, 86)\t0.2919708551400885\n",
      "  (0, 25)\t0.2919708551400885\n",
      "  (0, 24)\t0.2919708551400885\n",
      "  (0, 23)\t0.2919708551400885\n",
      "  (0, 1)\t0.2919708551400885\n",
      "  (0, 0)\t0.2919708551400885\n",
      "  (1, 180)\t0.3333333333333333\n",
      "  (1, 179)\t0.3333333333333333\n",
      "  (1, 178)\t0.3333333333333333\n",
      "  (1, 127)\t0.3333333333333333\n",
      "  (1, 123)\t0.3333333333333333\n",
      "  (1, 122)\t0.3333333333333333\n",
      "  (1, 82)\t0.3333333333333333\n",
      "  (1, 81)\t0.3333333333333333\n",
      "  (1, 80)\t0.3333333333333333\n",
      "  (2, 220)\t0.28867513459481287\n",
      "  (2, 219)\t0.28867513459481287\n",
      "  (2, 184)\t0.28867513459481287\n",
      "  (2, 163)\t0.28867513459481287\n",
      "  :\t:\n",
      "  (10, 230)\t0.19360286426407214\n",
      "  (10, 229)\t0.19360286426407214\n",
      "  (10, 225)\t0.19360286426407214\n",
      "  (10, 224)\t0.19360286426407214\n",
      "  (10, 223)\t0.19360286426407214\n",
      "  (10, 222)\t0.19360286426407214\n",
      "  (10, 221)\t0.33096936063604365\n",
      "  (10, 165)\t0.19360286426407214\n",
      "  (10, 164)\t0.19360286426407214\n",
      "  (10, 144)\t0.19360286426407214\n",
      "  (10, 143)\t0.19360286426407214\n",
      "  (10, 138)\t0.14553451131600192\n",
      "  (10, 116)\t0.19360286426407214\n",
      "  (10, 103)\t0.19360286426407214\n",
      "  (10, 102)\t0.19360286426407214\n",
      "  (10, 101)\t0.19360286426407214\n",
      "  (10, 100)\t0.19360286426407214\n",
      "  (10, 99)\t0.16548468031802183\n",
      "  (10, 98)\t0.16548468031802183\n",
      "  (10, 51)\t0.19360286426407214\n",
      "  (10, 50)\t0.19360286426407214\n",
      "  (10, 49)\t0.19360286426407214\n",
      "  (10, 11)\t0.19360286426407214\n",
      "  (10, 10)\t0.19360286426407214\n",
      "  (10, 9)\t0.16548468031802183\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = tfidf_vectorizer.transform(sentences)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "66a6bc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.29197086, 0.29197086, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_matrix = tf_idf_matrix.todense()\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6edbab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_the_', '_the_ woman', 'abhorr', 'abhorr cold', 'abhorr cold precis', 'action', 'adjust', 'adjust tempera', 'adjust tempera introduc', 'adler', 'adler dubious', 'adler dubious question', 'admir', 'admir balanc', 'admir balanc mind', 'admir thing', 'admir thing observer—excel', 'admit', 'admit intrus', 'admit intrus delic', 'akin', 'akin love', 'akin love iren', 'alway', 'alway _the_', 'alway _the_ woman', 'balanc', 'balanc mind', 'cold', 'cold precis', 'cold precis admir', 'crack', 'crack one', 'crack one high-pow', 'delic', 'delic fine', 'delic fine adjust', 'distract', 'distract factor', 'distract factor throw', 'disturb', 'disturb strong', 'disturb strong emot', 'doubt', 'doubt upon', 'doubt upon mental', 'draw', 'draw veil', 'draw veil men', 'dubious', 'dubious question', 'dubious question memori', 'eclips', 'eclips predomin', 'eclips predomin whole', 'emot', 'emot akin', 'emot akin love', 'emot natur', 'emot one', 'emot one particular', 'eye', 'eye eclips', 'eye eclips predomin', 'factor', 'factor throw', 'factor throw doubt', 'fals', 'fals posit', 'felt', 'felt emot', 'felt emot akin', 'fine', 'fine adjust', 'fine adjust tempera', 'gibe', 'gibe sneer', 'grit', 'grit sensit', 'grit sensit instrument', 'heard', 'heard mention', 'heard mention name', 'high-pow', 'high-pow lens', 'high-pow lens disturb', 'holm', 'holm alway', 'holm alway _the_', 'instrument', 'instrument crack', 'instrument crack one', 'introduc', 'introduc distract', 'introduc distract factor', 'intrus', 'intrus delic', 'intrus delic fine', 'iren', 'iren adler', 'iren adler dubious', 'late', 'late iren', 'late iren adler', 'lens', 'lens disturb', 'lens disturb strong', 'love', 'love iren', 'love iren adler', 'lover', 'lover place', 'lover place fals', 'machin', 'machin world', 'machin world seen', 'memori', 'men', 'men ’', 'men ’ motiv', 'mental', 'mental result', 'mention', 'mention name', 'mind', 'motiv', 'motiv action', 'name', 'natur', 'never', 'never spoke', 'never spoke softer', 'observ', 'observ machin', 'observ machin world', 'observer—excel', 'observer—excel draw', 'observer—excel draw veil', 'one', 'one high-pow', 'one high-pow lens', 'one particular', 'one particular abhorr', 'one woman', 'one woman woman', 'particular', 'particular abhorr', 'particular abhorr cold', 'passion', 'passion save', 'passion save gibe', 'perfect', 'perfect reason', 'perfect reason observ', 'place', 'place fals', 'place fals posit', 'posit', 'precis', 'precis admir', 'precis admir balanc', 'predomin', 'predomin whole', 'predomin whole sex', 'question', 'question memori', 'reason', 'reason admit', 'reason admit intrus', 'reason observ', 'reason observ machin', 'result', 'save', 'save gibe', 'save gibe sneer', 'seen', 'seen lover', 'seen lover place', 'seldom', 'seldom heard', 'seldom heard mention', 'sensit', 'sensit instrument', 'sensit instrument crack', 'sex', 'sherlock', 'sherlock holm', 'sherlock holm alway', 'sneer', 'softer', 'softer passion', 'softer passion save', 'spoke', 'spoke softer', 'spoke softer passion', 'strong', 'strong emot', 'strong emot natur', 'take', 'take perfect', 'take perfect reason', 'tempera', 'tempera introduc', 'tempera introduc distract', 'thing', 'thing observer—excel', 'thing observer—excel draw', 'throw', 'throw doubt', 'throw doubt upon', 'train', 'train reason', 'train reason admit', 'upon', 'upon mental', 'upon mental result', 'veil', 'veil men', 'veil men ’', 'whole', 'whole sex', 'woman', 'woman late', 'woman late iren', 'woman woman', 'woman woman late', 'world', 'world seen', 'world seen lover', 'yet', 'yet one', 'yet one woman', '’', '’ motiv', '’ motiv action']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39ca7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sherlock', 'holm', 'alway', '_the_', 'woman', 'sherlock holm', 'holm alway', 'alway _the_', '_the_ woman', 'sherlock holm alway', 'holm alway _the_', 'alway _the_ woman']\n"
     ]
    }
   ],
   "source": [
    "analyze = tfidf_vectorizer.build_analyzer()\n",
    "print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a00fb",
   "metadata": {},
   "source": [
    "### Char ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "78e9c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(\"../Chapter01/sherlock_holmes_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e75d37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_char_vectorizer = TfidfVectorizer(analyzer = 'char_wb',\n",
    "                                       max_df = 0.9, max_features=200000,\n",
    "                                       min_df=0.05,use_idf=True,\n",
    "                                       ngram_range=(1, 3))\n",
    "\n",
    "tfidf_char_vectorizer = tfidf_char_vectorizer.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "55475e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 763)\t0.12662434631923655\n",
      "  (0, 762)\t0.12662434631923655\n",
      "  (0, 753)\t0.05840470946313\n",
      "  (0, 745)\t0.10823388151187574\n",
      "  (0, 744)\t0.0850646359499111\n",
      "  (0, 733)\t0.12662434631923655\n",
      "  (0, 731)\t0.07679517427049085\n",
      "  (0, 684)\t0.07679517427049085\n",
      "  (0, 683)\t0.07679517427049085\n",
      "  (0, 675)\t0.05840470946313\n",
      "  (0, 639)\t0.21646776302375148\n",
      "  (0, 638)\t0.21646776302375148\n",
      "  (0, 623)\t0.16087778612557863\n",
      "  (0, 602)\t0.12662434631923655\n",
      "  (0, 600)\t0.09518563907785169\n",
      "  (0, 521)\t0.10823388151187574\n",
      "  (0, 519)\t0.07679517427049085\n",
      "  (0, 518)\t0.12662434631923655\n",
      "  (0, 515)\t0.09518563907785169\n",
      "  (0, 507)\t0.12662434631923655\n",
      "  (0, 506)\t0.12662434631923655\n",
      "  (0, 503)\t0.07679517427049085\n",
      "  (0, 460)\t0.10823388151187574\n",
      "  (0, 459)\t0.10823388151187574\n",
      "  (0, 442)\t0.12662434631923655\n",
      "  :\t:\n",
      "  (10, 102)\t0.10440492924682453\n",
      "  (10, 101)\t0.08607445743804548\n",
      "  (10, 100)\t0.08607445743804548\n",
      "  (10, 99)\t0.11564746590034126\n",
      "  (10, 95)\t0.11564746590034126\n",
      "  (10, 88)\t0.05220246462341226\n",
      "  (10, 87)\t0.10935865461197974\n",
      "  (10, 73)\t0.08607445743804548\n",
      "  (10, 72)\t0.08607445743804548\n",
      "  (10, 62)\t0.06470360935853478\n",
      "  (10, 61)\t0.05782373295017063\n",
      "  (10, 59)\t0.07290576974131983\n",
      "  (10, 52)\t0.05782373295017063\n",
      "  (10, 50)\t0.04333276127902405\n",
      "  (10, 47)\t0.08607445743804548\n",
      "  (10, 46)\t0.05782373295017063\n",
      "  (10, 43)\t0.07357331270292299\n",
      "  (10, 40)\t0.039701319888289746\n",
      "  (10, 38)\t0.04333276127902405\n",
      "  (10, 22)\t0.08607445743804548\n",
      "  (10, 17)\t0.05782373295017063\n",
      "  (10, 13)\t0.05782373295017063\n",
      "  (10, 10)\t0.05220246462341226\n",
      "  (10, 8)\t0.10935865461197974\n",
      "  (10, 5)\t0.05220246462341226\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_char_vectorizer.transform(sentences)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97d2a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12662435 0.12662435 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.07119069 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.17252729 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "670ddf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' _', ' _t', ' a ', ' ab', ' ac', ' ad', ' ak', ' al', ' an', ' as', ' b', ' ba', ' be', ' bu', ' c', ' co', ' cr', ' d', ' de', ' di', ' do', ' dr', ' du', ' e', ' ec', ' em', ' ey', ' f', ' fa', ' fe', ' fi', ' fo', ' fr', ' g', ' gi', ' gr', ' ha', ' he', ' hi', ' ho', ' i', ' i ', ' in', ' ir', ' is', ' it', ' l', ' la', ' le', ' lo', ' m', ' ma', ' me', ' mi', ' mo', ' n', ' na', ' ne', ' no', ' o', ' ob', ' of', ' on', ' or', ' ot', ' ow', ' p', ' pa', ' pe', ' pl', ' po', ' pr', ' q', ' qu', ' r', ' re', ' s', ' sa', ' se', ' sh', ' sn', ' so', ' sp', ' st', ' su', ' ta', ' te', ' th', ' to', ' tr', ' u', ' un', ' up', ' v', ' ve', ' wa', ' we', ' wh', ' wi', ' wo', ' y', ' ye', ',', ', ', '-', '-p', '-po', '_', '_ ', '_t', '_th', 'a ', 'ab', 'abh', 'abl', 'ac', 'ace', 'ach', 'ack', 'act', 'ad', 'adj', 'adl', 'adm', 'ai', 'ain', 'ak', 'ake', 'aki', 'al', 'al ', 'ala', 'all', 'als', 'alw', 'am', 'ame', 'an ', 'an.', 'anc', 'and', 'any', 'ar', 'ard', 'arl', 'art', 'as', 'as ', 'as,', 'aso', 'ass', 'at', 'at ', 'ate', 'atu', 'av', 'ave', 'aw', 'awi', 'ay', 'ays', 'b', 'ba', 'bal', 'be', 'be ', 'bh', 'bho', 'bi', 'bin', 'bio', 'bl', 'ble', 'bly', 'bs', 'bse', 'bt', 'bt ', 'bu', 'but', 'c', 'ca', 'cat', 'ce', 'ce ', 'ced', 'cel', 'ch', 'ch ', 'chi', 'ci', 'cis', 'ck', 'ck ', 'cl', 'cli', 'co', 'col', 'cr', 'cra', 'ct', 'ct ', 'cti', 'cto', 'cu', 'cul', 'd ', 'd,', 'd, ', 'd.', 'd. ', 'de', 'del', 'der', 'di', 'dis', 'dj', 'dju', 'dl', 'dle', 'dm', 'dmi', 'do', 'dom', 'dou', 'dr', 'dra', 'du', 'dub', 'duc', 'e.', 'e. ', 'e_', 'e_ ', 'ea', 'ear', 'eas', 'ec', 'eci', 'ecl', 'ect', 'ed', 'ed ', 'edo', 'ee', 'een', 'eer', 'ei', 'eil', 'el', 'eld', 'elf', 'eli', 'ell', 'elt', 'ely', 'em', 'emo', 'emp', 'en', 'en,', 'ene', 'ens', 'ent', 'en’', 'er ', 'er,', 'er.', 'era', 'ere', 'erf', 'erl', 'erv', 'er—', 'es', 'es ', 'es,', 'est', 'esu', 'et', 'et ', 'ev', 'eve', 'ex', 'ex.', 'exc', 'ey', 'ey ', 'eye', 'f', 'f ', 'fa', 'fac', 'fal', 'fe', 'fec', 'fel', 'fi', 'fin', 'fo', 'for', 'fr', 'fro', 'ft', 'fte', 'g', 'g ', 'gh', 'gh-', 'ght', 'gi', 'gib', 'gr', 'gri', 'gs', 'gs ', 'h ', 'h-', 'h-p', 'ha', 'han', 'has', 'hat', 'hav', 'he', 'he ', 'he_', 'hea', 'her', 'hey', 'hi', 'hic', 'hig', 'him', 'hin', 'his', 'ho', 'hol', 'hor', 'hr', 'hro', 'ht', 'ht ', 'i ', 'ib', 'ibe', 'ic', 'ica', 'ich', 'icu', 'ig', 'igh', 'il', 'il ', 'im', 'im ', 'im,', 'ims', 'in', 'in ', 'ina', 'ind', 'ine', 'ing', 'ins', 'int', 'io', 'ion', 'iou', 'ip', 'ips', 'ir', 'ira', 'ire', 'is', 'is ', 'is.', 'ise', 'ist', 'it', 'it ', 'it,', 'ith', 'iti', 'iv', 'ive', 'j', 'ju', 'jus', 'k', 'k ', 'ke', 'ke ', 'ki', 'kin', 'l ', 'la', 'lac', 'lan', 'lar', 'lat', 'ld', 'ld ', 'ld,', 'ldo', 'le', 'le ', 'len', 'ler', 'lf', 'lf ', 'li', 'lic', 'lip', 'll', 'll ', 'lle', 'lm', 'lme', 'lo', 'loc', 'lov', 'ls', 'lse', 'lt', 'lt ', 'lts', 'lw', 'lwa', 'ly', 'ly ', 'ly,', 'm ', 'm,', 'm, ', 'ma', 'mac', 'man', 'me', 'me.', 'mem', 'men', 'mes', 'mi', 'mig', 'min', 'mir', 'mit', 'mo', 'mor', 'mos', 'mot', 'mp', 'mpe', 'ms', 'mse', 'n ', 'n,', 'n, ', 'n.', 'n. ', 'na', 'nab', 'nam', 'nat', 'nc', 'nce', 'nd', 'nd ', 'nd.', 'nde', 'ne', 'ne ', 'ned', 'nee', 'nel', 'ner', 'nev', 'ng', 'ng ', 'ngs', 'ni', 'nin', 'no', 'not', 'ns', 'ns ', 'ns,', 'ns.', 'nse', 'nsi', 'nst', 'nt', 'nt ', 'nt,', 'nta', 'nti', 'nto', 'ntr', 'ny', 'ny ', 'n’', 'n’s', 'o ', 'ob', 'obs', 'oc', 'ock', 'od', 'odu', 'of', 'of ', 'oft', 'ok', 'oke', 'ol', 'old', 'ole', 'olm', 'om', 'om ', 'oma', 'omi', 'on', 'on ', 'on.', 'ona', 'one', 'ong', 'oni', 'ons', 'or', 'or ', 'ore', 'orl', 'orr', 'ory', 'os', 'osi', 'ost', 'ot', 'ot ', 'oth', 'oti', 'ou', 'oub', 'oul', 'ous', 'ov', 'ove', 'ow', 'ow ', 'owe', 'own', 'p', 'pa', 'par', 'pas', 'pe', 'per', 'pl', 'pla', 'po', 'pok', 'pon', 'pos', 'pow', 'pr', 'pre', 'ps', 'pse', 'q', 'qu', 'que', 'r ', 'r,', 'r, ', 'r.', 'r. ', 'ra', 'rab', 'rac', 'rai', 'ram', 'raw', 'rb', 'rbi', 'rd', 'rd ', 're', 're ', 'rea', 'rec', 'red', 'ren', 'res', 'rf', 'rfe', 'ri', 'rit', 'rl', 'rld', 'rlo', 'rly', 'ro', 'rod', 'rom', 'ron', 'row', 'rr', 'rre', 'rt', 'rti', 'ru', 'rum', 'rus', 'rv', 'rve', 'rvi', 'ry', 'ry.', 'r—', 'r—e', 's ', 's,', 's, ', 's.', 's. ', 'sa', 'sav', 'se', 'se ', 'see', 'sel', 'sen', 'ser', 'ses', 'sex', 'sh', 'she', 'si', 'sio', 'sit', 'sn', 'sne', 'so', 'sof', 'son', 'sp', 'spo', 'ss', 'ssi', 'st', 'st ', 'ste', 'sti', 'str', 'stu', 'su', 'suc', 'sul', 't ', 't,', 't, ', 'ta', 'tak', 'tal', 'te', 'te ', 'ted', 'tem', 'ter', 'tes', 'th ', 'tha', 'the', 'thi', 'thr', 'ti', 'tic', 'tin', 'tio', 'tiv', 'to', 'to ', 'tor', 'tr', 'tra', 'tro', 'tru', 'ts', 'ts.', 'tu', 'tur', 'u', 'ub', 'ubi', 'ubt', 'uc', 'uce', 'uch', 'ue', 'ues', 'ul', 'ula', 'uld', 'ult', 'um', 'ume', 'un', 'und', 'up', 'upo', 'ur', 'urb', 'ure', 'us', 'us ', 'usi', 'ust', 'ut', 'ut ', 'v', 've', 've ', 'vei', 'ver', 'ves', 'vi', 'vin', 'w ', 'wa', 'was', 'way', 'we', 'wer', 'wh', 'whi', 'who', 'wi', 'win', 'wit', 'wn', 'wn ', 'wo', 'wom', 'wor', 'wou', 'x', 'x.', 'x. ', 'xc', 'xce', 'y', 'y ', 'y,', 'y, ', 'y.', 'y. ', 'ye', 'yes', 'yet', 'ys', 'ys ', '—', '—e', '—ex', '’', '’s', '’s ']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_char_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fdda71ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'o', ' ', ' t', 'to', 'o ', ' to', 'to ', ' ', 's', 'h', 'e', 'r', 'l', 'o', 'c', 'k', ' ', ' s', 'sh', 'he', 'er', 'rl', 'lo', 'oc', 'ck', 'k ', ' sh', 'she', 'her', 'erl', 'rlo', 'loc', 'ock', 'ck ', ' ', 'h', 'o', 'l', 'm', 'e', 's', ' ', ' h', 'ho', 'ol', 'lm', 'me', 'es', 's ', ' ho', 'hol', 'olm', 'lme', 'mes', 'es ', ' ', 's', 'h', 'e', ' ', ' s', 'sh', 'he', 'e ', ' sh', 'she', 'he ', ' ', 'i', 's', ' ', ' i', 'is', 's ', ' is', 'is ', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', ' a', 'al', 'lw', 'wa', 'ay', 'ys', 's ', ' al', 'alw', 'lwa', 'way', 'ays', 'ys ', ' ', '_', 't', 'h', 'e', '_', ' ', ' _', '_t', 'th', 'he', 'e_', '_ ', ' _t', '_th', 'the', 'he_', 'e_ ', ' ', 'w', 'o', 'm', 'a', 'n', '.', ' ', ' w', 'wo', 'om', 'ma', 'an', 'n.', '. ', ' wo', 'wom', 'oma', 'man', 'an.', 'n. ']\n"
     ]
    }
   ],
   "source": [
    "analyze = tfidf_char_vectorizer.build_analyzer()\n",
    "print(analyze(\"To Sherlock Holmes she is always _the_ woman.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c91c7",
   "metadata": {},
   "source": [
    "# Using word embeddings\n",
    "\n",
    "* These are powerful because they are a result of training a neural network that predicts a word from all other words in the sentence. \n",
    "\n",
    "\n",
    "* The resulting vector embeddings are similar for words that occur in similar contexts. We will use the embeddings to show these similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba01d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d748275",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_model_path = \"40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b829ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(w2vec_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dd36aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.309647 -0.127936 -0.136244 -0.252969  0.410695  0.206325  0.119236\n",
      " -0.244745 -0.436801  0.058889  0.237439  0.247656  0.072103  0.044183\n",
      " -0.424878  0.367344  0.153287  0.343856  0.232269 -0.181432 -0.050021\n",
      "  0.225756  0.71465  -0.564166 -0.168468 -0.153668  0.300445 -0.220122\n",
      " -0.021261  0.25779  -0.581744  0.320341 -0.236189  0.224906  0.029358\n",
      " -0.295143  0.483847 -0.05832   0.010784  0.050842 -0.034141  0.420114\n",
      "  0.126926 -0.405974 -0.421415  0.006092 -0.137557  0.038477  0.100005\n",
      "  0.151401  0.287163 -0.433263 -0.249083 -0.057834  0.367427 -0.181977\n",
      "  0.31608   0.063203 -0.486009 -0.127354 -0.283149  0.028113 -0.150146\n",
      " -0.38704   0.033237  0.146932  0.470853 -0.151154  0.064424  0.146739\n",
      " -0.164267 -0.094909  0.443384 -0.055244  0.117268 -0.221496 -0.185951\n",
      "  0.056249 -0.176986 -0.449508  0.345431 -0.096014 -0.19798   0.117698\n",
      " -0.162563 -0.181655 -0.18644  -0.158727  0.595464  0.161437 -0.382661\n",
      "  0.148537  0.173535  0.370556 -0.346765  0.055452  0.024405 -0.002895\n",
      "  0.081445  0.354575]\n"
     ]
    }
   ],
   "source": [
    "print(model['holmes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2c465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sherlock', 0.8416914939880371), ('parker', 0.8099909424781799), ('moriarty', 0.8039607405662537), ('sawyer', 0.8002702593803406), ('moore', 0.7932805418968201), ('wolfe', 0.7923581600189209), ('hale', 0.7910093069076538), ('doyle', 0.7906038761138916), ('holmes.the', 0.7895271182060242), ('watson', 0.7887691259384155), ('yates', 0.7882786393165588), ('stevenson', 0.7879441380500793), ('spencer', 0.7877693176269531), ('goodwin', 0.7866846323013306), ('baxter', 0.7864187359809875)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(['holmes'], topn=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "855deff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"It was not that he felt any emotion akin to love for Irene Adler.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1736b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(sentecne, model):\n",
    "    word_vectors = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word_vector = model.get_vector(word.lower())\n",
    "            word_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a47bd733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(word_vectors):\n",
    "    matrix = np.array(word_vectors)\n",
    "    centroid = np.mean(matrix[:,:],axis=0)\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3793411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09226871  0.14478634  0.23788658 -0.31754282  0.42911175 -0.05198449\n",
      "  0.12572111  0.01170996 -0.01138579  0.05200932  0.15247145  0.34026343\n",
      "  0.12961692  0.05010585 -0.09165132  0.3782767   0.08390289  0.30078036\n",
      " -0.24396846  0.42507184 -0.13556597  0.157348    0.19739327 -0.13114193\n",
      " -0.16301586  0.19061208 -0.17776786  0.00779739  0.22080304  0.00757292\n",
      "  0.08214489  0.05292403 -0.26995075  0.00906517  0.18542539 -0.20518285\n",
      " -0.09054315  0.02091755  0.15495133 -0.03320368 -0.03254781  0.35649517\n",
      " -0.14889626  0.07488623  0.13680871  0.4443542  -0.14066774  0.10251798\n",
      " -0.18436027  0.11045676 -0.17975916 -0.02136871 -0.11026109 -0.18642433\n",
      " -0.05931851  0.01703786  0.3544097   0.17131186 -0.31452173 -0.12231107\n",
      " -0.08258836  0.15248556  0.12112819 -0.32618955  0.01297824 -0.04008434\n",
      "  0.35412577 -0.13917081 -0.19634432 -0.03216437  0.30779663 -0.00925971\n",
      "  0.2535734  -0.14927842 -0.2347377  -0.32309702 -0.41007644  0.42555934\n",
      " -0.05917206  0.07272248  0.05830745  0.23424557  0.16473134  0.3544628\n",
      "  0.1398271   0.18607852 -0.17587464 -0.31830567  0.6772756  -0.12841727\n",
      " -0.10423013  0.25595132  0.01353108  0.10604009  0.09106217  0.23265913\n",
      " -0.10956775  0.14588395 -0.06585392  0.19673738]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = get_word_vectors(sentence, model)\n",
    "sentence_vector = get_sentence_vector(word_vectors)\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56f7737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['banana', 'apple', 'computer', 'strawberry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a429458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c428f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glass\n"
     ]
    }
   ],
   "source": [
    "word = 'cup'\n",
    "words = ['glass', 'computer', 'pencil', 'watch']\n",
    "print(model.most_similar_to_given(word, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f847ac",
   "metadata": {},
   "source": [
    "## Training your own embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bbd3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e8445e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Chapter03.bag_of_words_1 import get_sentences\n",
    "from Chapter01.tokenization import tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "570de5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = \"word2vec.model\"\n",
    "books_dir = \"1025_1853_bundle_archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8b751b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_book_sentences(directory):\n",
    "    text_files = [join(directory, f) for f in listdir(directory) if \\\n",
    "                 isfile(join(directory,f)) and '.txt' in f]\n",
    "    all_sentences = []\n",
    "    for text_file in text_files:\n",
    "        sentences = get_sentences(text_file)\n",
    "        all_sentences = all_sentences + sentences\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ee6054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(words, word2vec_model_path):\n",
    "    model = gensim.models.Word2Vec(words, window=5, \n",
    "                                   vector_size=200)\n",
    "    model.train(words, total_examples=len(words), epochs=200)\n",
    "    pickle.dump(model, open(word2vec_model_path, 'wb'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "75bde137",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_all_book_sentences(books_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32b5d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from striprtf.striprtf import rtf_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f01ba533",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [rtf_to_text(s).strip() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62622dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89975"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6bc6ae9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan DoyleThis eBook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7d5f6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [tokenize_nltk(s.lower()) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54741db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'adventures',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " ',',\n",
       " 'by',\n",
       " 'arthur',\n",
       " 'conan',\n",
       " 'doylethis',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'withalmost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " '.']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd369c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_word2vec(sentences, word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "167f6807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('illinois', 0.4451196789741516), ('road', 0.4412068724632263), ('mile', 0.4335242807865143), ('canoe', 0.4315584599971771), ('shore', 0.4308687448501587), ('passage', 0.41741570830345154), ('path', 0.4171934127807617), ('rhine', 0.4164977967739105), ('island', 0.4142218828201294), ('strand', 0.41371068358421326)]\n"
     ]
    }
   ],
   "source": [
    "w1 = \"river\"\n",
    "words = model.wv.most_similar(w1, topn=10)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1f2cd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f2a5d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(word2vec_model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29ebaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_score, word_list = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ae9b3ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14359892569382274"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8302c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2fe24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = \"40/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d23e10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = KeyedVectors.load_word2vec_format(pretrained_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b25697f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_score, word_list = pretrained_model.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15417db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5867802524889665\n"
     ]
    }
   ],
   "source": [
    "print(analogy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0b5cd",
   "metadata": {},
   "source": [
    "# Representing phrases – phrase2vec\n",
    "\n",
    "* Encoding words is useful, but usually, we deal with more complex units, such as phrases and sentences. \n",
    "\n",
    "\n",
    "* Phrases are important because they specify more detail than just words. For example, the phrase delicious fried rice is very different than just the word rice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53b40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from langdetect import detect\n",
    "import pickle\n",
    "from nltk import FreqDist\n",
    "from Chapter01.dividing_into_sentences import \\\n",
    "divide_into_sentences_nltk\n",
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e7ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766a319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_file = \"/Volumes/MAC-DISK/yelp_dataset/yelp_academic_dataset_review.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915ad0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_review(filename):\n",
    "    reader = pd.read_json(filename, orient='records',\n",
    "                         lines=True, chunksize=1000)\n",
    "    chunk = next(reader)\n",
    "    text = ''\n",
    "    for index, row in chunk.iterrows():\n",
    "        row_text = row['text']\n",
    "        lang = detect(row_text)\n",
    "        if lang =='en':\n",
    "            text = text + row_text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1403a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(text):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    phrases = {}\n",
    "    current_phrase = []\n",
    "    for word in words:\n",
    "        if word in stop_words or word in string.punctuation:\n",
    "            if len(current_phrase) > 1:\n",
    "                phrases[\" \".join(current_phrase)] = '_'.join(current_phrase)\n",
    "                current_phrase = []\n",
    "        else:\n",
    "            current_phrase.append(word)\n",
    "        \n",
    "        if len(current_phrase) > 1:\n",
    "            phrases[\" \".join(current_phrase)] = '_'.join(current_phrase)\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6380724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phrases(phrases_dict, text):\n",
    "    for phrase in phrases_dict.keys():\n",
    "        text = text.replace(phrase, phrases_dict[phrase])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649fe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_to_file(text, filename):\n",
    "    text_file = open(filename, 'w', encoding='utf-8')\n",
    "    text_file.write(text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a9b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_frequency_list(word_list, filename):\n",
    "    fdist = FreqDist(word_list)\n",
    "    pickle.dump(fdist, open(filename, 'wb'))\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1786d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_yelp_review(yelp_reviews_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c3dd395",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-65ccf3098847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwrite_text_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_text.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-e880727b2cb3>\u001b[0m in \u001b[0;36mreplace_phrases\u001b[0;34m(phrases_dict, text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplace_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrases_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrases_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phrases = get_phrases(text)\n",
    "text = replace_phrases(phrases, text)\n",
    "write_text_to_file(text, 'all_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5347f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_text.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = divide_into_sentences_nltk(text)\n",
    "all_sentece_words = [tokenize_nltk(sentence.lower()) for sentence in sentences]\n",
    "flat_word_list = [word.lower() for sentence in all_sentece_words for word in sentence]\n",
    "fdist = create_and_save_frequency_list(flat_word_list, 'fdist.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49968c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 58177), ('the', 53315), (',', 39252), ('and', 37563), ('i', 29031), ('a', 27953), ('to', 25349), ('was', 18698), ('of', 15052), ('it', 14691), ('is', 13431), ('for', 12294), ('in', 11866), ('!', 11453), ('my', 9945), ('that', 9503), ('with', 8693), ('but', 8563), ('this', 7907), ('they', 7898), ('we', 7667), ('on', 7409), ('you', 6990), ('have', 6594), ('not', 6572), ('had', 6230), ('so', 5661), ('were', 5648), ('are', 5305), ('at', 5288), (')', 4795), ('be', 4618), ('(', 4436), ('as', 4283), ('very', 4156), ('there', 4079), ('me', 4079), (\"'s\", 3786), ('if', 3646), ('all', 3508), ('out', 3507), ('just', 3369), ('here', 3322), ('place', 3132), ('our', 3050), ('from', 3007), ('their', 2994), ('when', 2959), ('food', 2830), ('up', 2828), ('or', 2792), ('an', 2752), ('about', 2652), ('which', 2505), ('what', 2300), ('some', 2294), ('...', 2215), ('will', 2191), ('-', 2170), ('he', 2157), ('been', 2113), ('no', 2105), ('only', 2070), ('she', 2053), ('your', 2030), (\"n't\", 2012), ('more', 2009), ('good', 1953), ('do', 1942), ('$', 1937), ('?', 1931), ('them', 1897), ('``', 1893), (\"''\", 1865), ('by', 1842), ('can', 1825), ('other', 1722), ('because', 1722), ('service', 1715), ('has', 1711), ('too', 1707), ('did', 1651), ('after', 1628), (':', 1583), ('than', 1476), ('get', 1464), ('one', 1374), ('great', 1369), ('over', 1281), ('who', 1244), ('time', 1222), ('go', 1205), ('then', 1162), ('her', 1101), ('got', 1085), ('would', 1067), ('before', 1057), ('again', 1049), ('how', 1048), ('love', 1034)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9e3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_word2vec_model(words, filename):\n",
    "    model = gensim.models.Word2Vec(words, min_count=1)\n",
    "    model.train(words, total_examples=model.corpus_count, epochs=400)\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76edb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_and_save_word2vec_model(all_sentece_words, 'phrases.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab8cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('would_highly_recommend', 0.6025195121765137), ('definitely_recommend', 0.5658879280090332), ('would_definitely_recommend', 0.5532544255256653), ('would_recommend', 0.4962897300720215), ('ate_around', 0.46942415833473206), ('love', 0.46003633737564087), ('thank', 0.4555864930152893), ('recommend', 0.44822776317596436), ('really_enjoyed', 0.42181792855262756), ('would_strongly_recommend', 0.41948580741882324)]\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.most_similar('highly_recommend', topn=10)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91728e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comfort_food_choices', 0.4160696268081665), ('pricy', 0.3976624011993408), ('cuisine', 0.3934507668018341), ('food_court', 0.3922019302845001), ('citywalk', 0.38666999340057373), ('thai', 0.3826940357685089), ('classic', 0.3824933171272278), ('cool_drink', 0.3738446533679962), ('finally_came', 0.37134498357772827), ('overpriced', 0.37012335658073425)]\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.most_similar(\"dim_sum\", topn=10)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc3166",
   "metadata": {},
   "source": [
    "# Using BERT instead of word embeddings\n",
    "\n",
    "\n",
    "* A recent development in the embeddings world is BERT, also known as Bidirectional Encoder Representations from Transformers, which, like word embeddings, gives a vector representation, but it takes context into account and can represent a whole sentence. \n",
    "\n",
    "\n",
    "* We can use the Hugging Face sentence_transformers package to represent sentences as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fea52f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from Chapter01.dividing_into_sentences import read_text_file, divide_into_sentences_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074fedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_text_file(\"../Chapter01/sherlock_holmes.txt\")\n",
    "sentences = divide_into_sentences_nltk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1898560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcec465c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4108902 ,  1.1092619 ,  0.65330595, ..., -0.92320925,\n",
       "         0.47286835,  0.36298928],\n",
       "       [-0.16485526,  0.6998441 ,  0.70763886, ..., -0.40428248,\n",
       "        -0.30385858, -0.3291513 ],\n",
       "       [-0.37814918,  0.34770998, -0.09765318, ...,  0.13831207,\n",
       "         0.36044088,  0.12381998],\n",
       "       ...,\n",
       "       [-0.251487  ,  0.57580566,  1.459657  , ...,  0.56890166,\n",
       "        -0.6003895 , -0.02739903],\n",
       "       [-0.64917624,  0.360967  ,  1.1350368 , ..., -0.0405464 ,\n",
       "         0.07568653,  0.18094465],\n",
       "       [-0.42418253,  0.4814613 ,  0.93001944, ...,  0.73677164,\n",
       "        -0.09357806, -0.00368021]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d633dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.61978924e-02, -5.74670076e-01,  1.08264256e+00,\n",
       "         7.36554265e-01,  5.51345468e-01, -9.39117551e-01,\n",
       "        -2.80430049e-01, -5.41625977e-01,  7.50948846e-01,\n",
       "        -4.40971613e-01,  5.31526685e-01, -5.41883230e-01,\n",
       "         1.92792654e-01,  3.44117582e-01,  1.50266385e+00,\n",
       "        -6.26989722e-01, -2.42828563e-01, -3.66734564e-01,\n",
       "         5.57459652e-01, -2.21802279e-01, -9.69591439e-01,\n",
       "        -4.38950717e-01, -7.93552220e-01, -5.84923029e-01,\n",
       "        -1.55690983e-01,  2.12004229e-01,  4.02013630e-01,\n",
       "        -2.63063580e-01,  6.21910393e-01,  5.97237408e-01,\n",
       "         9.78125483e-02,  7.20052421e-01, -4.66322839e-01,\n",
       "         3.86450231e-01, -8.24903369e-01,  1.09985697e+00,\n",
       "        -3.59134972e-01, -4.31918561e-01,  2.56565101e-02,\n",
       "         5.73159695e-01,  2.40237087e-01, -7.67571330e-01,\n",
       "         9.38899398e-01, -3.60024542e-01, -8.77114952e-01,\n",
       "        -2.47680426e-01, -8.65839005e-01,  1.04203534e+00,\n",
       "         3.65989566e-01, -6.47715852e-02, -7.04246700e-01,\n",
       "         5.91077190e-03, -8.04807842e-01,  2.21370175e-01,\n",
       "        -1.79775357e-01,  8.04759026e-01, -4.44356829e-01,\n",
       "        -4.46379274e-01,  7.55989626e-02, -2.17623949e-01,\n",
       "         6.87522054e-01, -4.70606565e-01,  7.68602252e-01,\n",
       "         3.06245983e-01, -9.10274506e-01,  6.28714621e-01,\n",
       "         8.11625183e-01, -3.83964851e-02,  2.41827101e-01,\n",
       "        -3.13487113e-01,  9.08200741e-01,  9.62719172e-02,\n",
       "        -4.04239655e-01,  3.88272077e-01, -4.22080278e-01,\n",
       "        -4.33438957e-01,  7.79737115e-01, -1.52796507e-01,\n",
       "         9.48364377e-01,  9.40597892e-01,  7.34893441e-01,\n",
       "         6.64678335e-01,  3.90085161e-01,  1.34034228e+00,\n",
       "         1.08233318e-01, -3.95383656e-01,  2.27669239e-01,\n",
       "         2.79988974e-01, -1.70993865e+00,  4.19878632e-01,\n",
       "         2.10107952e-01, -5.28874397e-01,  7.26883411e-01,\n",
       "        -1.09560502e+00, -1.37757929e-02,  9.04118061e-01,\n",
       "         3.81059572e-02, -2.46198580e-01,  5.54345608e-01,\n",
       "        -3.61581028e-01,  6.56911969e-01, -1.13580930e+00,\n",
       "        -3.63160133e-01,  1.89224958e-01, -9.07701492e-01,\n",
       "         4.76393521e-01,  5.17776608e-01, -1.19439971e+00,\n",
       "         9.81190205e-01, -4.90277931e-02, -1.50155589e-01,\n",
       "         5.42353272e-01,  7.50451684e-01,  4.16537285e-01,\n",
       "        -8.21164772e-02,  1.74670964e-01, -1.52958226e+00,\n",
       "         7.14822650e-01,  2.90298074e-01,  7.76944578e-01,\n",
       "        -3.69304389e-01, -5.91734871e-02,  4.75243665e-02,\n",
       "        -5.92363551e-02,  5.25073409e-01, -7.08114743e-01,\n",
       "        -2.52816588e-01,  1.29740524e+00, -1.28921819e+00,\n",
       "         1.40943483e-01,  2.23808572e-01,  3.13144833e-01,\n",
       "         5.11784196e-01, -2.58186311e-02, -2.22357418e-02,\n",
       "         1.43362671e-01,  3.71723354e-01, -8.38745415e-01,\n",
       "        -1.68651605e+00,  1.41473874e-01, -1.02585089e+00,\n",
       "         2.03563094e-01,  2.62083739e-01,  7.15595558e-02,\n",
       "         8.99083734e-01, -5.28086543e-01,  4.57701206e-01,\n",
       "        -1.53959394e-01, -1.18477903e-01, -8.72816592e-02,\n",
       "         3.76756378e-02,  7.07110286e-01, -7.49195933e-01,\n",
       "         6.55253679e-02,  5.15915990e-01,  8.10095906e-01,\n",
       "        -5.19098401e-01,  2.18548223e-01,  3.93635184e-01,\n",
       "         6.09232903e-01,  3.38943779e-01,  9.46189523e-01,\n",
       "        -1.22255194e+00, -1.30252838e-01,  6.54754162e-01,\n",
       "        -8.41753364e-01,  8.06662217e-02,  3.00382704e-01,\n",
       "        -8.12576592e-01,  5.76909661e-01,  3.84841889e-01,\n",
       "        -4.66208279e-01,  2.26867199e-02, -1.82356954e-01,\n",
       "         3.94847095e-01, -4.98396963e-01, -3.28548968e-01,\n",
       "        -9.03554559e-01,  8.20927992e-02, -1.04095936e+00,\n",
       "        -2.30055958e-01, -3.92115533e-01,  4.96068537e-01,\n",
       "        -2.35530466e-01, -4.00794208e-01, -6.51569724e-01,\n",
       "        -2.91195005e-01,  1.30913958e-01,  4.64919657e-01,\n",
       "        -3.41870159e-01,  2.17010736e-01, -1.27972937e+00,\n",
       "        -4.89795133e-02, -2.56777078e-01,  5.36798656e-01,\n",
       "         1.87181950e-01, -1.41931936e-01,  1.18326187e+00,\n",
       "        -2.36010715e-01, -4.17852461e-01,  7.82344162e-01,\n",
       "         2.52221286e-01, -5.96156418e-01, -1.00841321e-01,\n",
       "        -8.53248000e-01, -4.25655022e-02,  1.98200077e-01,\n",
       "        -4.74220574e-01, -7.92613149e-01, -5.80051616e-02,\n",
       "        -5.30738831e-01, -4.08137664e-02,  1.06413770e+00,\n",
       "        -1.95783645e-01, -1.18166256e+00, -3.84621739e-01,\n",
       "         1.83776170e-01,  1.07725207e-02, -5.73451459e-01,\n",
       "        -6.28656000e-02,  3.68159115e-01,  2.95109693e-02,\n",
       "        -3.12129110e-01,  7.59689450e-01,  1.71070144e-01,\n",
       "        -2.51229823e-01,  1.21716106e+00, -1.22804642e-01,\n",
       "        -1.25155675e+00,  2.96520561e-01, -1.59363002e-01,\n",
       "        -3.07772219e-01,  7.38719940e-01, -3.21678758e-01,\n",
       "         4.56360579e-01,  6.06843174e-01, -1.03566669e-01,\n",
       "         5.64707458e-01,  1.14319181e+00,  1.58939511e-01,\n",
       "         4.00118172e-01, -1.40942246e-01,  1.12744331e-01,\n",
       "        -5.38727403e-01, -1.14471185e+00,  2.97612041e-01,\n",
       "         9.01935756e-01,  1.17314421e-01,  1.23890437e-01,\n",
       "         1.81390569e-01,  2.68770754e-01, -1.31405786e-01,\n",
       "        -7.04882368e-02, -1.44888520e+00,  6.95817530e-01,\n",
       "         9.22023952e-01, -3.09454530e-01, -3.62523556e-01,\n",
       "        -6.64734840e-02,  3.39314133e-01, -1.24447644e-01,\n",
       "        -6.02809012e-01, -1.31329441e+00, -2.52024144e-01,\n",
       "        -1.04210663e+00, -1.28538656e+00, -4.44637150e-01,\n",
       "         1.38655871e-01,  2.24620253e-01,  2.20691204e-01,\n",
       "        -2.93923140e-01,  3.47230211e-02, -2.68138736e-01,\n",
       "         2.52771914e-01,  8.13671470e-01, -2.96582073e-01,\n",
       "         9.17571187e-01, -4.60456192e-01, -4.12233829e-01,\n",
       "        -6.67554021e-01,  8.68171751e-02,  1.60344839e-01,\n",
       "        -1.63572562e+00, -3.02731812e-01, -8.21481347e-01,\n",
       "        -6.96784556e-01,  1.73272640e-01,  4.73819494e-01,\n",
       "        -3.36352468e-01, -5.97198308e-01, -5.73052764e-01,\n",
       "        -3.03226411e-01,  4.93410021e-01, -9.86553848e-01,\n",
       "         1.13781428e+00,  2.27787226e-01,  5.98225951e-01,\n",
       "        -3.63629907e-01, -4.52825308e-01, -2.30596829e-02,\n",
       "        -7.20068455e-01,  5.94032705e-01,  6.28295094e-02,\n",
       "        -1.67542076e+00,  6.84947371e-01,  3.15437913e-01,\n",
       "         1.30231786e+00,  5.92271090e-01,  5.25537968e-01,\n",
       "        -5.70111334e-01, -2.30185896e-01,  4.87738438e-02,\n",
       "        -1.46310925e+00,  2.91679144e-01,  5.30582249e-01,\n",
       "         4.55556244e-01, -1.05712247e+00,  1.30680823e+00,\n",
       "        -5.77882417e-02,  9.02808681e-02, -4.90736723e-01,\n",
       "         2.87301630e-01,  2.68229216e-01,  6.72244549e-01,\n",
       "         1.36548162e+00,  5.68987150e-03, -7.43365765e-01,\n",
       "         1.22896099e+00, -6.58429325e-01,  3.99420440e-01,\n",
       "        -1.41790360e-01,  2.50783592e-01,  1.06889471e-01,\n",
       "         1.99269146e-01, -1.92880481e-01,  9.59797502e-01,\n",
       "        -6.98571503e-01, -7.77557433e-01, -3.92656475e-01,\n",
       "         3.84375095e-01,  9.25755858e-01, -7.16385663e-01,\n",
       "        -2.27031618e-01, -1.80263311e-01,  8.75891864e-01,\n",
       "         6.64649010e-01, -4.05385867e-02, -3.86090875e-02,\n",
       "        -3.71947229e-01, -9.97884154e-01,  7.03277946e-01,\n",
       "        -4.95072365e-01, -1.86441064e-01, -5.57139397e-01,\n",
       "        -5.84804416e-01,  8.57470989e-01,  9.73084807e-01,\n",
       "        -4.38925624e-02,  2.94235405e-02, -5.07005095e-01,\n",
       "         5.19699454e-01,  6.78203344e-01, -7.75590420e-01,\n",
       "        -2.01014563e-01, -6.46364316e-02,  1.69209927e-01,\n",
       "        -3.15657318e-01, -6.38075694e-02,  4.14385259e-01,\n",
       "        -7.92833209e-01,  7.81127334e-01,  4.93745804e-01,\n",
       "        -1.67177409e-01,  3.25104296e-01,  4.08136845e-01,\n",
       "        -1.98128313e-01, -1.57883704e+00, -3.42696756e-01,\n",
       "         2.81852096e-01,  1.56250262e+00,  5.42057827e-02,\n",
       "        -3.72796476e-01,  3.45635444e-01,  2.87223123e-02,\n",
       "         3.25957239e-01,  4.45556074e-01, -2.36087739e-02,\n",
       "        -3.59087139e-01,  2.44492814e-01,  1.08073756e-01,\n",
       "        -2.71439855e-03,  8.00017238e-01,  1.64834231e-01,\n",
       "        -7.84868822e-02,  6.20461762e-01, -8.95981669e-01,\n",
       "         9.13146019e-01,  4.57475513e-01,  4.43795294e-01,\n",
       "         9.52291191e-01,  5.36398351e-01,  2.86281705e-01,\n",
       "        -1.04407477e+00,  6.62327409e-01, -4.04091179e-01,\n",
       "        -4.31125641e-01,  8.86067212e-01, -4.57726806e-01,\n",
       "        -5.38968027e-01,  4.08299208e-01,  8.11756492e-01,\n",
       "         3.18704069e-01, -3.00523788e-01, -2.06661314e-01,\n",
       "        -7.86024034e-01, -5.08803964e-01,  3.13620418e-01,\n",
       "        -8.04826081e-01, -1.63369104e-01,  8.13572049e-01,\n",
       "         1.00145601e-01,  4.00417805e-01,  7.28013098e-01,\n",
       "         5.52753925e-01,  3.58506531e-01, -1.27685100e-01,\n",
       "         1.08199143e+00, -3.31548065e-01, -2.09919065e-01,\n",
       "         7.80606627e-01,  4.58833545e-01,  3.08563888e-01,\n",
       "        -1.65190947e+00, -2.91312158e-01,  2.00888991e-01,\n",
       "        -7.44069368e-02, -8.22850242e-02, -7.58320868e-01,\n",
       "         9.04613853e-01,  1.21439159e-01,  6.82858348e-01,\n",
       "         2.89186060e-01,  4.92761672e-01,  3.60833347e-01,\n",
       "         7.89761126e-01, -6.59668803e-01,  2.36380011e-01,\n",
       "        -5.94367564e-01, -3.39692682e-02,  1.18597291e-01,\n",
       "         1.19545031e-02,  3.18183571e-01,  2.10744783e-01,\n",
       "        -9.42379013e-02,  2.62368500e-01,  6.69895589e-01,\n",
       "        -1.56853020e-01,  1.33296394e+00, -2.77232714e-02,\n",
       "        -3.85722309e-01, -1.11986542e+00, -1.05736101e+00,\n",
       "        -1.63956389e-01, -3.50023776e-01,  5.55418909e-01,\n",
       "         8.63682404e-02,  2.35158391e-02,  8.91652703e-01,\n",
       "        -8.80943462e-02,  4.73888665e-02,  2.20449403e-01,\n",
       "         4.89862785e-02, -3.63905013e-01,  9.15377259e-01,\n",
       "         6.34511560e-03,  2.01183647e-01, -1.10757422e+00,\n",
       "         1.27832353e+00,  3.28630626e-01,  1.38878036e+00,\n",
       "         8.49069729e-02,  1.00432619e-01,  3.45135450e-01,\n",
       "        -2.10845321e-01, -1.00216460e+00,  5.91331184e-01,\n",
       "         2.21506476e-01, -2.87747175e-01,  1.82966545e-01,\n",
       "        -7.25790501e-01, -4.80669774e-02, -2.03027919e-01,\n",
       "         4.38335724e-02, -8.87951255e-01, -1.55841827e+00,\n",
       "        -4.66843784e-01, -8.82860005e-01,  7.36437261e-01,\n",
       "         4.56633270e-02,  7.06463277e-01, -4.05852497e-01,\n",
       "         6.14356101e-01,  5.78302503e-01, -1.18286157e+00,\n",
       "        -2.77493708e-02, -7.50104725e-01, -3.99379283e-01,\n",
       "        -7.83226728e-01, -2.69309551e-01,  4.57273811e-01,\n",
       "         4.13473517e-01,  6.31410301e-01, -2.75247991e-01,\n",
       "        -7.64645934e-01, -1.52929902e+00,  3.75116140e-01,\n",
       "        -6.66492343e-01,  3.35962057e-01, -9.38857734e-01,\n",
       "        -7.68575490e-01, -2.29994059e-01, -6.83000088e-01,\n",
       "        -2.81757414e-01, -1.94847281e-03,  3.74063194e-01,\n",
       "         2.74861038e-01,  6.72254264e-01,  5.03118575e-01,\n",
       "        -9.38203216e-01, -9.07174766e-01, -1.03282094e+00,\n",
       "        -6.58716261e-01, -6.61198974e-01, -1.01838481e+00,\n",
       "         3.76533926e-01,  1.15521634e+00, -7.43075848e-01,\n",
       "        -7.23348320e-01, -9.15834248e-01,  5.64644337e-01,\n",
       "         4.47726727e-01,  6.31319821e-01,  1.31399632e-02,\n",
       "        -2.28386685e-01, -2.94719160e-01,  6.34674430e-01,\n",
       "         2.57517397e-01, -4.38147455e-01,  6.84131861e-01,\n",
       "         5.28099537e-01, -6.23339236e-01, -9.34385434e-02,\n",
       "         6.24624610e-01, -1.16408467e+00, -9.08659935e-01,\n",
       "         4.05243099e-01, -1.46841908e+00,  3.72077674e-01,\n",
       "        -7.48038441e-02,  3.50936264e-01,  8.60909462e-01,\n",
       "         4.29133892e-01, -1.42858118e-01,  1.14047766e-01,\n",
       "         3.74012589e-02, -3.59079897e-01, -4.20887303e-03,\n",
       "        -2.62010664e-01,  3.37051749e-01,  1.58273470e+00,\n",
       "        -6.01672769e-01, -2.27530763e-01,  1.19591630e+00,\n",
       "         6.50878310e-01, -8.92956629e-02, -4.18905586e-01,\n",
       "         1.57126606e+00, -3.31305116e-01, -2.72812873e-01,\n",
       "         1.76590547e-01, -7.38167241e-02,  3.12449127e-01,\n",
       "        -8.96932208e-04, -2.13332847e-01, -1.53492773e+00,\n",
       "         9.10114795e-02,  2.28632331e-01, -1.51050591e+00,\n",
       "        -3.86804909e-01,  2.57271945e-01,  8.85265231e-01,\n",
       "        -4.24517930e-01, -3.86550307e-01,  8.38054836e-01,\n",
       "        -2.59797871e-01,  5.40384412e-01, -2.86055028e-01,\n",
       "         5.55786848e-01,  6.21417642e-01, -1.23508967e-01,\n",
       "         1.77528150e-02, -5.28725743e-01, -1.20459840e-01,\n",
       "         2.90037900e-01,  2.68551141e-01,  1.10356197e-01,\n",
       "        -1.02779245e+00, -9.56030071e-01,  4.13787991e-01,\n",
       "         2.25954145e-01, -1.31037712e+00, -2.35217905e+00,\n",
       "        -1.86778307e-01, -1.20109797e+00, -4.42366600e-01,\n",
       "        -8.44804585e-01, -3.90224069e-01,  6.69505954e-01,\n",
       "        -1.70657322e-01, -3.98018491e-03, -2.59338379e-01,\n",
       "         3.82893592e-01, -5.44769347e-01, -6.13513999e-02,\n",
       "        -2.61051238e-01, -3.40793073e-01, -9.61916223e-02,\n",
       "         5.89638837e-02,  3.95657510e-01, -5.89383304e-01,\n",
       "         2.99506247e-01, -1.74588203e-01,  2.27259710e-01,\n",
       "        -8.18412960e-01, -7.32798517e-01,  2.97234297e-01,\n",
       "         2.28747040e-01,  9.10663247e-01, -6.07199371e-02,\n",
       "        -4.96814251e-01, -1.18732966e-01, -7.73223579e-01,\n",
       "        -2.96833903e-01,  6.71157300e-01, -3.38998079e-01,\n",
       "        -9.53685194e-02, -5.03132701e-01,  2.47434467e-01,\n",
       "         4.27117348e-01,  1.45333618e-01,  8.05351138e-01,\n",
       "        -8.57041836e-01,  7.25859821e-01, -9.15385112e-02,\n",
       "         2.53698349e-01,  1.85381711e-01, -5.47813058e-01,\n",
       "        -5.04315197e-01,  1.36256087e+00, -8.62132251e-01,\n",
       "         6.29884750e-02, -1.83468878e-01, -7.75032103e-01,\n",
       "         3.41291726e-01,  5.35521448e-01, -1.12125707e+00,\n",
       "        -4.04213648e-03,  1.34707674e-01,  1.16122112e-01,\n",
       "        -3.41756761e-01,  9.87665176e-01, -3.03871036e-01,\n",
       "        -1.51143730e-01, -1.04746354e+00, -4.28524315e-01,\n",
       "        -4.62444961e-01,  7.40947902e-01,  9.54195142e-01,\n",
       "        -8.21143508e-01,  8.59958827e-01, -2.04626331e-03,\n",
       "         3.10486645e-01,  5.44805408e-01, -9.86975968e-01,\n",
       "        -8.76125336e-01, -4.50644672e-01,  4.49648768e-01,\n",
       "         4.62937444e-01,  3.74305308e-01,  1.16935849e+00,\n",
       "        -7.40901828e-01,  2.80613810e-01, -2.25817189e-02,\n",
       "        -1.63027799e+00,  4.21418846e-01,  8.17476451e-01,\n",
       "         5.69595158e-01,  8.26955810e-02,  9.53551292e-01,\n",
       "         3.85208130e-01, -1.18330574e+00,  7.06427276e-01,\n",
       "         6.79352462e-01, -1.24054742e+00, -1.43386674e+00,\n",
       "        -5.74116051e-01, -2.98397243e-01, -4.57422495e-01,\n",
       "        -6.46811783e-01, -1.45287260e-01,  5.62382996e-01,\n",
       "         2.07258314e-01,  8.17670703e-01,  6.23559535e-01,\n",
       "         2.27314625e-02,  1.39370799e-01, -6.04807615e-01,\n",
       "         2.03888744e-01,  4.75691170e-01, -4.73644555e-01,\n",
       "        -6.28173172e-01,  3.59570205e-01,  2.00764537e-01,\n",
       "         3.36479485e-01,  2.37539619e-01, -3.53950530e-01,\n",
       "         1.01171538e-01, -2.28819966e-01, -2.18077615e-01,\n",
       "        -2.69778550e-01,  9.67437088e-01,  2.01076984e-01,\n",
       "        -1.27904519e-01,  7.99281418e-01, -1.35350868e-01,\n",
       "        -3.20219904e-01,  2.12029144e-01, -2.10678861e-01,\n",
       "        -7.97776699e-01,  3.23254287e-01, -9.47452366e-01,\n",
       "         8.75161946e-01,  4.30704594e-01, -2.36586571e-01,\n",
       "        -7.85881698e-01,  4.22883704e-02, -1.80886000e-01,\n",
       "         2.75951321e-03, -6.27907336e-01,  6.07978046e-01,\n",
       "         1.15655160e+00,  8.13455224e-01, -6.18542075e-01,\n",
       "        -1.47315606e-01, -5.30723929e-01,  9.00457025e-01,\n",
       "        -5.07012248e-01, -3.95729601e-01, -3.64804380e-02,\n",
       "        -4.39003468e-01, -5.66665605e-02, -7.64992356e-01,\n",
       "        -1.15275955e+00, -5.64237237e-01,  2.25647762e-01,\n",
       "        -8.70941043e-01, -1.07499683e+00, -3.86935484e-04]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = model.encode(['the beautiful lake'])\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bcbc9",
   "metadata": {},
   "source": [
    "# Getting started with semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc87013",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e90b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
