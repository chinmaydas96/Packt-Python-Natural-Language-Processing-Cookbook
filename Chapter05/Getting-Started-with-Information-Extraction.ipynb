{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83f0cce",
   "metadata": {},
   "source": [
    "### Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e33953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas python-Levenshtein spacy nltk textblob tqdm transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796f4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "data_file = 'DataScientist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e29d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "      <th>Easy Apply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hopper\\n3.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Travel Agencies</td>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>At Noom, we use scientifically proven methods ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Noom US\\n4.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>2008</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Health, Beauty, &amp; Fitness</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                          Job Title  \\\n",
       "0           0      0              Senior Data Scientist   \n",
       "1           1      1  Data Scientist, Product Analytics   \n",
       "\n",
       "                Salary Estimate  \\\n",
       "0  $111K-$181K (Glassdoor est.)   \n",
       "1  $111K-$181K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  Company Name  \\\n",
       "0  ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...     3.5   Hopper\\n3.5   \n",
       "1  At Noom, we use scientifically proven methods ...     4.5  Noom US\\n4.5   \n",
       "\n",
       "       Location      Headquarters                    Size  Founded  \\\n",
       "0  New York, NY  Montreal, Canada   501 to 1000 employees     2007   \n",
       "1  New York, NY      New York, NY  1001 to 5000 employees     2008   \n",
       "\n",
       "   Type of ownership                   Industry             Sector  \\\n",
       "0  Company - Private            Travel Agencies   Travel & Tourism   \n",
       "1  Company - Private  Health, Beauty, & Fitness  Consumer Services   \n",
       "\n",
       "                    Revenue Competitors Easy Apply  \n",
       "0  Unknown / Non-Applicable          -1         -1  \n",
       "1  Unknown / Non-Applicable          -1         -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1074400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(df, regex, column_name):\n",
    "    df[column_name] = df['Job Description'].apply(lambda x:re.findall(regex, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b3c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_items(df, column_name):\n",
    "    items = []\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row[column_name]) > 0:\n",
    "            for item in list(row[column_name]):\n",
    "                if (type(item) is tuple) and (len(item) > 1):\n",
    "                    item = item[0]\n",
    "                if item not in items:\n",
    "                    items.append(item)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66206c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(df):\n",
    "    email_regex = '[^\\s:|()\\']+@[a-zA-Z0-9\\.]+\\.[a-zA-Z]+'\n",
    "    df['emails'] = df['Job Description'].apply(lambda x: re.findall(email_regex, x))\n",
    "    emails = get_list_of_items(df, 'emails')\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f798484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(df):\n",
    "    url_regex = '(http[s]?://(www\\.)?[A-Za-z0-9–_\\.\\-]+\\.[A-Za-z]+/?[A-Za-z0-9$\\–_\\-\\/\\.\\?]*)[\\.)\\\"]*'\n",
    "    df = get_items(df, url_regex, 'urls')\n",
    "    urls = get_list_of_items(df, 'urls')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf5838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4aba160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jsmith@quartethealth.com', 'security@quartethealth.com', 'talent@quartethealth.com', 'accommodations-ext@fb.com', 'talent@ebay.com']\n"
     ]
    }
   ],
   "source": [
    "emails = get_emails(df)\n",
    "print(emails[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4095a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.decode-m.com/', 'https://www.amazon.jobs/en/disability/us.', 'https://www.techatbloomberg.com/nlp/', 'https://bloomberg.com/company/d4gx/', 'https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm']\n"
     ]
    }
   ],
   "source": [
    "urls = get_urls(df)\n",
    "print(urls[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae427e",
   "metadata": {},
   "source": [
    "## Finding similar strings: the Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15bae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cdef9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataScientist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lavenshtein(input_string, df):\n",
    "    df['distance_to_' + input_string] = df['emails'].apply(lambda x: Levenshtein.distance(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2420cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_mail_lev(df, email):\n",
    "    df = find_lavenshtein(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    minimum_value_email_index = df[column_name].idxmin()\n",
    "    email = df.loc[minimum_value_email_index]['emails']\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a084281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails  = get_emails(df)\n",
    "new_df = pd.DataFrame(emails, columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d9931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsmith@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>security@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talent@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accommodations-ext@fb.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talent@ebay.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       emails\n",
       "0    jsmith@quartethealth.com\n",
       "1  security@quartethealth.com\n",
       "2    talent@quartethealth.com\n",
       "3   accommodations-ext@fb.com\n",
       "4             talent@ebay.com"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f6e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_mail_lev(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f372d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaro(input_string, df):\n",
    "    df['distance_to_' + input_string] = df['emails'].apply(lambda x: Levenshtein.jaro(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc30d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_email_jaro(df, email):\n",
    "    df = find_jaro(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    maximum_value_email_index = df[column_name].idxmax()\n",
    "    email = df.loc[maximum_value_email_index]['emails']\n",
    "    return email\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c29c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails = get_emails(df)\n",
    "new_df = pd.DataFrame(emails, columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf3753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_email_jaro(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c8d5226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(Levenshtein.jaro_winkler(\"rohit.mcdonald@prolim.com\",\n",
    "      \"rohit.mcdonald@prolim.org\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7fb83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298110b",
   "metadata": {},
   "source": [
    "## Performing named entity recognition using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b77345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33296bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec0d823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abfd8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6150edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"iPhone 12: Apple makes jump to 5G\n",
    "Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks. \n",
    "The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen. \n",
    "The US firm bucked a wider industry downturn by increasing its handset sales over the past year. \n",
    "But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6. \n",
    "…\n",
    "\"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,\" \n",
    "predicted Ben Wood from the consultancy CCS Insight. Apple typically unveils its new iPhones in September, but opted for a later date this year. \n",
    "It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. The firm's shares ended the day 2.7% lower. \n",
    "This has been linked to reports that several Chinese internet platforms opted not to carry the livestream, \n",
    "although it was still widely viewed and commented on via the social media network Sina Weibo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc1eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38db5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 7 9 CARDINAL\n",
      "Apple 11 16 ORG\n",
      "5 31 32 CARDINAL\n",
      "iPhone 58 64 ORG\n",
      "12 65 67 CARDINAL\n",
      "first 89 94 ORDINAL\n",
      "5 113 114 CARDINAL\n",
      "Mini 185 189 WORK_OF_ART\n",
      "5.4 216 219 CARDINAL\n",
      "US 235 237 GPE\n",
      "the past year 313 326 DATE\n",
      "Apple 372 377 ORG\n",
      "2014 416 420 DATE\n",
      "Ben Wood 643 651 PERSON\n",
      "iPhones 718 725 ORG\n",
      "September 729 738 DATE\n",
      "a later date this year 754 776 DATE\n",
      "2.7% 925 929 PERCENT\n",
      "Chinese 983 990 NORP\n",
      "Sina Weibo 1128 1138 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7dfbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78ddb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7652f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aebdca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone 12 0 9 LAW\n",
      "Apple 11 16 ORG\n",
      "5 31 32 CARDINAL\n",
      "G\n",
      "Apple 32 39 ORG\n",
      "iPhone 12 58 67 LAW\n",
      "first 89 94 ORDINAL\n",
      "5 113 114 CARDINAL\n",
      "5.4 216 219 CARDINAL\n",
      "US 235 237 GPE\n",
      "the past year 313 326 DATE\n",
      "Apple 372 377 ORG\n",
      "2014 416 420 DATE\n",
      "Ben Wood 643 651 PERSON\n",
      "CCS Insight 673 684 ORG\n",
      "Apple 686 691 ORG\n",
      "iPhones 718 725 ORG\n",
      "September 729 738 DATE\n",
      "a later date this year 754 776 DATE\n",
      "the day 917 924 DATE\n",
      "2.7% 925 929 PERCENT\n",
      "Chinese 983 990 NORP\n",
      "Sina Weibo 1128 1138 ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1e295",
   "metadata": {},
   "source": [
    "## Training your own NER model with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8adc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.language import Language\n",
    "from spacy.training.example import Example\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "964b3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data  = [\n",
    "    (\"A fakir from far-away India travels to Asterix's village and asks Cacofonix to save his land from drought since his singing can cause rain.\", \n",
    "        {'entities':[(39, 46, \"PERSON\"), (66, 75, \"PERSON\")]}),\n",
    "    (\"Cacofonix, accompanied by Asterix and Obelix, must travel to India aboard a magic carpet to save the life of the princess Orinjade, who is to be sacrificed to stop the drought.\", \n",
    "        {'entities':[(0, 9, \"PERSON\"), (26, 33, \"PERSON\"), (38, 44, \"PERSON\"), (61, 66, \"LOC\"), (122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6ad0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LABEL = \"GAULISH_WARRIOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe4c031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIFIED_DATA = [\n",
    "    (\"A fakir from far-away India travels to Asterix's village and asks Cacofonix to save his land from drought since his singing can cause rain.\", \n",
    "        {'entities':[(39, 46, NEW_LABEL), (66, 75, NEW_LABEL)]}),\n",
    "    (\"Cacofonix, accompanied by Asterix and Obelix, must travel to India aboard a magic carpet to save the life of the princess Orinjade, who is to be sacrificed to stop the drought.\", \n",
    "        {'entities':[(0, 9, NEW_LABEL), (26, 33, NEW_LABEL), (38, 44, NEW_LABEL), (61, 66, \"LOC\"), (122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97316a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 100\n",
    "OUTPUT_DIR = './model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a99e8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(nlp, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f4e8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(input_dir):\n",
    "    nlp = spacy.load(input_dir)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58e3d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model):\n",
    "    if (model is not None):\n",
    "        nlp = spacy.load(model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26633789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_to_model(nlp):\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        #ner = nlp.create_pipe(\"ner\")\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "        #nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    return (nlp, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "729907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(ner, data):\n",
    "    for sentence, annotations in data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2418066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None):\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner, Data)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(Data)\n",
    "            losses = {}\n",
    "            batches = minibatch(Data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                \n",
    "                example = []\n",
    "                # Update the model with iterating each text\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                \n",
    "                # Update the model\n",
    "                nlp.update(example, drop=0.5, losses=losses)\n",
    "                #nlp.update(texts,annotations,drop=0.5,losses=losses,)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd09307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new_entity_type(model=None):\n",
    "    random.seed(0)\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner,  MODIFIED_DATA)\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(MODIFIED_DATA)\n",
    "            batches = minibatch(MODIFIED_DATA, size=sizes)\n",
    "            losses = {}\n",
    "\n",
    "            for batch in batches:\n",
    "                example = []\n",
    "                texts, annotations = zip(*batch)\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                # Update the model\n",
    "                nlp.update(example, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "                #nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04fe858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(nlp, data):\n",
    "    for text, annotations in data:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "153f607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_training(data=Data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    test_model(nlp, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8659eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India 22 27 GPE\n",
      "Asterix 39 46 GPE\n",
      "Cacofonix 66 75 PERSON\n",
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 GPE\n",
      "Obelix 38 44 GPE\n",
      "India 61 66 GPE\n"
     ]
    }
   ],
   "source": [
    "without_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb62b410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 56.69999474287033}\n",
      "Losses {'ner': 54.65525949001312}\n",
      "Losses {'ner': 52.21192675828934}\n",
      "Losses {'ner': 49.51241624355316}\n",
      "Losses {'ner': 46.80381381511688}\n",
      "Losses {'ner': 42.628850281238556}\n",
      "Losses {'ner': 37.85238900780678}\n",
      "Losses {'ner': 33.73577493429184}\n",
      "Losses {'ner': 29.69480389356613}\n",
      "Losses {'ner': 22.596926644444466}\n",
      "Losses {'ner': 21.58876144886017}\n",
      "Losses {'ner': 17.270310796797276}\n",
      "Losses {'ner': 15.028922208584845}\n",
      "Losses {'ner': 12.894236700609326}\n",
      "Losses {'ner': 12.03823997383006}\n",
      "Losses {'ner': 12.263488875469193}\n",
      "Losses {'ner': 11.612107684282819}\n",
      "Losses {'ner': 12.792779989271367}\n",
      "Losses {'ner': 10.861707160034712}\n",
      "Losses {'ner': 11.740244571854419}\n",
      "Losses {'ner': 10.405306547995224}\n",
      "Losses {'ner': 12.098870538786286}\n",
      "Losses {'ner': 10.670357074843196}\n",
      "Losses {'ner': 10.84322915098528}\n",
      "Losses {'ner': 8.765899955840723}\n",
      "Losses {'ner': 8.99649038881762}\n",
      "Losses {'ner': 7.267627827266551}\n",
      "Losses {'ner': 8.23463920148788}\n",
      "Losses {'ner': 9.245679128798656}\n",
      "Losses {'ner': 6.677640905545559}\n",
      "Losses {'ner': 7.142850541677035}\n",
      "Losses {'ner': 7.09621848736424}\n",
      "Losses {'ner': 4.762896219617687}\n",
      "Losses {'ner': 7.986275227405713}\n",
      "Losses {'ner': 6.986748158709133}\n",
      "Losses {'ner': 4.281525792388493}\n",
      "Losses {'ner': 7.636853939343723}\n",
      "Losses {'ner': 5.764851586479196}\n",
      "Losses {'ner': 4.894471790461466}\n",
      "Losses {'ner': 4.669479360271396}\n",
      "Losses {'ner': 5.10368761867921}\n",
      "Losses {'ner': 3.8358384364142353}\n",
      "Losses {'ner': 2.8255370683267347}\n",
      "Losses {'ner': 1.8356816793626578}\n",
      "Losses {'ner': 1.7701229467612194}\n",
      "Losses {'ner': 1.5276717237303419}\n",
      "Losses {'ner': 2.9251276348866666}\n",
      "Losses {'ner': 4.050560313999513}\n",
      "Losses {'ner': 1.2683169492719037}\n",
      "Losses {'ner': 1.6097503264790798}\n",
      "Losses {'ner': 3.33266159217297}\n",
      "Losses {'ner': 1.8318980985257922}\n",
      "Losses {'ner': 1.7127505416910287}\n",
      "Losses {'ner': 2.490269160571598}\n",
      "Losses {'ner': 1.410815514643431}\n",
      "Losses {'ner': 1.011151678138675}\n",
      "Losses {'ner': 1.486424007267935}\n",
      "Losses {'ner': 1.5913145626795313}\n",
      "Losses {'ner': 1.0024212883708676}\n",
      "Losses {'ner': 1.7324720740256754}\n",
      "Losses {'ner': 0.6383832728472598}\n",
      "Losses {'ner': 1.3384929896881212}\n",
      "Losses {'ner': 1.2832192734263688}\n",
      "Losses {'ner': 1.000018915198099}\n",
      "Losses {'ner': 1.871480383825168}\n",
      "Losses {'ner': 0.3983696456946621}\n",
      "Losses {'ner': 0.54189319301171}\n",
      "Losses {'ner': 0.6964695683552643}\n",
      "Losses {'ner': 0.08247847082266425}\n",
      "Losses {'ner': 0.5175575577847296}\n",
      "Losses {'ner': 1.0055564362808158}\n",
      "Losses {'ner': 0.40738223205669877}\n",
      "Losses {'ner': 0.36478184848649214}\n",
      "Losses {'ner': 0.27031344954693065}\n",
      "Losses {'ner': 1.9851495584492045}\n",
      "Losses {'ner': 0.19292638623220149}\n",
      "Losses {'ner': 0.11725360330235429}\n",
      "Losses {'ner': 0.251261048567978}\n",
      "Losses {'ner': 0.2931403205890501}\n",
      "Losses {'ner': 0.74000974347564}\n",
      "Losses {'ner': 0.07697950879898036}\n",
      "Losses {'ner': 0.13013694887606636}\n",
      "Losses {'ner': 1.9527125044776341}\n",
      "Losses {'ner': 1.3927090457227935}\n",
      "Losses {'ner': 0.0038503228103319662}\n",
      "Losses {'ner': 0.011077531805226094}\n",
      "Losses {'ner': 1.8388452275083353}\n",
      "Losses {'ner': 0.004069801786824186}\n",
      "Losses {'ner': 1.003022295238609}\n",
      "Losses {'ner': 0.06158354711121031}\n",
      "Losses {'ner': 0.021665113674766284}\n",
      "Losses {'ner': 0.004054797806972226}\n",
      "Losses {'ner': 0.011792776989253543}\n",
      "Losses {'ner': 0.000827782479550236}\n",
      "Losses {'ner': 0.00759283831896994}\n",
      "Losses {'ner': 9.730829428231431e-05}\n",
      "Losses {'ner': 0.00447188977886417}\n",
      "Losses {'ner': 0.09508101590700917}\n",
      "Losses {'ner': 4.947548101823628e-05}\n",
      "Losses {'ner': 0.0007963169393038484}\n",
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 PERSON\n",
      "Obelix 38 44 PERSON\n",
      "India 61 66 LOC\n",
      "Orinjade 122 130 PERSON\n",
      "Asterix 39 46 PERSON\n",
      "Cacofonix 66 75 PERSON\n"
     ]
    }
   ],
   "source": [
    "#without_training()\n",
    "model = \"en_core_web_sm\"\n",
    "#nlp = train_model(model)\n",
    "nlp = train_model()\n",
    "#nlp = train_model_new_entity_type(model)\n",
    "test_model(nlp, Data)\n",
    "save_model(nlp, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5569522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 13.833397086210384}\n",
      "Losses {'ner': 12.06066074300557}\n",
      "Losses {'ner': 10.277722794286394}\n",
      "Losses {'ner': 11.12566416977232}\n",
      "Losses {'ner': 10.030372032258466}\n",
      "Losses {'ner': 9.221372415713452}\n",
      "Losses {'ner': 6.209818925285421}\n",
      "Losses {'ner': 7.119223205799258}\n",
      "Losses {'ner': 7.007090452450164}\n",
      "Losses {'ner': 7.167410578372828}\n",
      "Losses {'ner': 9.190571758886474}\n",
      "Losses {'ner': 7.210321693976583}\n",
      "Losses {'ner': 7.938652887215326}\n",
      "Losses {'ner': 7.462402515182248}\n",
      "Losses {'ner': 6.0228200492836095}\n",
      "Losses {'ner': 5.074052806686353}\n",
      "Losses {'ner': 6.028645019086298}\n",
      "Losses {'ner': 4.106114403799211}\n",
      "Losses {'ner': 5.262888969964024}\n",
      "Losses {'ner': 3.4062218435029763}\n",
      "Losses {'ner': 2.703608875707914}\n",
      "Losses {'ner': 2.166068909361911}\n",
      "Losses {'ner': 0.5908334210672024}\n",
      "Losses {'ner': 0.32384065843127985}\n",
      "Losses {'ner': 0.4206208965498135}\n",
      "Losses {'ner': 0.1302787805054968}\n",
      "Losses {'ner': 0.026190293190013758}\n",
      "Losses {'ner': 0.013180213722610745}\n",
      "Losses {'ner': 0.2451141320226471}\n",
      "Losses {'ner': 0.007819687759697283}\n",
      "Losses {'ner': 0.0011259627743123027}\n",
      "Losses {'ner': 0.001077284417884551}\n",
      "Losses {'ner': 0.07931323379047957}\n",
      "Losses {'ner': 0.0004129843156727836}\n",
      "Losses {'ner': 0.004961133466952984}\n",
      "Losses {'ner': 0.00606341889198383}\n",
      "Losses {'ner': 2.9735635618313275e-05}\n",
      "Losses {'ner': 0.0003009796668665502}\n",
      "Losses {'ner': 0.0004160649358612569}\n",
      "Losses {'ner': 0.07331260105624798}\n",
      "Losses {'ner': 4.825157766487696e-06}\n",
      "Losses {'ner': 1.675595342886559e-05}\n",
      "Losses {'ner': 3.8775727474746994e-05}\n",
      "Losses {'ner': 2.366771196822191e-07}\n",
      "Losses {'ner': 1.151609501428452e-06}\n",
      "Losses {'ner': 1.276410696871996e-07}\n",
      "Losses {'ner': 4.9836528699985115e-06}\n",
      "Losses {'ner': 3.51209618440293e-06}\n",
      "Losses {'ner': 1.3777720857372594e-05}\n",
      "Losses {'ner': 1.739984275178215e-05}\n",
      "Losses {'ner': 0.0005002145722501188}\n",
      "Losses {'ner': 1.9977682293149112e-05}\n",
      "Losses {'ner': 1.4261343667870161e-07}\n",
      "Losses {'ner': 0.001597562863798554}\n",
      "Losses {'ner': 2.2668477023168376e-05}\n",
      "Losses {'ner': 9.836026110080964e-07}\n",
      "Losses {'ner': 0.0002944439659969868}\n",
      "Losses {'ner': 0.006223125448576939}\n",
      "Losses {'ner': 1.4187483873934805e-05}\n",
      "Losses {'ner': 6.037814616806273e-06}\n",
      "Losses {'ner': 2.1112023769911713e-07}\n",
      "Losses {'ner': 3.3774167477189894e-07}\n",
      "Losses {'ner': 1.4345578136589633e-07}\n",
      "Losses {'ner': 7.057651255693236e-07}\n",
      "Losses {'ner': 2.904095836862369e-05}\n",
      "Losses {'ner': 0.0030817518201189035}\n",
      "Losses {'ner': 0.0024704634794769087}\n",
      "Losses {'ner': 1.124323077860697e-06}\n",
      "Losses {'ner': 1.7863337316943657e-06}\n",
      "Losses {'ner': 9.785549073689733e-08}\n",
      "Losses {'ner': 1.2995766661976267e-09}\n",
      "Losses {'ner': 4.3124857415294616e-07}\n",
      "Losses {'ner': 5.388053591107613e-07}\n",
      "Losses {'ner': 6.720250827543142e-06}\n",
      "Losses {'ner': 6.722390267967344e-07}\n",
      "Losses {'ner': 1.2492670710073121e-06}\n",
      "Losses {'ner': 1.6422087086798394e-07}\n",
      "Losses {'ner': 2.7112988154977363e-06}\n",
      "Losses {'ner': 1.1979789135829226e-07}\n",
      "Losses {'ner': 2.1312715324123444e-06}\n",
      "Losses {'ner': 1.0719812308609048e-06}\n",
      "Losses {'ner': 8.665173325681626e-08}\n",
      "Losses {'ner': 0.0003585291614949703}\n",
      "Losses {'ner': 2.596850073227448e-07}\n",
      "Losses {'ner': 5.595959700203719e-07}\n",
      "Losses {'ner': 2.400550838550402e-08}\n",
      "Losses {'ner': 5.6096394355125e-07}\n",
      "Losses {'ner': 5.999714360827206e-08}\n",
      "Losses {'ner': 5.188417042758134e-07}\n",
      "Losses {'ner': 8.189670294202858e-07}\n",
      "Losses {'ner': 2.155641583287416e-07}\n",
      "Losses {'ner': 4.518729436299118e-08}\n",
      "Losses {'ner': 5.229099771713204e-07}\n",
      "Losses {'ner': 1.8959362546229248e-08}\n",
      "Losses {'ner': 3.2006585164820137e-09}\n",
      "Losses {'ner': 4.2435129841776235e-08}\n",
      "Losses {'ner': 1.8736670731045047e-07}\n",
      "Losses {'ner': 1.166737676402988e-05}\n",
      "Losses {'ner': 5.872752326363992e-07}\n",
      "Losses {'ner': 2.993128853143354e-07}\n",
      "Asterix 39 46 GAULISH_WARRIOR\n",
      "Cacofonix 66 75 GAULISH_WARRIOR\n",
      "Cacofonix 0 9 GAULISH_WARRIOR\n",
      "Asterix 26 33 GAULISH_WARRIOR\n",
      "Obelix 38 44 GAULISH_WARRIOR\n",
      "India 61 66 LOC\n",
      "Orinjade 122 130 PERSON\n"
     ]
    }
   ],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "nlp = train_model_new_entity_type(model)\n",
    "test_model(nlp, MODIFIED_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ff385",
   "metadata": {},
   "source": [
    "## Discovering sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c29ada99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bat/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19c2ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ca59319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love going to school!\", \"I hate going to school!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e00101aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8621afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_sentiment(sentence):\n",
    "    result = TextBlob(sentence).sentiment\n",
    "    print(sentence, result.polarity)\n",
    "    return result.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2597a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nltk_sentiment(sentence):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(sentence, ss['compound'])\n",
    "    return ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49832046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.6696\n",
      "I hate going to school! -0.6114\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_nltk_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f72627b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.625\n",
      "I hate going to school! -1.0\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_blob_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67452310",
   "metadata": {},
   "source": [
    "### Sentiment for short texts using LSTM: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85400265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from wordseg import segment\n",
    "import html\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#from Chapter04.lstm_classification import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6953966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2dd026b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 50000\n",
    "EMBEDDING_DIM = 500\n",
    "twitter_csv = \"training.1600000.processed.noemoticon.csv\"\n",
    "english_twitter = \"./twitter_english.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3728ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82039f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(df, save_path):\n",
    "    df['language'] = df['tweet'].progress_apply(lambda t: lang_detect(t))\n",
    "    df = df[df['language']=='en']\n",
    "    df.to_csv(save_path, encoding='latin1', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78fb72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, save_path='./data.csv', num_datapoints=80000):\n",
    "    df = pd.read_csv(filename, encoding='latin1')\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query','username', 'tweet']\n",
    "    df = pd.concat([df.head(num_datapoints), df.tail(num_datapoints)])\n",
    "    df = filter_english(df, save_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ae8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(text):\n",
    "    lang = \"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"None\"\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea1c2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hashtags(tweet):\n",
    "    matches = re.findall(r'#[a-z0-9]+', tweet)\n",
    "    for match in matches:\n",
    "        tweet = re.sub(match, ' '.join(segment(match)[0]), tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d981cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #Lowercase all tweets\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: t.lower())\n",
    "    #Decode HTML\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: html.unescape(t))\n",
    "    #Remove @ mentions\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(r'@[A-Za-z0-9]+','',t))\n",
    "    #Remove URLs\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t:re.sub('https?://[A-Za-z0-9./]+','',t))\n",
    "    #Segment hashtags\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: segment_hashtags(t))\n",
    "    #Remove remaining non-alpha characters\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(\"[^a-zA-Z]\", \" \", t))\n",
    "    #Re-label positive tweets with 1 instead of 4\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda t: 1 if t==4 else t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a561c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1,0)\n",
    "    y_pred = [pred[0] for pred in y_pred]\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a2e3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(df['tweet'].values)\n",
    "    save_tokenizer(tokenizer, 'twitter_tokenizer.pkl')\n",
    "    X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "    X = pad_sequences(X)\n",
    "    y = df['sentiment'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n",
    "                                                        random_state=42, stratify=df['sentiment'])\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    epochs = 7\n",
    "    batch_size = 64\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=es)\n",
    "    \n",
    "    accr = model.evaluate(X_test, y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    model.save('twitter_model.h5')\n",
    "    evaluate(model, X_test, Y_test)\n",
    "    plot_model(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c051b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27dee65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training.1600000.processed.noemoticon.csv'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c0522f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e40a8b711141599454f06b76863964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_data(twitter_csv, 'twitter_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29a89d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebb6e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b48af54c7d94eee849e0fe8f6e191bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9bbfc3aa2d4dd0a23abee56eb1c013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1af7aed17441f98d1600a6b41910ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1568c663b7f64d31b323f7d76eaa1c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7c5ea20a84452288007142ac268fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef1f4671b3b4a03b5f90a257523f6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/148617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "  67/1301 [>.............................] - ETA: 13:15 - loss: 0.6931 - accuracy: 0.5065"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-08796fcdcc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-671075e46196>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0maccr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425cde6",
   "metadata": {},
   "source": [
    "## Using BERT for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_column_name, gold_column_name, test_percent):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[train_column_name], df[gold_column_name], test_size=test_percent, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #Lowercase all tweets\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: t.lower())\n",
    "    #Decode HTML\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: html.unescape(t))\n",
    "    #Remove @ mentions\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(r'@[A-Za-z0-9]+','',t))\n",
    "    #Remove URLs\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub('https?://[A-Za-z0-9./]+','',t))\n",
    "    #Segment hashtags\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: segment_hashtags(t))\n",
    "    #Remove remaining non-alpha characters\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(\"[^a-zA-Z]\", \" \", t))\n",
    "    #Re-label positive tweets with 1 instead of 4\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda t: 1 if t==4 else t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f77c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "DATASET_SIZE = 4000\n",
    "english_twitter = \"./twitter_english.csv\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['tweet']\n",
    "        label = row['sentiment']\n",
    "        bert_input = tokenizer.encode_plus(\n",
    "                        tweet,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = max_length, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "            )        \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_inputs_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0970690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_inputs_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\"input_ids\": input_ids,\"token_type_ids\": token_type_ids,\"attention_mask\": attention_masks,}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b84ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, size=int(DATASET_SIZE/2)):\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat([df.head(size),df.tail(size)])\n",
    "    df = df.sample(frac = 1)\n",
    "    ds = encode_data(df)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_val_datasets(ds, size=DATASET_SIZE, batch_size=batch_size):\n",
    "    ds.shuffle(32)\n",
    "    train_size = int(0.7 * size)\n",
    "    val_size = int(0.15 * size)\n",
    "    test_size = int(0.15 * size)\n",
    "    train_dataset = ds.take(train_size).batch(batch_size)\n",
    "    test_dataset = ds.skip(train_size)\n",
    "    val_dataset = test_dataset.skip(test_size).batch(batch_size)\n",
    "    test_dataset = test_dataset.take(test_size).batch(batch_size)\n",
    "    return (train_dataset, test_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6251af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(ds, export_dir):\n",
    "    (train_dataset, test_dataset, val_dataset) = get_test_train_val_datasets(ds)\n",
    "    learning_rate = 2e-5\n",
    "    number_of_epochs = 1\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    bert_history = model.fit(train_dataset, epochs=number_of_epochs, validation_data=val_dataset)\n",
    "    model.save_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1360a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_model(export_dir):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_existing_file(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[['tweet', 'sentiment']]\n",
    "    return df\n",
    "\n",
    "#df = read_existing_file(english_twitter)\n",
    "#df = get_data(english_twitter,\"twitter_english.csv\")\n",
    "#df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996be22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fine_tune_model(dataset,'./bert_twitter_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf47abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(input_text):\n",
    "    tokenized = tokenizer.tokenize(input_text)\n",
    "    bert_input = tokenizer.encode_plus(\n",
    "                    input_text,                      \n",
    "                    add_special_tokens = True, # add [CLS], [SEP]\n",
    "                    max_length = max_length, # max length of the text that can go to BERT\n",
    "                    pad_to_max_length = True, # add [PAD] tokens\n",
    "                    return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "                    return_tensors='tf'\n",
    "        )\n",
    "    return bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a30bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(model_path, tweet):\n",
    "    model = load_existing_model(model_path)\n",
    "    bert_input = encode_example(tweet)\n",
    "    tf_output = model.predict([bert_input['input_ids'], \n",
    "                              bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "    tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "    new_label = np.argmax(tf_pred, axis=-1)\n",
    "    print(new_label)\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_example('./bert_twitter_model', 'I hate going to the school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = []\n",
    "    for tweet in X_test:\n",
    "        bert_input = encode_example(tweet)\n",
    "        tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "        tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "        new_label = np.argmax(tf_pred, axis=-1)\n",
    "        y_pred.append(new_label)\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_existing_model(export_dir, num_points=200):\n",
    "    model = load_existing_model(export_dir)\n",
    "    df = read_existing_file(english_twitter)\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat([df.head(num_points),df.tail(num_points)])\n",
    "    (X_train, X_test, y_train, y_test) = split_dataset(df, 'tweet', 'sentiment',20)\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_evaluate_existing_model('bert_twitter_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
