{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83f0cce",
   "metadata": {},
   "source": [
    "### Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e33953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas python-Levenshtein spacy nltk textblob tqdm transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796f4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "data_file = 'DataScientist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e29d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "      <th>Easy Apply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hopper\\n3.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Travel Agencies</td>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>At Noom, we use scientifically proven methods ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Noom US\\n4.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>2008</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Health, Beauty, &amp; Fitness</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                          Job Title  \\\n",
       "0           0      0              Senior Data Scientist   \n",
       "1           1      1  Data Scientist, Product Analytics   \n",
       "\n",
       "                Salary Estimate  \\\n",
       "0  $111K-$181K (Glassdoor est.)   \n",
       "1  $111K-$181K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  Company Name  \\\n",
       "0  ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission ...     3.5   Hopper\\n3.5   \n",
       "1  At Noom, we use scientifically proven methods ...     4.5  Noom US\\n4.5   \n",
       "\n",
       "       Location      Headquarters                    Size  Founded  \\\n",
       "0  New York, NY  Montreal, Canada   501 to 1000 employees     2007   \n",
       "1  New York, NY      New York, NY  1001 to 5000 employees     2008   \n",
       "\n",
       "   Type of ownership                   Industry             Sector  \\\n",
       "0  Company - Private            Travel Agencies   Travel & Tourism   \n",
       "1  Company - Private  Health, Beauty, & Fitness  Consumer Services   \n",
       "\n",
       "                    Revenue Competitors Easy Apply  \n",
       "0  Unknown / Non-Applicable          -1         -1  \n",
       "1  Unknown / Non-Applicable          -1         -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1074400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(df, regex, column_name):\n",
    "    df[column_name] = df['Job Description'].apply(lambda x:re.findall(regex, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b3c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_items(df, column_name):\n",
    "    items = []\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row[column_name]) > 0:\n",
    "            for item in list(row[column_name]):\n",
    "                if (type(item) is tuple) and (len(item) > 1):\n",
    "                    item = item[0]\n",
    "                if item not in items:\n",
    "                    items.append(item)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66206c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(df):\n",
    "    email_regex = '[^\\s:|()\\']+@[a-zA-Z0-9\\.]+\\.[a-zA-Z]+'\n",
    "    df['emails'] = df['Job Description'].apply(lambda x: re.findall(email_regex, x))\n",
    "    emails = get_list_of_items(df, 'emails')\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f798484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(df):\n",
    "    url_regex = '(http[s]?://(www\\.)?[A-Za-z0-9–_\\.\\-]+\\.[A-Za-z]+/?[A-Za-z0-9$\\–_\\-\\/\\.\\?]*)[\\.)\\\"]*'\n",
    "    df = get_items(df, url_regex, 'urls')\n",
    "    urls = get_list_of_items(df, 'urls')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf5838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4aba160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jsmith@quartethealth.com', 'security@quartethealth.com', 'talent@quartethealth.com', 'accommodations-ext@fb.com', 'talent@ebay.com']\n"
     ]
    }
   ],
   "source": [
    "emails = get_emails(df)\n",
    "print(emails[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4095a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.decode-m.com/', 'https://www.amazon.jobs/en/disability/us.', 'https://www.techatbloomberg.com/nlp/', 'https://bloomberg.com/company/d4gx/', 'https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm']\n"
     ]
    }
   ],
   "source": [
    "urls = get_urls(df)\n",
    "print(urls[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae427e",
   "metadata": {},
   "source": [
    "## Finding similar strings: the Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15bae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cdef9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'DataScientist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lavenshtein(input_string, df):\n",
    "    df['distance_to_' + input_string] = df['emails'].apply(lambda x: Levenshtein.distance(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2420cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_mail_lev(df, email):\n",
    "    df = find_lavenshtein(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    minimum_value_email_index = df[column_name].idxmin()\n",
    "    email = df.loc[minimum_value_email_index]['emails']\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a084281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails  = get_emails(df)\n",
    "new_df = pd.DataFrame(emails, columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d9931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsmith@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>security@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talent@quartethealth.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accommodations-ext@fb.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talent@ebay.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       emails\n",
       "0    jsmith@quartethealth.com\n",
       "1  security@quartethealth.com\n",
       "2    talent@quartethealth.com\n",
       "3   accommodations-ext@fb.com\n",
       "4             talent@ebay.com"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f6e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_mail_lev(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f372d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaro(input_string, df):\n",
    "    df['distance_to_' + input_string] = df['emails'].apply(lambda x: Levenshtein.jaro(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc30d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_email_jaro(df, email):\n",
    "    df = find_jaro(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    maximum_value_email_index = df[column_name].idxmax()\n",
    "    email = df.loc[maximum_value_email_index]['emails']\n",
    "    return email\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c29c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails = get_emails(df)\n",
    "new_df = pd.DataFrame(emails, columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf3753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_email_jaro(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c8d5226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(Levenshtein.jaro_winkler(\"rohit.mcdonald@prolim.com\",\n",
    "      \"rohit.mcdonald@prolim.org\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7fb83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298110b",
   "metadata": {},
   "source": [
    "## Performing named entity recognition using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b77345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33296bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec0d823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abfd8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6150edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"iPhone 12: Apple makes jump to 5G\n",
    "Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks. \n",
    "The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen. \n",
    "The US firm bucked a wider industry downturn by increasing its handset sales over the past year. \n",
    "But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6. \n",
    "…\n",
    "\"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,\" \n",
    "predicted Ben Wood from the consultancy CCS Insight. Apple typically unveils its new iPhones in September, but opted for a later date this year. \n",
    "It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. The firm's shares ended the day 2.7% lower. \n",
    "This has been linked to reports that several Chinese internet platforms opted not to carry the livestream, \n",
    "although it was still widely viewed and commented on via the social media network Sina Weibo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc1eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38db5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 7 9 CARDINAL\n",
      "Apple 11 16 ORG\n",
      "5 31 32 CARDINAL\n",
      "iPhone 58 64 ORG\n",
      "12 65 67 CARDINAL\n",
      "first 89 94 ORDINAL\n",
      "5 113 114 CARDINAL\n",
      "Mini 185 189 WORK_OF_ART\n",
      "5.4 216 219 CARDINAL\n",
      "US 235 237 GPE\n",
      "the past year 313 326 DATE\n",
      "Apple 372 377 ORG\n",
      "2014 416 420 DATE\n",
      "Ben Wood 643 651 PERSON\n",
      "iPhones 718 725 ORG\n",
      "September 729 738 DATE\n",
      "a later date this year 754 776 DATE\n",
      "2.7% 925 929 PERCENT\n",
      "Chinese 983 990 NORP\n",
      "Sina Weibo 1128 1138 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7dfbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78ddb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7652f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aebdca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone 12 0 9 LAW\n",
      "Apple 11 16 ORG\n",
      "5 31 32 CARDINAL\n",
      "G\n",
      "Apple 32 39 ORG\n",
      "iPhone 12 58 67 LAW\n",
      "first 89 94 ORDINAL\n",
      "5 113 114 CARDINAL\n",
      "5.4 216 219 CARDINAL\n",
      "US 235 237 GPE\n",
      "the past year 313 326 DATE\n",
      "Apple 372 377 ORG\n",
      "2014 416 420 DATE\n",
      "Ben Wood 643 651 PERSON\n",
      "CCS Insight 673 684 ORG\n",
      "Apple 686 691 ORG\n",
      "iPhones 718 725 ORG\n",
      "September 729 738 DATE\n",
      "a later date this year 754 776 DATE\n",
      "the day 917 924 DATE\n",
      "2.7% 925 929 PERCENT\n",
      "Chinese 983 990 NORP\n",
      "Sina Weibo 1128 1138 ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1e295",
   "metadata": {},
   "source": [
    "## Training your own NER model with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8adc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.language import Language\n",
    "from spacy.training.example import Example\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "964b3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data  = [\n",
    "    (\"A fakir from far-away India travels to Asterix's village and asks Cacofonix to save his land from drought since his singing can cause rain.\", \n",
    "        {'entities':[(39, 46, \"PERSON\"), (66, 75, \"PERSON\")]}),\n",
    "    (\"Cacofonix, accompanied by Asterix and Obelix, must travel to India aboard a magic carpet to save the life of the princess Orinjade, who is to be sacrificed to stop the drought.\", \n",
    "        {'entities':[(0, 9, \"PERSON\"), (26, 33, \"PERSON\"), (38, 44, \"PERSON\"), (61, 66, \"LOC\"), (122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6ad0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LABEL = \"GAULISH_WARRIOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe4c031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIFIED_DATA = [\n",
    "    (\"A fakir from far-away India travels to Asterix's village and asks Cacofonix to save his land from drought since his singing can cause rain.\", \n",
    "        {'entities':[(39, 46, NEW_LABEL), (66, 75, NEW_LABEL)]}),\n",
    "    (\"Cacofonix, accompanied by Asterix and Obelix, must travel to India aboard a magic carpet to save the life of the princess Orinjade, who is to be sacrificed to stop the drought.\", \n",
    "        {'entities':[(0, 9, NEW_LABEL), (26, 33, NEW_LABEL), (38, 44, NEW_LABEL), (61, 66, \"LOC\"), (122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97316a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 100\n",
    "OUTPUT_DIR = './model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a99e8e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(nlp, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f4e8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(input_dir):\n",
    "    nlp = spacy.load(input_dir)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58e3d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model):\n",
    "    if (model is not None):\n",
    "        nlp = spacy.load(model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26633789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_to_model(nlp):\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        #ner = nlp.create_pipe(\"ner\")\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "        #nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    return (nlp, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "729907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(ner, data):\n",
    "    for sentence, annotations in data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2418066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None):\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner, Data)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(Data)\n",
    "            losses = {}\n",
    "            batches = minibatch(Data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                \n",
    "                example = []\n",
    "                # Update the model with iterating each text\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                \n",
    "                # Update the model\n",
    "                nlp.update(example, drop=0.5, losses=losses)\n",
    "                #nlp.update(texts,annotations,drop=0.5,losses=losses,)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd09307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new_entity_type(model=None):\n",
    "    random.seed(0)\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner,  MODIFIED_DATA)\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(MODIFIED_DATA)\n",
    "            batches = minibatch(MODIFIED_DATA, size=sizes)\n",
    "            losses = {}\n",
    "\n",
    "            for batch in batches:\n",
    "                example = []\n",
    "                texts, annotations = zip(*batch)\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                # Update the model\n",
    "                nlp.update(example, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "                #nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04fe858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(nlp, data):\n",
    "    for text, annotations in data:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "153f607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_training(data=Data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    test_model(nlp, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8659eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India 22 27 GPE\n",
      "Asterix 39 46 GPE\n",
      "Cacofonix 66 75 PERSON\n",
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 GPE\n",
      "Obelix 38 44 GPE\n",
      "India 61 66 GPE\n"
     ]
    }
   ],
   "source": [
    "without_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb62b410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 56.69999474287033}\n",
      "Losses {'ner': 54.884569227695465}\n",
      "Losses {'ner': 52.86061328649521}\n",
      "Losses {'ner': 50.27370989322662}\n",
      "Losses {'ner': 47.39870876073837}\n",
      "Losses {'ner': 44.05413407087326}\n",
      "Losses {'ner': 40.115330934524536}\n",
      "Losses {'ner': 35.70743525028229}\n",
      "Losses {'ner': 30.579927146434784}\n",
      "Losses {'ner': 26.77979999780655}\n",
      "Losses {'ner': 21.51287230104208}\n",
      "Losses {'ner': 18.088320158421993}\n",
      "Losses {'ner': 14.24777695350349}\n",
      "Losses {'ner': 14.539475839119405}\n",
      "Losses {'ner': 12.0304206085857}\n",
      "Losses {'ner': 11.881406278931536}\n",
      "Losses {'ner': 12.785370905221498}\n",
      "Losses {'ner': 11.276580670262774}\n",
      "Losses {'ner': 11.762214268601383}\n",
      "Losses {'ner': 13.435343387875037}\n",
      "Losses {'ner': 11.225084163806969}\n",
      "Losses {'ner': 10.514982813161623}\n",
      "Losses {'ner': 12.101529440493323}\n",
      "Losses {'ner': 11.35916692308274}\n",
      "Losses {'ner': 9.195317626654287}\n",
      "Losses {'ner': 9.15186405768327}\n",
      "Losses {'ner': 8.266794491864857}\n",
      "Losses {'ner': 8.196629001642577}\n",
      "Losses {'ner': 7.530735152831767}\n",
      "Losses {'ner': 8.034987869817996}\n",
      "Losses {'ner': 6.4466545427712845}\n",
      "Losses {'ner': 6.590631624225352}\n",
      "Losses {'ner': 6.488058167335112}\n",
      "Losses {'ner': 6.199558827349392}\n",
      "Losses {'ner': 6.310919030403966}\n",
      "Losses {'ner': 4.430980081648499}\n",
      "Losses {'ner': 4.86551601647534}\n",
      "Losses {'ner': 4.055164784725164}\n",
      "Losses {'ner': 4.197485874837298}\n",
      "Losses {'ner': 3.0949458946693653}\n",
      "Losses {'ner': 2.5246893871031233}\n",
      "Losses {'ner': 4.371634549257321}\n",
      "Losses {'ner': 2.609505405564981}\n",
      "Losses {'ner': 4.021383951897659}\n",
      "Losses {'ner': 3.038635423926598}\n",
      "Losses {'ner': 2.5348004037974126}\n",
      "Losses {'ner': 2.3037012387754716}\n",
      "Losses {'ner': 3.2827747302747867}\n",
      "Losses {'ner': 1.5485822231886504}\n",
      "Losses {'ner': 1.8864469467706924}\n",
      "Losses {'ner': 1.1649325974965152}\n",
      "Losses {'ner': 1.8923942859264986}\n",
      "Losses {'ner': 1.2380227046390355}\n",
      "Losses {'ner': 1.077667331824541}\n",
      "Losses {'ner': 1.957334255056491}\n",
      "Losses {'ner': 0.7867551016589139}\n",
      "Losses {'ner': 2.8162391915230502}\n",
      "Losses {'ner': 1.2758159355117453}\n",
      "Losses {'ner': 0.5631408514002875}\n",
      "Losses {'ner': 1.03758633983447}\n",
      "Losses {'ner': 1.7045583042561625}\n",
      "Losses {'ner': 1.5732757162296325}\n",
      "Losses {'ner': 1.4603948259303217}\n",
      "Losses {'ner': 0.17834118893466633}\n",
      "Losses {'ner': 0.4873748631750418}\n",
      "Losses {'ner': 2.086575480678106}\n",
      "Losses {'ner': 0.819744559511062}\n",
      "Losses {'ner': 0.7514226933156785}\n",
      "Losses {'ner': 0.0959368724900405}\n",
      "Losses {'ner': 0.5559724195793656}\n",
      "Losses {'ner': 0.036624109137675075}\n",
      "Losses {'ner': 0.32878115209887504}\n",
      "Losses {'ner': 1.3178706429858644}\n",
      "Losses {'ner': 0.2622861755625914}\n",
      "Losses {'ner': 0.09656887853752796}\n",
      "Losses {'ner': 0.05712893541154649}\n",
      "Losses {'ner': 0.01452808499265063}\n",
      "Losses {'ner': 0.3126014106776801}\n",
      "Losses {'ner': 0.12573746990228674}\n",
      "Losses {'ner': 0.2658414871575919}\n",
      "Losses {'ner': 1.4504725718644518}\n",
      "Losses {'ner': 0.006025079581845721}\n",
      "Losses {'ner': 0.0007197830274064492}\n",
      "Losses {'ner': 0.0014473437632519207}\n",
      "Losses {'ner': 0.005958140068928984}\n",
      "Losses {'ner': 0.026364242752864545}\n",
      "Losses {'ner': 0.04850514430973649}\n",
      "Losses {'ner': 5.1367331534489326e-05}\n",
      "Losses {'ner': 0.00016997395008367473}\n",
      "Losses {'ner': 0.0035506744570826363}\n",
      "Losses {'ner': 2.8107659657511085e-06}\n",
      "Losses {'ner': 0.11479593980958348}\n",
      "Losses {'ner': 3.446505131194581e-06}\n",
      "Losses {'ner': 1.6515441724813376e-06}\n",
      "Losses {'ner': 5.3417101725666744e-06}\n",
      "Losses {'ner': 0.0009685117198325213}\n",
      "Losses {'ner': 0.00020326813750117678}\n",
      "Losses {'ner': 1.038326428935459e-07}\n",
      "Losses {'ner': 2.336798129575133e-09}\n",
      "Losses {'ner': 0.0004816405223421132}\n",
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 PERSON\n",
      "Obelix 38 44 PERSON\n",
      "India 61 66 LOC\n",
      "Orinjade 122 130 PERSON\n",
      "Asterix 39 46 PERSON\n",
      "Cacofonix 66 75 PERSON\n"
     ]
    }
   ],
   "source": [
    "#without_training()\n",
    "model = \"en_core_web_sm\"\n",
    "#nlp = train_model(model)\n",
    "nlp = train_model()\n",
    "#nlp = train_model_new_entity_type(model)\n",
    "test_model(nlp, Data)\n",
    "save_model(nlp, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5569522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 13.301665338898532}\n",
      "Losses {'ner': 10.678101060874738}\n",
      "Losses {'ner': 11.772821921631962}\n",
      "Losses {'ner': 9.92527117028845}\n",
      "Losses {'ner': 10.318264807839107}\n",
      "Losses {'ner': 10.606606642121271}\n",
      "Losses {'ner': 7.102839201923451}\n",
      "Losses {'ner': 8.697619872281484}\n",
      "Losses {'ner': 6.593124436356559}\n",
      "Losses {'ner': 6.339870168012189}\n",
      "Losses {'ner': 8.520820829607718}\n",
      "Losses {'ner': 6.552293223721062}\n",
      "Losses {'ner': 6.770518031179842}\n",
      "Losses {'ner': 6.698897829834912}\n",
      "Losses {'ner': 7.325170956846549}\n",
      "Losses {'ner': 6.419882341047355}\n",
      "Losses {'ner': 5.163986666599982}\n",
      "Losses {'ner': 5.789036422025973}\n",
      "Losses {'ner': 5.743149365183655}\n",
      "Losses {'ner': 4.331177296742766}\n",
      "Losses {'ner': 4.097342268382135}\n",
      "Losses {'ner': 3.856819978182301}\n",
      "Losses {'ner': 3.5587065494192984}\n",
      "Losses {'ner': 2.315906678776173}\n",
      "Losses {'ner': 1.2138578298512357}\n",
      "Losses {'ner': 0.5971390740558888}\n",
      "Losses {'ner': 0.10967950855038566}\n",
      "Losses {'ner': 0.33815597488990345}\n",
      "Losses {'ner': 0.04745420310065973}\n",
      "Losses {'ner': 0.015289350992098355}\n",
      "Losses {'ner': 0.07861957201308523}\n",
      "Losses {'ner': 0.009371183604415254}\n",
      "Losses {'ner': 0.07994827951224269}\n",
      "Losses {'ner': 0.004570402030222105}\n",
      "Losses {'ner': 0.00020980681294463619}\n",
      "Losses {'ner': 0.001152713421033126}\n",
      "Losses {'ner': 3.853244639070944e-06}\n",
      "Losses {'ner': 0.000819688382480093}\n",
      "Losses {'ner': 5.747129999638515e-05}\n",
      "Losses {'ner': 1.64455663259816e-06}\n",
      "Losses {'ner': 3.5648856265630545e-05}\n",
      "Losses {'ner': 1.601123035984969e-05}\n",
      "Losses {'ner': 9.381084586734334e-05}\n",
      "Losses {'ner': 0.0040330551551225145}\n",
      "Losses {'ner': 0.031691980967180094}\n",
      "Losses {'ner': 1.978714920082209e-06}\n",
      "Losses {'ner': 1.0022268739348029e-05}\n",
      "Losses {'ner': 8.950501529010971e-07}\n",
      "Losses {'ner': 3.1484727064704035e-06}\n",
      "Losses {'ner': 0.002993806030433706}\n",
      "Losses {'ner': 2.2677456541558752e-05}\n",
      "Losses {'ner': 2.2154568220791607e-07}\n",
      "Losses {'ner': 1.3260947214736993e-05}\n",
      "Losses {'ner': 6.47788508592997e-07}\n",
      "Losses {'ner': 1.3028563246063506e-05}\n",
      "Losses {'ner': 1.1301033859139633e-06}\n",
      "Losses {'ner': 9.83184163431844e-06}\n",
      "Losses {'ner': 0.00015853316657371208}\n",
      "Losses {'ner': 1.1143567750475607e-05}\n",
      "Losses {'ner': 1.0145511469866755e-07}\n",
      "Losses {'ner': 5.487993620892744e-06}\n",
      "Losses {'ner': 4.188746577867416e-06}\n",
      "Losses {'ner': 9.344746432124499e-08}\n",
      "Losses {'ner': 1.8721184579686905e-05}\n",
      "Losses {'ner': 3.6886766876094887e-06}\n",
      "Losses {'ner': 0.00013184990967130531}\n",
      "Losses {'ner': 1.4207500967877603e-07}\n",
      "Losses {'ner': 2.249914178100019e-07}\n",
      "Losses {'ner': 6.752450110477659e-08}\n",
      "Losses {'ner': 6.664990076726031e-06}\n",
      "Losses {'ner': 1.3014356549053619e-06}\n",
      "Losses {'ner': 7.821955108716739e-07}\n",
      "Losses {'ner': 8.639043674034813e-08}\n",
      "Losses {'ner': 2.609707513476544e-07}\n",
      "Losses {'ner': 6.7840666089032945e-06}\n",
      "Losses {'ner': 5.120251519498792e-06}\n",
      "Losses {'ner': 2.86260406317687e-06}\n",
      "Losses {'ner': 1.373443600510517e-06}\n",
      "Losses {'ner': 0.0020093007975028744}\n",
      "Losses {'ner': 1.032165277819e-07}\n",
      "Losses {'ner': 7.652454405246824e-06}\n",
      "Losses {'ner': 1.1403187523225293e-07}\n",
      "Losses {'ner': 4.1514957198307225e-07}\n",
      "Losses {'ner': 2.7251543661718786e-08}\n",
      "Losses {'ner': 1.8412546245039616e-07}\n",
      "Losses {'ner': 6.761579641541512e-08}\n",
      "Losses {'ner': 8.323252152143913e-08}\n",
      "Losses {'ner': 2.277967285028494e-06}\n",
      "Losses {'ner': 4.38738611552079e-07}\n",
      "Losses {'ner': 1.2889266450263288e-06}\n",
      "Losses {'ner': 2.4282998809280427e-05}\n",
      "Losses {'ner': 5.625619038115572e-09}\n",
      "Losses {'ner': 1.0142506817411556e-07}\n",
      "Losses {'ner': 1.0007173549349673e-05}\n",
      "Losses {'ner': 2.5998520936869963e-06}\n",
      "Losses {'ner': 8.08469994017598e-07}\n",
      "Losses {'ner': 4.504524886847662e-05}\n",
      "Losses {'ner': 5.978984367377853e-09}\n",
      "Losses {'ner': 1.202833846979912e-07}\n",
      "Losses {'ner': 5.356657212882094e-07}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d2e345906a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_new_entity_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Date' is not defined"
     ]
    }
   ],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "nlp = train_model_new_entity_type(model)\n",
    "test_model(nlp, Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ff385",
   "metadata": {},
   "source": [
    "## Discovering sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c29ada99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bat/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19c2ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ca59319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love going to school!\", \"I hate going to school!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e00101aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8621afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_sentiment(sentence):\n",
    "    result = TextBlob(sentence).sentiment\n",
    "    print(sentence, result.polarity)\n",
    "    return result.polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2597a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nltk_sentiment(sentence):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(sentence, ss['compound'])\n",
    "    return ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49832046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.6696\n",
      "I hate going to school! -0.6114\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_nltk_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f72627b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.625\n",
      "I hate going to school! -1.0\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_blob_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67452310",
   "metadata": {},
   "source": [
    "### Sentiment for short texts using LSTM: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85400265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from wordseg import segment\n",
    "import html\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#from Chapter04.lstm_classification import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6953966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd026b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 50000\n",
    "EMBEDDING_DIM = 500\n",
    "twitter_csv = \"training.1600000.processed.noemoticon.csv\"\n",
    "english_twitter = \"./twitter_english.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3728ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82039f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(df, save_path):\n",
    "    df['language'] = df['tweet'].progress_apply(lambda t: lang_detect(t))\n",
    "    df = df[df['language']=='en']\n",
    "    df.to_csv(save_path, encoding='latin1', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fb72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, save_path='./data.csv', num_datapoints=80000):\n",
    "    df = pd.read_csv(filename, encoding='latin1')\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query','username', 'tweet']\n",
    "    df = pd.concat([df.head(num_datapoints), df.tail(num_datapoints)])\n",
    "    df = filter_english(df, save_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(text):\n",
    "    lang = \"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"None\"\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1c2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hashtags(tweet):\n",
    "    matches = re.findall(r'#[a-z0-9]+', tweet)\n",
    "    for match in matches:\n",
    "        tweet = re.sub(match, ' '.join(segment(match)[0]), tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d981cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #Lowercase all tweets\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: t.lower())\n",
    "    #Decode HTML\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: html.unescape(t))\n",
    "    #Remove @ mentions\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(r'@[A-Za-z0-9]+','',t))\n",
    "    #Remove URLs\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t:re.sub('https?://[A-Za-z0-9./]+','',t))\n",
    "    #Segment hashtags\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: segment_hashtags(t))\n",
    "    #Remove remaining non-alpha characters\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(\"[^a-zA-Z]\", \" \", t))\n",
    "    #Re-label positive tweets with 1 instead of 4\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda t: 1 if t==4 else t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a561c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1,0)\n",
    "    y_pred = [pred[0] for pred in y_pred]\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2e3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(df['tweet'].values)\n",
    "    save_tokenizer(tokenizer, 'twitter_tokenizer.pkl')\n",
    "    X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "    X = pad_sequences(X)\n",
    "    y = df['sentiment'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n",
    "                                                        random_state=42, stratify=df['sentiment'])\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    epochs = 15\n",
    "    batch_size = 32\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=es)\n",
    "    \n",
    "    accr = model.evaluate(X_test, y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    model.save('twitter_model.h5')\n",
    "    evaluate(model, X_test, Y_test)\n",
    "    plot_model(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c051b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a60ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training.1600000.processed.noemoticon.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0522f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [20:57<00:00, 127.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df = get_data(twitter_csv, 'twitter_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb6e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148633/148633 [00:00<00:00, 438190.58it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 571841.60it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 273271.10it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 315740.69it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 185917.65it/s]\n",
      "100%|██████████| 148633/148633 [00:01<00:00, 110976.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2602/2602 [==============================] - 1394s 534ms/step - loss: 0.6828 - accuracy: 0.5856 - val_loss: 0.6615 - val_accuracy: 0.6327\n",
      "Epoch 2/15\n",
      "2602/2602 [==============================] - 1439s 553ms/step - loss: 0.6260 - accuracy: 0.6656 - val_loss: 0.5907 - val_accuracy: 0.7084\n",
      "Epoch 3/15\n",
      "2602/2602 [==============================] - 1463s 562ms/step - loss: 0.5715 - accuracy: 0.7159 - val_loss: 0.5501 - val_accuracy: 0.7404\n",
      "Epoch 4/15\n",
      "2602/2602 [==============================] - 1881s 723ms/step - loss: 0.5360 - accuracy: 0.7433 - val_loss: 0.5243 - val_accuracy: 0.7549\n",
      "Epoch 5/15\n",
      "2602/2602 [==============================] - 37127s 14s/step - loss: 0.5125 - accuracy: 0.7566 - val_loss: 0.5070 - val_accuracy: 0.7652\n",
      "Epoch 6/15\n",
      "2602/2602 [==============================] - 1536s 590ms/step - loss: 0.4951 - accuracy: 0.7692 - val_loss: 0.4933 - val_accuracy: 0.7732\n",
      "Epoch 7/15\n",
      "2602/2602 [==============================] - 1447s 556ms/step - loss: 0.4792 - accuracy: 0.7782 - val_loss: 0.4828 - val_accuracy: 0.7769\n",
      "Epoch 8/15\n",
      "2602/2602 [==============================] - 1556s 598ms/step - loss: 0.4671 - accuracy: 0.7844 - val_loss: 0.4742 - val_accuracy: 0.7811\n",
      "Epoch 9/15\n",
      "2602/2602 [==============================] - 1582s 608ms/step - loss: 0.4563 - accuracy: 0.7910 - val_loss: 0.4675 - val_accuracy: 0.7837\n",
      "Epoch 10/15\n",
      "2602/2602 [==============================] - 1651s 635ms/step - loss: 0.4477 - accuracy: 0.7971 - val_loss: 0.4622 - val_accuracy: 0.7860\n",
      "Epoch 11/15\n",
      "2602/2602 [==============================] - 1574s 605ms/step - loss: 0.4396 - accuracy: 0.8001 - val_loss: 0.4581 - val_accuracy: 0.7882\n",
      "Epoch 12/15\n",
      "1655/2602 [==================>...........] - ETA: 36:27 - loss: 0.4317 - accuracy: 0.8040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-08796fcdcc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-958d789f3b2b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0maccr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425cde6",
   "metadata": {},
   "source": [
    "## Using BERT for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d576e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "724fbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_column_name, gold_column_name, test_percent):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[train_column_name], df[gold_column_name], test_size=test_percent, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068d6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #Lowercase all tweets\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: t.lower())\n",
    "    #Decode HTML\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: html.unescape(t))\n",
    "    #Remove @ mentions\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(r'@[A-Za-z0-9]+','',t))\n",
    "    #Remove URLs\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub('https?://[A-Za-z0-9./]+','',t))\n",
    "    #Segment hashtags\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: segment_hashtags(t))\n",
    "    #Remove remaining non-alpha characters\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(\"[^a-zA-Z]\", \" \", t))\n",
    "    #Re-label positive tweets with 1 instead of 4\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda t: 1 if t==4 else t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21f77c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "DATASET_SIZE = 4000\n",
    "english_twitter = \"./twitter_english.csv\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a0d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataset to tensorflow dataset\n",
    "def encode_data(df):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['tweet']\n",
    "        label = row['sentiment']\n",
    "        tokenize = tokenizer.tokenize(tweet)\n",
    "        bert_input = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=max_length,\n",
    "                                          padding=True, return_attention_mask=True)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "        return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list,\n",
    "                                                  token_type_ids_list, label_list)).map(map_inputs_to_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0970690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_inputs_to_dict(input_ids, attention_masks,token_type_ids, label):\n",
    "    return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05b84ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat(([df.head(int(DATASET_SIZE/2)), df.tail(int(DATASET_SIZE/2))]))\n",
    "    df = df.sample(frac = 1)\n",
    "    ds = encode_data(df)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bec6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_val_datasets(ds, size=DATASET_SIZE, batch_size=batch_size):\n",
    "    ds.shuffle(32)\n",
    "    train_size = int(0.7 * size)\n",
    "    val_size = int(0.15 * size)\n",
    "    test_size = int(0.15 * size)\n",
    "    train_dataset = ds.take(train_size).batch(batch_size)\n",
    "    test_dataset = ds.skip(train_size)\n",
    "    val_dataset = test_dataset.skip(test_size).batch(batch_size)\n",
    "    test_dataset = test_dataset.take(test_size).batch(batch_size)\n",
    "    return (train_dataset, test_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc6251af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(ds, export_dir):\n",
    "    train_dataset, test_dataset, val_dataset = get_test_train_val_datasets(ds)\n",
    "    learning_rate = 2e-5\n",
    "    number_of_epochs = 3\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss,metrics=[metric])\n",
    "    bert_history = model.fit(train_dataset, epochs=number_of_epochs, validation_data=val_dataset)\n",
    "    model.save_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e1360a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_model(export_dir):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2c1e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_existing_file(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[['tweet', 'sentiment']]\n",
    "    return df\n",
    "\n",
    "#df = read_existing_file(english_twitter)\n",
    "#df = get_data(english_twitter,\"twitter_english.csv\")\n",
    "#df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc6fc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148633/148633 [00:00<00:00, 485762.07it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 659960.60it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 402913.01it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 351540.45it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 379220.06it/s]\n",
      "100%|██████████| 148633/148633 [00:01<00:00, 120023.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "996be22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fac40363ca0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fac40363ca0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /Users/bat/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1/1 [==============================] - 26s 26s/step - loss: 0.5960 - accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4392 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 1000ms/step - loss: 0.3703 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = fine_tune_model(dataset,'./bert_twitter_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bf47abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(input_text):\n",
    "    tokenized = tokenizer.tokenize(input_text)\n",
    "    bert_input = tokenizer.encode_plus(\n",
    "                    input_text,                      \n",
    "                    add_special_tokens = True, # add [CLS], [SEP]\n",
    "                    max_length = max_length, # max length of the text that can go to BERT\n",
    "                    pad_to_max_length = True, # add [PAD] tokens\n",
    "                    return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "                    return_tensors='tf'\n",
    "        )\n",
    "    return bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69a30bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(model_path, tweet):\n",
    "    model = load_existing_model(model_path)\n",
    "    bert_input = encode_example(tweet)\n",
    "    tf_output = model.predict([bert_input['input_ids'], \n",
    "                              bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "    tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "    new_label = np.argmax(tf_pred, axis=-1)\n",
    "    print(new_label)\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d71e44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./bert_twitter_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./bert_twitter_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example('./bert_twitter_model', 'I hate going to the school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f29fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = []\n",
    "    for tweet in X_test:\n",
    "        bert_input = encode_example(tweet)\n",
    "        tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "        tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "        new_label = np.argmax(tf_pred, axis=-1)\n",
    "        y_pred.append(new_label)\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc59d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_existing_model(export_dir, num_points=200):\n",
    "    model = load_existing_model(export_dir)\n",
    "    df = read_existing_file(english_twitter)\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat([df.head(num_points),df.tail(num_points)])\n",
    "    (X_train, X_test, y_train, y_test) = split_dataset(df, 'tweet', 'sentiment',20)\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51da09ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_twitter_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_twitter_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 666700.16it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 592902.98it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 364318.08it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 386959.33it/s]\n",
      "100%|██████████| 148633/148633 [00:00<00:00, 210267.77it/s]\n",
      "100%|██████████| 148633/148633 [00:01<00:00, 124839.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.17      0.24        12\n",
      "    positive       0.33      0.62      0.43         8\n",
      "\n",
      "    accuracy                           0.35        20\n",
      "   macro avg       0.37      0.40      0.34        20\n",
      "weighted avg       0.37      0.35      0.32        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_and_evaluate_existing_model('bert_twitter_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473ea76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
