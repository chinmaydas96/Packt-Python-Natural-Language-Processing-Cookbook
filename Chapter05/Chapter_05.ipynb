{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"DataScientist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(df, regex, column_name):\n",
    "    df[column_name] = df['Job Description'].apply(lambda x: re.findall(regex, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_items(df, column_name):\n",
    "    items = []\n",
    "    for index, row in df.iterrows():\n",
    "        if (len(row[column_name]) > 0):\n",
    "            for item in list(row[column_name]):\n",
    "                if (type(item) is tuple and len(item) > 1):\n",
    "                    item = item[0]\n",
    "                if (item not in items):\n",
    "                    items.append(item)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(df):\n",
    "    original_email_regex = '[\\S]+@[a-zA-Z0-9\\.]+\\.[a-zA-Z]+'\n",
    "    email_regex = '[^\\s:|()\\']+@[a-zA-Z0-9\\.]+\\.[a-zA-Z]+'\n",
    "    df['emails'] = df['Job Description'].apply(lambda x: re.findall(email_regex, x))\n",
    "    emails = get_list_of_items(df, 'emails')\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(df):\n",
    "    url_regex = '(http[s]?://(www\\.)?[A-Za-z0-9–_\\.\\-]+\\.[A-Za-z]+/?[A-Za-z0-9$\\–_\\-\\/\\.]*)[\\.)\\\"]*'\n",
    "    df = get_items(df, url_regex, 'urls')\n",
    "    urls = get_list_of_items(df, 'urls')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "      <th>Easy Apply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>ABOUT HOPPER\\r\\n\\r\\nAt Hopper, we’re on a miss...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hopper\\r\\n3.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Travel Agencies</td>\n",
       "      <td>Travel &amp; Tourism</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist, Product Analytics</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>At Noom, we use scientifically proven methods ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Noom US\\r\\n4.5</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>2008</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Health, Beauty, &amp; Fitness</td>\n",
       "      <td>Consumer Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Decode_M\\r\\n\\r\\nhttps://www.decode-m.com/\\r\\n\\...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Decode_M</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1 to 50 employees</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Sapphire Digital seeks a dynamic and driven mi...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Sapphire Digital\\r\\n3.4</td>\n",
       "      <td>Lyndhurst, NJ</td>\n",
       "      <td>Lyndhurst, NJ</td>\n",
       "      <td>201 to 500 employees</td>\n",
       "      <td>2019</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>Zocdoc, Healthgrades</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Director, Data Science</td>\n",
       "      <td>$111K-$181K (Glassdoor est.)</td>\n",
       "      <td>Director, Data Science - (200537)\\r\\nDescripti...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>United Entertainment Group\\r\\n3.4</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>2007</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>BBDO, Grey Group, Droga5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                          Job Title  \\\n",
       "0           0      0              Senior Data Scientist   \n",
       "1           1      1  Data Scientist, Product Analytics   \n",
       "2           2      2               Data Science Manager   \n",
       "3           3      3                       Data Analyst   \n",
       "4           4      4             Director, Data Science   \n",
       "\n",
       "                Salary Estimate  \\\n",
       "0  $111K-$181K (Glassdoor est.)   \n",
       "1  $111K-$181K (Glassdoor est.)   \n",
       "2  $111K-$181K (Glassdoor est.)   \n",
       "3  $111K-$181K (Glassdoor est.)   \n",
       "4  $111K-$181K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  \\\n",
       "0  ABOUT HOPPER\\r\\n\\r\\nAt Hopper, we’re on a miss...     3.5   \n",
       "1  At Noom, we use scientifically proven methods ...     4.5   \n",
       "2  Decode_M\\r\\n\\r\\nhttps://www.decode-m.com/\\r\\n\\...    -1.0   \n",
       "3  Sapphire Digital seeks a dynamic and driven mi...     3.4   \n",
       "4  Director, Data Science - (200537)\\r\\nDescripti...     3.4   \n",
       "\n",
       "                        Company Name       Location      Headquarters  \\\n",
       "0                      Hopper\\r\\n3.5   New York, NY  Montreal, Canada   \n",
       "1                     Noom US\\r\\n4.5   New York, NY      New York, NY   \n",
       "2                           Decode_M   New York, NY      New York, NY   \n",
       "3            Sapphire Digital\\r\\n3.4  Lyndhurst, NJ     Lyndhurst, NJ   \n",
       "4  United Entertainment Group\\r\\n3.4   New York, NY      New York, NY   \n",
       "\n",
       "                     Size  Founded  Type of ownership  \\\n",
       "0   501 to 1000 employees     2007  Company - Private   \n",
       "1  1001 to 5000 employees     2008  Company - Private   \n",
       "2       1 to 50 employees       -1            Unknown   \n",
       "3    201 to 500 employees     2019  Company - Private   \n",
       "4     51 to 200 employees     2007  Company - Private   \n",
       "\n",
       "                    Industry                  Sector  \\\n",
       "0            Travel Agencies        Travel & Tourism   \n",
       "1  Health, Beauty, & Fitness       Consumer Services   \n",
       "2                         -1                      -1   \n",
       "3                   Internet  Information Technology   \n",
       "4    Advertising & Marketing       Business Services   \n",
       "\n",
       "                    Revenue               Competitors Easy Apply  \n",
       "0  Unknown / Non-Applicable                        -1         -1  \n",
       "1  Unknown / Non-Applicable                        -1         -1  \n",
       "2  Unknown / Non-Applicable                        -1       True  \n",
       "3  Unknown / Non-Applicable      Zocdoc, Healthgrades         -1  \n",
       "4  Unknown / Non-Applicable  BBDO, Grey Group, Droga5         -1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jsmith@quartethealth.com', 'security@quartethealth.com', 'talent@quartethealth.com', 'accommodations-ext@fb.com', 'talent@ebay.com', 'recruiting-inquiries@deshaw.com', 'cvwithdraw@deshaw.com', 'backgroundcheck-inquiries@deshaw.com', 'Candidate.Accommodations@Disney.com', 'careers@springhealth.com', 'TalentAcquisition@grubhub.com', 'privacy@grubhub.com', 'jobs@temboo.com', 'erinn@cybercoders.com', 'careers@Healthfirst.org', 'kevin@ingenium.agency', 'mailbox_tas_recruit@humana.com', 'Application_Accommodation@colpal.com', 'accommodations@teacherspayteachers.com', 'nick.sottile@bombas.com', 'atÂshravya@techprojects.us', 'shravya@techprojects.us', 'careers@edo.com', 'paypalglobaltalentacquisition@paypal.com', 'ieo.mailbox@aero.org', 'careers@mpulsemobile.com', 'recruiter@avianaglobal.com', 'elizabeth@biophaseinc.com', 'accommodations-ext@snap.com', 'usrecruitment@sdl.com', 'privacy@honest.com', 'vtopcagic@relevante.biz', 'lhatteme@usc.edu', 'Brooke.Schoen@kellyservices.com', 'careers@gilead.com', 'uschr@usc.edu', 'accommodations@dollarshaveclub.com', 'bflores1@lacourt.org', 'info@governmentjobs.com', 'AMunoz4@dhs.lacounty.gov', 'bwaldron@apexsystems.com', 'employeeservices@apexsystemsinc.com', 'jim@ingenium.agency', 'accommodations@dollarshaveclub.comPlease', 'hr@exponent.com', 'naga@clouddatasystemsus.com', 'irvine@camdenkelly.com', 'ldmayberry@apexsystems.com', 'jjovovic@wintrustwealth.com', 'accommodations@climate.com', 'human.resources@thrivent.com', 'suda@sutoer.com', 'careers@greenkeytech.com', 'pranali@fisecglobal.net', 'internalrecruiting@civisanalytics.com', 'cvshealthsupport@us.ibm.com', 'disability-accommodations@treehousefoods.com', 'shai879@kellyservices.com', 'careers@3redpartners.com', 'pathfinder@pnc.com', 'rkilgore@relevante.com', 'compliance@integralads.com', 'recruitingagencies@integralads.com', 'sai@kanisol.com', 'cvs_support@modernhire.com', 'info@njf.com', 'ndarian@smithhanley.com', 'talent.acquisition@servicenow.com', 'dgingerich@carisls.com', 'gjimenez@uic.edu', 'Accommodation.Reques@am.jll.com', 'rajeshk@avtechsol.com', 'jobs@primusglobal.com', 'abbas@infotreeglobal.com', 'icfcareercenter@icf.com', 'ADACoordinator@bmd.hctx.net', 'hari.haran@wisemen.com', 'spam.leidos@leidos.com', 'hr@texastoolbelt.com.com', 'liza.k@wisemen.com', 'Josh.carlos@itbtalent.com', 'DisabilityAccommodations@catalent.com', 'andrew.cooper@modis.com', 'krobinson@carisls.com', 'accommodations@hioscar.com', 'Melissa@kingmelissa.com', 'RecruitingAccommodation@guidehouse.com', 'rama@diverselynx.com', 'rama.diverse@gmail.com', 'sdonovan@judge.com', 'divyansh@net2source.com', 'TalentAcquisition@email.chop.edu', 'Emily.Selzam@sig.com', 'CareersUS@QVC.com', 'dkuphal@myvoca.com', 'nico.molenschot@dllgroup.com', 'jason.weinstein@mondo.com', 'ukdiversity.recruitment@gsk.com', 'dneto@apexsystems.com', 'mreilly@itechsolutions.com', 'mfetterolf@apexsystems.com', 'bridget@alphaconsulting.com', 'faqpchr@phila.gov', 'hrhelpdesk@phila.gov', 'Phila.OHR@phila.gov', 'employment@liprop.com', 'Shibu.S@avacend.com', 'shaylyn.jaworowski@mondo.com', 'tim@blackfynn.com', 'mike.pachella@sig.com', 'workday_recruiting@iqvia.com', 'botten@itechsolutions.com', 'recruitment@numericjobs.com', 'accommodations@relx.com', 'career@infoquestgroup.com', 'LMurray@TechUSA.net', 'accommodations@relx.com.Elsevier', 'staffingaadar@msd.com', 'agiangiulio@fredbeans.com', 'dherbine@pipercompanies.com', 'accommodations@koniag.com', 'khartley@apexsystems.com', 'Sam.Bodian@CyberCoders.com', 'hesquivel@deaconrecruiting.com', 'Talent.manager@techquarry.com', 'jfowler@deaconrecruiting.com', 'favalos@deaconrecruiting.com', 'accommodations@illumina.com', 'talentacquisition@dexcom.com', 'Lilly_Recruiting_Compliance@lists.lilly.com', 'Amit@apninc.com', 'MyRecruiter@alere.com', 'debasisb@askstaffing.com', 'TONM369@kellyservices.com', 'hr@btsresearch.com', 'vinay@logicplanet.com', 'recruitinghelp@mitre.org', 'Ranjith.vemula@idctechnologies.com', 'dani@pavetalent.com', 'careers@atai.life', 'mturner@simplybiotech.com', 'JEND557@kellyscientific.com', 'applyassistance@danaher.com', 'Aleo431@KellyScientific.com', 'algo__autobench_research_scientist_a84151f42us@ivy.greenhouse.io', 'US_Employment_Compliance@cgi.com', 'bri.haggerty@cybercoders.com', 'ignite@atos.netHere', 'cskumar@xcelogroup.com', 'sachin@apninc.com', 'rakesh@athreyainc.com', 'Ârakesh@athreyainc.com', 'Mallikharjun@conchtech.com', 'vandanat@sysmind.com', 'cgarza@mleehealthcare.com', 'rohit.mcdonald@prolim.com', 'info@prolim.com', 'krishna@itminds.net', 'naveen.remkumar@two95intl.com', 'mani@net2source.com', 'chandikumar@infowaygroup.com', 'sclagett@apexsystems.com', 'shobana@infowaygroup.com', 'dallas@camdenkelly.com', 'bhavna@tekleaders.com', 'Âbhavna@tekleaders.com', 'karthik@infovision.com', 'Âvikram@pddninc.net', 'Rohan.g@avacend.com', 'Careers.NorthAmerica@sap.com', 'Careers.LatinAmerica@sap.com', 'Careers.APJ@sap.com', 'Careers@sap.com', 'ekingery@apexsystems.com', 'hala.hillo@motektech.com', 'ajones@simplybiotech.com', 'brenna.boies@cybercoders.com', 'info@springml.com', 'HR@dtexsystems.com', 'recruiting@kariusdx.com', 'brenna.boies@gmail.com', 'accommodations@ehealthinsurance.com', 'careers@tredence.com', 'tansib@infotreeglobal.com', 'Marcus.Quigley@cybercoders.com', 'Sandra@OSIEngineering.com', 'kushal@nextgentechnologiesinc.com', 'chandra@ebasetek.com', 'pawans@ustechsolutionsinc.com', 'resumes@nextgentechinc.com', 'Niyaz.Ansari@artechinfo.com', 'bchitneedi@esharpedge.com', 'ram.kishor@diverselynx.com', 'TalentAcquisitionTeam@dnb.com', 'hr@cloudflare.com', 'reasonable.accommodation@mastercard.com', 'statjobs@austin.utexas.edu', 'jayaraman@infowaygroup.com', 'sukh@arksolutionsinc.com', 'vishal@arksolutionsinc.com', 'shane@arksolutionsinc.com', 'neeraj.h@avacend.com', 'tharrell@chiptonross.com', 'compliance@integralads.com.To', 'kushal@apninc.com', 'HR-Accommodations@FireEye.com', 'e.riley@nigelfrank.com', 'Vikasy@apninc.com', 'usaccommodations@globalfoundries.com', 'lyssa.heine@arlut.utexas.edu', 'Âanvesh@conchtech.com', 'careers@crowley.com', 'sjohnson@arcgonline.com', 'tom@emeraldresourcegroup.com', 'naveen@dsiginc.com', 'Prajwal.Rajappa@nationwidechildrens.org', 'Kerry.orton@nationwidechildrens.org', 'Alok.Kumar@artech.com', 'Motao.Zhu@NationwideChildrens.org', 'naveen.b@avanitechsolutions.com']\n"
     ]
    }
   ],
   "source": [
    "emails = get_emails(df)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.decode-m.com/', 'https://www.amazon.jobs/en/disability/us.', 'https://www.techatbloomberg.com/nlp/', 'https://bloomberg.com/company/d4gx/', 'https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm', 'http://www.tapestry.com/', 'https://www.arena.io/about/careers.html', 'http://www.fujitsu.com/global/about/corporate/info/', 'http://www.fujitsu.com/global/digitalannealer/', 'http://www.fujitsu.com/global/solutions/business-technology/ai/', 'https://whisk.com/cognitive-food-platform/', 'https://youtu.be/QzKe9iZszD0', 'https://argogroup.wd1.myworkdayjobs.com/Argo', 'https://prolancer.com/jobs/send-proposal/298', 'http://www-01.ibm.com/employment/us/benefits', 'http://business.pinto.co', 'https://bit.ly/2m14P0P', 'https://bit.ly/2lCOcYS', 'https://bit.ly/2m1510', 'http://www.colgatepalmolive.com.', 'http://www.colgatebsbf.com.', 'http://www.hillspet.com.', 'http://www.tomsofmaine.com.', 'https://www.medlypharmacy.com/', 'https://oim.unjspf.org/.', 'http://www.nasdaq.com', 'https://studentaid.ed.gov/sa/repay-loans/forgiveness-cancellation/public-service', 'https://a127-jobs.nyc.gov/.', 'https://www.home.neustar.', 'https://hirepeopletree.com/ccpa', 'http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf.', 'https://www.gofundme.com/2019https', 'https://medium.com/gofundme-storieshttps', 'http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law', 'http://ccmb.usc.edu', 'https://www.ibm.com/internet-of-things/', 'https://www.ibm.com/watson/', 'https://www.ibm.com/analytics/', 'https://blog.activision.com', 'https://wd5.myworkday.com/usc/d/inst/1$9925/9925$45297.htmld', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/1300/general-physical-science-series-1301/', 'http://www-01.ibm.com/employment/us/benefitsIBM', 'http://ope.ed.gov/accreditation', 'http://whed.net/home.php.', 'http://aice-eval.org/members/.', 'http://www-01.ibm.com/employment/us/benefits/', 'https://www-03.ibm.com/press/us/en/pressrelease/50744.wss', 'http://www.ibm.com/ibm/responsibility/initiatives.html', 'http://www.ibm.com/ibm/responsibility/corporateservicecorps', 'http://www.galvanize.com/courses/data-science/', 'http://instagram.com/lifeatservicetitan', 'https://www.themuse.com/profiles/servicetitan', 'https://www.builtinla.com/company/servicetitan-inc', 'http://www.arete.comAret', 'https://www.youtube.com/watch', 'http://eleverpro.com', 'http://www.eleverpro.com/executive-recruiter-jobs/', 'https://youtu.be/CC-seRMEo8s', 'http://www.mylife.com.', 'https://prolancer.com/jobs/send-proposal/301', 'http://www.cslbehring.com', 'http://www.cslplasma.com/careers', 'https://www.cslbehring.com/careers/eeo-statement', 'http://www.tetratech.com/en/benefits.For', 'http://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/', 'http://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/1300/general-physical-science-series-1301/', 'http://camdenkelly.com/jobs', 'https://twitter.com/camdenkellycorp', 'http://www.linkedin.com/company/3279780', 'https://www.facebook.com/camdenkellycorp', 'https://plus.google.com/u/0/106722962279155885960', 'https://www.cedars-sinai.edu/Research/Research-Labs/Guarnerio-Lab/', 'http://www.oakwoodsearch.com', 'https://www.cedars-sinai.org/research/labs/svendsen.html.', 'http://www.opm.gov/qualifications/standards/index-Standards.asp', 'http://www.opm.gov/qualifications/standards/indexes/num-ndx.asp', 'https://youtu.be/c5TgbpE9UBI', 'https://climate.com/careers', 'https://www.uptake.com/careers.What', 'https://upt.ac/16ee15fc.What', 'https://www.kalderos.com/company/about', 'https://www.kalderos.com/company/culture', 'https://careers.nordstrom.com/', 'https://www.facebook.com/HPECareers', 'https://twitter.com/HPE_Careers', 'https://imanage.com/privacy-policy/', 'https://drw.com/privacy-notice.California', 'https://drw.com/california-privacy-notice.', 'https://bmoharriscareers.com.', 'https://jobs.lever.co/kraken.', 'http://securityreport.uchicago.edu.', 'https://bmoharriscareers.com.BMO', 'http://www1.eeoc.gov/employers/poster.cfm', 'https://relevante.jobs.net/en-US/join', 'http://integralads.com/', 'https://muse.cm/2t8eGlN', 'https://searchitchannel.techtarget.com/feature/Data-center-infrastructure-spending-gets-AI-boost', 'https://www.insight.com/en_US/about/newsroom/press-releases/2019/07012019-insight-recognized-at-no14-on-crns-2019-solution-provider-500-list.html', 'https://investor.insight.com/press-release/insight-enterprises-acquire-pcm-inc', 'https://insight.hqprod.businesswire.com/press-release/insight-hosts-global-ai-competition-healthcare-innovation-cincinnati', 'https://investor.insight.com/press-release/houston-schools-deploying-iot-enabled-building-safety-platform-improve-emergency', 'https://investor.insight.com/press-release/insight-recognized-magic-quadrant-managed-workplace-services-north-america', 'https://cvshealth.com/covid-19', 'http://www.tetratech.com/en/benefits.', 'https://drw.com/privacy-notice.', 'https://clientapps.jobadder.com/45103/fast-switch', 'https://twitter.com/FastSwitchGL', 'https://www.instagram.com/fastswitchgl/', 'https://www.linkedin.com/company/fast-switch-great-lakes', 'https://tinyurl.com/v4p9yy7', 'https://www.gpo.gov/fdsys/pkg/CFR-2011-title20-vol3/pdf/CFR-2011-title20-vol3-sec656-3.pdf', 'https://www.cdc.gov/hai/epicenters/chicago.html.', 'http://ope.ed.gov/accreditation/.', 'http://www.ed.gov/about/offices/list/ous/international/usnei/us/edlite-visitus-forrecog.html.', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/.', 'https://www.nextstepsystems.com/employers_submit_raul.html.', 'https://www.nextstepsystems.com/jobseekers_submit_raul.html.', 'https://www.nextstepsystems.com', 'http://www.itrecruiters.technology.', 'http://www.recruiter.technology', 'http://www.parallelpartners.com.', 'https://www.dascena.com/', 'https://h-gac.com/careersH-GAC', 'https://h-gac.com/careers', 'http://www.walkerelliott.com/candidates/jobs/jobDetail/default.aspx', 'https://hrrm.harriscountytx.gov/Pages/EqualEmploymentOpportunityPlan.aspx', 'https://hrrm.harriscountytx.gov/Pages/Medical.aspx', 'http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.html', 'https://www.facebook.com/Mindteckglobal/', 'https://www.linkedin.com/company/mindteck', 'https://www.youtube.com/user/MindteckVideos', 'https://twitter.com/mindteck', 'https://www.pinterest.com/mindteck/', 'https://mobilemathlab.com/', 'https://www.soarenmanagement.com/', 'https://www.instagram.com/gatestonebpo', 'https://usa.healthcare.siemens.com/careers.', 'https://usa.healthcare.siemens.com/about.', 'https://www.facebook.com/Access-Control-Group-Access-100773445034653/', 'http://www.kingmelissa.com/', 'https://www.usajobs.gov/Help/working-in-government/unique-hiring-paths/students/', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/1500/computer-science-series-1550/', 'https://www.linkedin.com/in/rama-shanker-verma-aman-01a283a/', 'https://tinyurl.com/ycqdhb98', 'http://one2one.sparklight.com/tag/coronavirus/', 'https://www.sparklight.com/about/careers', 'https://www.linkedin.com/in/divyansh-srivastava-11563041/', 'http://www2.deloitte.com/us/en/pages/about-deloitte/articles/deloitte-corporate-citizenship.html', 'http://www2.deloitte.com/us/en/pages/careers/topics/recruiting-tips.htmlCategory', 'http://www.med.upenn.edu/', 'http://www.cbica.upenn.edu/', 'https://safety.temple.edu/reports-logs/annual-security-report', 'http://www.med.upenn.edu/This', 'http://www.phila.gov/personnel/ExamsFAQ.html', 'http://www.camadvgrp.com/', 'http://www.med.upenn.edu/.', 'https://www.linkedin.com/company/integress-inc./', 'http://jconnectinc.com/', 'http://www.angeiongroup.com.', 'https://www.amringusa.com/company-2/', 'https://www.mainlinehealth.org/careers', 'https://wd5.myworkday.com/iheartmedia/d/task/3005$4482.htmld', 'http://iheartmediacareers.com/Pages/EEO.aspx', 'https://wd5.myworkday.com/iheartmedia/d/task/3005$1999.htmld', 'https://iheartmedia.com/', 'http://www.compqsoft.com/current-openings.html', 'https://www.swri.org/technical-divisions/defense-intelligence-solutions.', 'https://www.opm.gov/policy-data-oversight/pay-leave/salaries-wages/IF', 'http://cherokee-cna.com/Pages/Home.aspx', 'http://cherokee-os.com/Pages/Home.aspx', 'https://cherokee-federal.com/', 'http://cherokeenationbusinesses.com/careers/Pages/home.aspx', 'http://cherokeenationbusinesses.com/federalSolutions/Pages/overview.aspx', 'http://www.leoniegroup.com/careers', 'https://ivs.swri.org.', 'http://www2.ed.gov/admins/finaid/accred/index.html', 'https://hpi.swri.org.', 'http://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/0800/all-professional-engineering-positions-0800/', 'http://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/1500/computer-science-series-1550/', 'http://cherokee-cnts.com/Pages/home.aspx', 'http://cherokeenationbusinesses.com/federalSolutions', 'http://cherokeenationbusinesses.com/Pages/home.aspx', 'https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf', 'https://www.dol.gov/ofccp/pdf/pay-transp_', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/0400/general-natural-resources-management-and-biological-sciences-series-0401/', 'http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf', 'http://www.sorrentotherapeutics.com', 'https://bfa.sdsu.edu/hr/jobs/job-opportunities.aspx', 'http://jobs.sdsu.edu', 'https://it.sdsu.edu/', 'https://bfa.sdsu.edu/hr/jobs/benefits', 'https://smokefree.sdsu.edu/', 'https://www.tandemdiabetes.com/careers/life-at-tandem', 'https://www.expesicor.com/jobs/research-scientist-san-diego/', 'https://grants.nih.gov/grants/guide/notice-files/NOT-OD-18-210.html', 'https://apol-recruit.ucsd.edu/apply/JPF02126', 'https://www.monocerosbio.com', 'http://www.absorption.com', 'https://www.facetec.com/', 'https://wd5.myworkday.com/usc/d/inst/1$9925/9925$46086.htmld', 'http://www-hr.ucsd.edu/saa/nondiscr.htmlUC', 'https://www.bluenalu.com/', 'http://adminrecords.ucsd.edu/PPM/docs/230-311.html', 'https://apol-recruit.ucsd.edu/apply/JPF02462', 'http://www.ucop.edu/academic-personnel-programs/_files/apm/apm-310.pdf', 'https://apol-recruit.ucsd.edu/apply/JPF02177', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/0600/medical-technologist-series-0644/In', 'https://www.opm.gov/policy-data-oversight/classification-qualifications/general-schedule-qualification-standards/0600/medical-technologist-series-0644/', 'http://med.ucsd.edu', 'https://apol-recruit.ucsd.edu/apply/JPF02186', 'https://www.naspovaluepoint.org/portfolio/mmis-provider-services-module-2018-2028/hhs-technology-group/', 'https://cgi-veterans.jobs/', 'https://qbrc.swmed.edu/', 'http://citylinedfw.com/about/', 'https://www.goldmansachs.com/careers/footer/disability-statement.html', 'https://lnkd.in/ePyq7pM', 'https://lnkd.in/e9ZpD5J', 'https://lnkd.in/eqttaa4', 'http://www.kairostech.com', 'https://www.linkedin.com/in/prasadmamidela', 'https://www.trinityra.org/joblist.htm', 'https://www.vdriveitsolutions.com/', 'http://www.xome.com', 'https://www.consumer.ftc.gov/JobScams', 'https://www.ftccomplaintassistant.gov/.', 'http://bit.ly/1mzJQeL', 'https://jobs.gartner.com/applicant-privacy-policy', 'https://www.pddninc.com/', 'http://www.corelogic.com/privacy.aspxConnect', 'https://benefits.northropgrumman.com/us/en2/BenefitsOverview/Pages/default.aspx', 'https://vimeo.com/311952506', 'https://match-relevant.com/your-story', 'https://calendly.com/jake-villarreal/calendar', 'https://teladochealth.com/en/about/', 'http://jobs.apple.com/', 'https://www.verizonmedia.com/careers/contact-us.html', 'https://www.amazon.jobs/en/disability/us', 'https://www.ebayinc.com/our-company/diversity-inclusion/.', 'http://adminguide.stanford.edu.', 'http://ophthalmology.stanford.edu/', 'https://join.collectivehealth.com/box', 'https://www.accountingprincipals.com/privacy-policy/', 'https://corporate.jd.com/', 'https://air.jd.com/', 'http://neon.life', 'https://e-verify.uscis.gov/esp/media/resourcesContents/EverifyPosterEnglish.pdf', 'https://e-verify.uscis.gov/esp/media/resourcesContents/EverifyPosterSpanish.pdf', 'https://e-verify.uscis.gov/esp/media/resourcesContents/WebBPPOSTERRtoWEnglishversion.pdf', 'https://e-verify.uscis.gov/esp/media/resourcesContents/WebBPPOSTERRtoWSpanishversion.pdf', 'https://portal.slac.stanford.edu/sites/lcls_public', 'http://www-group.slac.stanford.edu/esh/eshmanual/pdfs/ESHch01.pdf', 'http://adminguide.stanford.edu', 'http://www.sandia.gov', 'https://www.linkedin.com/in/niyazansari/', 'https://www.nvidia.com/en-us/contact/', 'https://dnb.wd1.myworkdayjobs.com/Careers', 'http://www.hr.sao.state.tx.us/Compensation/JobDescriptions.aspx', 'http://indeedhi.re/IndeedBenefits', 'https://www.indeed.com/legal/applicant-privacy', 'https://santannaenergyservices.com/', 'http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf', 'http://www.dol.gov/ofccp/regs/compliance/posters/pdf/ofccp_eeo_supplement_final_jrf_qa_508c.pdf', 'http://bit.ly/amazon-scot', 'https://getspruce.com', 'https://www.facebook.com/SpruceCrew', 'https://stockedrobotics.freshteam.com/jobs', 'http://www.hr.sao.state.tx.us/Compensation/JobDescriptions.aspx.', 'http://www.twdb.texas.gov/jobs/benefits.asp', 'https://bit.ly/2YT6oii', 'http://www.twdb.texas.gov/jobs/index.asp', 'http://www.twitter.com/forgerock.', 'https://encoura.org/privacy-policy/', 'http://www.hr.sao.state.tx.us/Compensation/JobDescriptions.aspx.HHS', 'https://www.facebook.com/irisplans', 'https://dellmed.utexas.edu/culture.', 'https://www.facebook.com/viahart', 'https://bit.ly/2UKaqWQ', 'https://muse.cm/2t8eGlNAttention', 'https://www.nvidia.com/en-us/geforce/products/geforce-now/', 'https://hypori.com/', 'https://www.county.org/', 'https://www.facebook.com/TexasCounties', 'https://mastercard.jobs/toronto-can/senior-data-engineer/2B400D4C9F4345E2B2039B9AB1845623/job/', 'https://www.praetorian.com/careers', 'https://mastercard.jobs/toronto-can/lead-data-engineer/567DD39603B141C5AAF86389682B751D/job/', 'https://www.linkedin.com/in/emma-riley-72028917a/', 'http://www.globalfoundries.com.', 'https://capps.taleo.net/careersection/ex/jobsearch.ftl', 'https://www.ers.texas.gov/', 'https://jobs.btginc.com.', 'http://www.tetratech.com/en/benefits', 'https://www.mindteck.com/career/life-at-mindteck.html', 'https://eng.joinroot.com/', 'https://www.opm.gov/policy-data-oversight/hiring-information/direct-hire-authority/You', 'https://www.ecdi.org', 'https://www.facebook.com/ECDIoh/', 'https://www.deeplens.ai/', 'https://www.facebook.com/deeplensai/', 'http://www.nationwidechildrens.org/patient-centered-pediatric-research-program.', 'http://www.avanitechsolutions.com']\n"
     ]
    }
   ],
   "source": [
    "urls = get_urls(df)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar strings: the Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "from Chapter05.regex import get_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"DataScientist.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_levenshtein(input_string, df):\n",
    "    df['distance_to_' + input_string] = df['emails'].apply(lambda x:Levenshtein.distance(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_email_lev(df, email):\n",
    "    df = find_levenshtein(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    minimum_value_email_index = df[column_name].idxmin()\n",
    "    email = df.loc[minimum_value_email_index]['emails']\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails = get_emails(df)\n",
    "new_df = pd.DataFrame(emails,columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_email_lev(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jaro(input_string, df):\n",
    "    df['distance_to_' + input_string] = \\\n",
    "    df['emails'].apply(lambda x: Levenshtein.jaro(input_string, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_email_jaro(df, email):\n",
    "    df = find_jaro(email, df)\n",
    "    column_name = 'distance_to_' + email\n",
    "    maximum_value_email_index = df[column_name].idxmax()\n",
    "    email = df.loc[maximum_value_email_index]['emails']\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file, encoding='utf-8')\n",
    "emails = get_emails(df)\n",
    "new_df = pd.DataFrame(emails,columns=['emails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohit.mcdonald@prolim.com\n"
     ]
    }
   ],
   "source": [
    "input_string = \"rohitt.macdonald@prelim.com\"\n",
    "email = get_closest_email_jaro(new_df, input_string)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49333333333333335\n"
     ]
    }
   ],
   "source": [
    "print(Levenshtein.jaro_winkler(\"biswajit.satapathy1998@gmail.com\",\"rohit.mcdonald@prolim.org\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing named entity recognition using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"iPhone 12: Apple makes jump to 5G\n",
    "Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks. \n",
    "The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen. \n",
    "The US firm bucked a wider industry downturn by increasing its handset sales over the past year. \n",
    "But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6. \n",
    "\"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, \n",
    "real-time interactivity and so much more,\" said chief executive Tim Cook. \n",
    "There has also been a cosmetic refresh this time round, with the sides of the devices getting sharper, flatter edges. \n",
    "The higher-end iPhone 12 Pro models also get bigger screens than before and a new sensor to help with low-light photography. \n",
    "However, for the first time none of the devices will be bundled with headphones or a charger. \n",
    "Apple said the move was to help reduce its impact on the environment. \"Tim Cook [has] the stage set for a super-cycle 5G product release,\" \n",
    "commented Dan Ives, an analyst at Wedbush Securities. \n",
    "He added that about 40% of the 950 million iPhones in use had not been upgraded in at least three-and-a-half years, presenting a \"once-in-a-decade\" opportunity. \n",
    "In theory, the Mini could dent Apple's earnings by encouraging the public to buy a product on which it makes a smaller profit than the other phones. \n",
    "But one expert thought that unlikely. \n",
    "\"Apple successfully launched the iPhone SE in April by introducing it at a lower price point without cannibalising sales of the iPhone 11 series,\" noted Marta Pinto from IDC. \n",
    "\"There are customers out there who want a smaller, cheaper phone, so this is a proven formula that takes into account market trends.\" \n",
    "The iPhone is already the bestselling smartphone brand in the UK and the second-most popular in the world in terms of market share. \n",
    "If forecasts of pent up demand are correct, it could prompt a battle between network operators, as customers become more likely to switch. \n",
    "\"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,\" \n",
    "predicted Ben Wood from the consultancy CCS Insight. Apple typically unveils its new iPhones in September, but opted for a later date this year. \n",
    "It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. The firm's shares ended the day 2.7% lower. \n",
    "This has been linked to reports that several Chinese internet platforms opted not to carry the livestream, \n",
    "although it was still widely viewed and commented on via the social media network Sina Weibo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone 12 0 9 LAW\n",
      "Apple 11 16 ORG\n",
      "5 31 32 CARDINAL\n",
      "G\n",
      "Apple 32 39 ORG\n",
      "iPhone 12 58 67 LAW\n",
      "first 89 94 ORDINAL\n",
      "5 113 114 CARDINAL\n",
      "5.4 216 219 CARDINAL\n",
      "US 235 237 GPE\n",
      "the past year 313 326 DATE\n",
      "Apple 372 377 ORG\n",
      "2014 416 420 DATE\n",
      "5 472 473 CARDINAL\n",
      "Tim Cook 661 669 PERSON\n",
      "iPhone 12 806 815 LAW\n",
      "first 934 939 ORDINAL\n",
      "Apple 1012 1017 ORG\n",
      "Tim Cook 1083 1091 PERSON\n",
      "5 1130 1131 CARDINAL\n",
      "Dan Ives 1162 1170 PERSON\n",
      "Wedbush Securities 1186 1204 ORG\n",
      "about 40% 1221 1230 PERCENT\n",
      "the 950 million 1234 1249 MONEY\n",
      "at least three 1290 1304 CARDINAL\n",
      "Mini 1384 1388 ORG\n",
      "Apple 1400 1405 ORG\n",
      "one 1523 1526 CARDINAL\n",
      "Apple 1559 1564 ORG\n",
      "April 1604 1609 DATE\n",
      "iPhone 11 1686 1695 LAW\n",
      "Marta Pinto 1711 1722 PERSON\n",
      "UK 1931 1933 GPE\n",
      "second 1942 1948 ORDINAL\n",
      "Ben Wood 2312 2320 PERSON\n",
      "CCS Insight 2342 2353 ORG\n",
      "Apple 2355 2360 ORG\n",
      "iPhones 2387 2394 ORG\n",
      "September 2398 2407 DATE\n",
      "a later date this year 2423 2445 DATE\n",
      "the day 2586 2593 DATE\n",
      "2.7% 2594 2598 PERCENT\n",
      "Chinese 2652 2659 NORP\n",
      "Sina Weibo 2797 2807 ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your own NER model with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.language import Language\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [\n",
    "(\"A fakir from far-away India travels to Asterix's\\\n",
    "village and asks Cacofonix to save his land from\\\n",
    "drought since his singing can cause rain.\",\n",
    "{'entities':[(39, 46, \"PERSON\"),\n",
    "(66, 75, \"PERSON\")]}),\n",
    "(\"Cacofonix, accompanied by Asterix and Obelix,\\\n",
    "must travel to India aboard a magic carpet to\\\n",
    "save the life of the princess Orinjade, who is to\\\n",
    "be sacrificed to stop the drought.\",\n",
    "{'entities':[(0, 9, \"PERSON\"),\n",
    "(26, 33, \"PERSON\"),\n",
    "(38, 44, \"PERSON\"),\n",
    "(61, 66, \"LOC\"),\n",
    "(122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER=100\n",
    "OUTPUT_DIR = \"model_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(nlp, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(input_dir):\n",
    "    nlp = spacy.load(input_dir)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model):\n",
    "    if (model is not None):\n",
    "        nlp = spacy.load(model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_to_model(nlp):\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "#         ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    return (nlp, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(ner, data):\n",
    "    for sentence, annotations in data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None):\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner, DATA)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "#         optimizer = nlp.begin_training()\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                example = []\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc,annotations[i]))\n",
    "                nlp.update(example,drop = 0.5,losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(nlp, data):\n",
    "    for text, annotations in data:\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.start_char, ent.end_char,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_training(data=DATA):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    test_model(nlp, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 GPE\n",
      "Obelix 38 44 GPE\n",
      "India 60 65 GPE\n",
      "India 22 27 GPE\n",
      "Cacofonix 65 74 PERSON\n"
     ]
    }
   ],
   "source": [
    "without_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\avenger\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\spacy\\training\\iob_utils.py:142: UserWarning: [W030] Some entities could not be aligned in the text \"A fakir from far-away India travels to Asterix'svi...\" with entities \"[(39, 46, 'PERSON'), (66, 75, 'PERSON')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "c:\\users\\avenger\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\spacy\\training\\iob_utils.py:142: UserWarning: [W030] Some entities could not be aligned in the text \"Cacofonix, accompanied by Asterix and Obelix,must ...\" with entities \"[(0, 9, 'PERSON'), (26, 33, 'PERSON'), (38, 44, 'P...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 7.478134186983587}\n",
      "Losses {'ner': 7.079384921160732}\n",
      "Losses {'ner': 3.9647030948674864}\n",
      "Losses {'ner': 5.458636039398684}\n",
      "Losses {'ner': 4.716612918468921}\n",
      "Losses {'ner': 2.865049818769101}\n",
      "Losses {'ner': 2.8791390814136384}\n",
      "Losses {'ner': 3.9946259122110295}\n",
      "Losses {'ner': 3.1887023907743615}\n",
      "Losses {'ner': 1.1698232001158833}\n",
      "Losses {'ner': 0.6487613320247944}\n",
      "Losses {'ner': 0.5211085102786254}\n",
      "Losses {'ner': 0.12813913493586931}\n",
      "Losses {'ner': 0.24927174507641392}\n",
      "Losses {'ner': 1.1630187066118505}\n",
      "Losses {'ner': 0.14046760549607476}\n",
      "Losses {'ner': 1.5041894885150013}\n",
      "Losses {'ner': 0.01509463729659992}\n",
      "Losses {'ner': 0.9986602307687434}\n",
      "Losses {'ner': 0.39692032644274294}\n",
      "Losses {'ner': 0.03300307390435229}\n",
      "Losses {'ner': 3.193059555359128}\n",
      "Losses {'ner': 0.026678552374368628}\n",
      "Losses {'ner': 0.0009904454312587596}\n",
      "Losses {'ner': 0.0031130123260644377}\n",
      "Losses {'ner': 0.0003205501815065002}\n",
      "Losses {'ner': 8.215160607576788e-05}\n",
      "Losses {'ner': 6.942958621436269e-05}\n",
      "Losses {'ner': 6.663354147440102e-06}\n",
      "Losses {'ner': 0.38198439416544994}\n",
      "Losses {'ner': 1.1098342419291124e-05}\n",
      "Losses {'ner': 2.5432374157489953e-05}\n",
      "Losses {'ner': 1.6685974587162827e-05}\n",
      "Losses {'ner': 1.3761301374648014}\n",
      "Losses {'ner': 5.3695387243947e-05}\n",
      "Losses {'ner': 0.25844955730888713}\n",
      "Losses {'ner': 1.3806527057413256e-06}\n",
      "Losses {'ner': 1.8535459878429163e-06}\n",
      "Losses {'ner': 2.3229363968290667e-05}\n",
      "Losses {'ner': 0.00010810029128931318}\n",
      "Losses {'ner': 0.0022586289057066884}\n",
      "Losses {'ner': 1.517958630496996e-07}\n",
      "Losses {'ner': 0.0026488929877615157}\n",
      "Losses {'ner': 4.626995971476405e-05}\n",
      "Losses {'ner': 0.05959670250115225}\n",
      "Losses {'ner': 8.397273131569173e-07}\n",
      "Losses {'ner': 6.730317450635118e-06}\n",
      "Losses {'ner': 2.2730165990243807e-08}\n",
      "Losses {'ner': 1.676358641427226e-05}\n",
      "Losses {'ner': 6.154953784871081e-07}\n",
      "Losses {'ner': 3.5542623076507254e-07}\n",
      "Losses {'ner': 0.013791492395716014}\n",
      "Losses {'ner': 2.4137377195858037e-06}\n",
      "Losses {'ner': 3.843773749111367e-09}\n",
      "Losses {'ner': 8.449776818595845e-08}\n",
      "Losses {'ner': 2.5284108886770225e-07}\n",
      "Losses {'ner': 1.8789320043625287e-07}\n",
      "Losses {'ner': 6.18453589274636e-08}\n",
      "Losses {'ner': 9.906078024432354e-09}\n",
      "Losses {'ner': 3.88763548540382e-06}\n",
      "Losses {'ner': 5.606062608996916e-08}\n",
      "Losses {'ner': 6.255129593671871e-08}\n",
      "Losses {'ner': 9.209771205914928e-05}\n",
      "Losses {'ner': 6.362327266656631e-09}\n",
      "Losses {'ner': 0.00015683917427922032}\n",
      "Losses {'ner': 4.026202155104959e-07}\n",
      "Losses {'ner': 1.2445496611558853e-06}\n",
      "Losses {'ner': 0.0003255781819187952}\n",
      "Losses {'ner': 1.9646608167553954e-07}\n",
      "Losses {'ner': 1.0500026338292931e-06}\n",
      "Losses {'ner': 1.499794972028299e-06}\n",
      "Losses {'ner': 0.00021759829476003317}\n",
      "Losses {'ner': 1.0302344679109523e-06}\n",
      "Losses {'ner': 1.8314031023554025e-07}\n",
      "Losses {'ner': 2.078392142290952e-07}\n",
      "Losses {'ner': 1.2955754655559038e-08}\n",
      "Losses {'ner': 1.1963594200560717e-07}\n",
      "Losses {'ner': 1.1680365877379387e-07}\n",
      "Losses {'ner': 1.739675825064378e-08}\n",
      "Losses {'ner': 9.438504748194064e-06}\n",
      "Losses {'ner': 4.642035704448075e-06}\n",
      "Losses {'ner': 4.743808001780012e-09}\n",
      "Losses {'ner': 4.5384573793519085e-08}\n",
      "Losses {'ner': 2.0266516160865566e-07}\n",
      "Losses {'ner': 4.2443792524090464e-07}\n",
      "Losses {'ner': 2.7375529959823913e-08}\n",
      "Losses {'ner': 8.5639319301442e-07}\n",
      "Losses {'ner': 0.019436989744078814}\n",
      "Losses {'ner': 6.594642039482774e-10}\n",
      "Losses {'ner': 5.944840515235875e-09}\n",
      "Losses {'ner': 9.48595668034628e-08}\n",
      "Losses {'ner': 7.030630058061267e-09}\n",
      "Losses {'ner': 8.755581718357573e-10}\n",
      "Losses {'ner': 1.0388205910372303e-07}\n",
      "Losses {'ner': 2.175815977509788e-08}\n",
      "Losses {'ner': 1.3049762958708066e-09}\n",
      "Losses {'ner': 9.178419502558572e-10}\n",
      "Losses {'ner': 3.0242186676876946e-07}\n",
      "Losses {'ner': 1.3475434860281737e-09}\n",
      "Losses {'ner': 5.0739990359374606e-11}\n",
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 PERSON\n",
      "Obelix 38 44 PERSON\n",
      "Orinjade 120 128 PERSON\n",
      "Cacofonix 65 74 PERSON\n"
     ]
    }
   ],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "nlp = train_model(model)\n",
    "test_model(nlp, DATA)\n",
    "save_model(nlp, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_test(model_dir, data=DATA):\n",
    "    nlp = load_model(model_dir)\n",
    "    test_model(nlp, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cacofonix 0 9 PERSON\n",
      "Asterix 26 33 PERSON\n",
      "Obelix 38 44 PERSON\n",
      "Orinjade 120 128 PERSON\n",
      "Cacofonix 65 74 PERSON\n"
     ]
    }
   ],
   "source": [
    "load_and_test(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_LABEL = \"GAULISH_WARRIOR\"\n",
    "\n",
    "MODIFIED_DATA = [\n",
    "    (\"A fakir from far-away India travels to Asterix's village and asks Cacofonix to save his land from drought since his singing can cause rain.\", \n",
    "        {'entities':[(39, 46, NEW_LABEL), (66, 75, NEW_LABEL)]}),\n",
    "    (\"Cacofonix, accompanied by Asterix and Obelix, must travel to India aboard a magic carpet to save the life of the princess Orinjade, who is to be sacrificed to stop the drought.\", \n",
    "        {'entities':[(0, 9, NEW_LABEL), (26, 33, NEW_LABEL), (38, 44, NEW_LABEL), (61, 66, \"LOC\"), (122, 130, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_new_entity_type(model=None):\n",
    "    random.seed(0)\n",
    "    nlp = create_model(model)\n",
    "    (nlp, ner) = add_ner_to_model(nlp)\n",
    "    ner = add_labels(ner,  MODIFIED_DATA)\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        for itn in range(N_ITER):\n",
    "            random.shuffle(MODIFIED_DATA)\n",
    "            batches = minibatch(MODIFIED_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                example = []\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc,annotations[i]))\n",
    "                nlp.update(example, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 13.214513263883413}\n",
      "Losses {'ner': 11.943205485844782}\n",
      "Losses {'ner': 10.047658913317136}\n",
      "Losses {'ner': 10.250536611198628}\n",
      "Losses {'ner': 9.365057273939735}\n",
      "Losses {'ner': 7.745143656782507}\n",
      "Losses {'ner': 7.596628683256899}\n",
      "Losses {'ner': 7.240837056568812}\n",
      "Losses {'ner': 6.89866299809213}\n",
      "Losses {'ner': 6.460418787063475}\n",
      "Losses {'ner': 6.577397620826645}\n",
      "Losses {'ner': 6.8908647172466}\n",
      "Losses {'ner': 5.738239612936489}\n",
      "Losses {'ner': 5.908730105287297}\n",
      "Losses {'ner': 5.320450621482806}\n",
      "Losses {'ner': 5.469039771515014}\n",
      "Losses {'ner': 4.917628410127236}\n",
      "Losses {'ner': 4.114538946539625}\n",
      "Losses {'ner': 3.371467329473944}\n",
      "Losses {'ner': 3.5378960510679462}\n",
      "Losses {'ner': 1.7844226330639685}\n",
      "Losses {'ner': 1.0530437162337876}\n",
      "Losses {'ner': 0.8379336489529123}\n",
      "Losses {'ner': 0.15605183812954238}\n",
      "Losses {'ner': 0.4652730799435858}\n",
      "Losses {'ner': 0.5642423174735622}\n",
      "Losses {'ner': 0.02614958144626578}\n",
      "Losses {'ner': 0.03228467679831955}\n",
      "Losses {'ner': 0.007304485428261958}\n",
      "Losses {'ner': 0.0016548051575706489}\n",
      "Losses {'ner': 1.5437142115876186}\n",
      "Losses {'ner': 0.10731963285307702}\n",
      "Losses {'ner': 0.0033758018531818024}\n",
      "Losses {'ner': 0.02783362968436394}\n",
      "Losses {'ner': 0.04686179114341364}\n",
      "Losses {'ner': 0.0034796599547392676}\n",
      "Losses {'ner': 2.0313183639675083e-06}\n",
      "Losses {'ner': 4.522069321137997e-05}\n",
      "Losses {'ner': 3.8696729771637543e-07}\n",
      "Losses {'ner': 7.070302987705332e-06}\n",
      "Losses {'ner': 0.00012048540480946582}\n",
      "Losses {'ner': 1.8479412038922999e-06}\n",
      "Losses {'ner': 0.0010958196853440082}\n",
      "Losses {'ner': 0.019007331960007364}\n",
      "Losses {'ner': 3.4272710141382146e-07}\n",
      "Losses {'ner': 3.506119436603168e-06}\n",
      "Losses {'ner': 4.800066583066895e-06}\n",
      "Losses {'ner': 7.023114972570971e-05}\n",
      "Losses {'ner': 1.730700380649587e-07}\n",
      "Losses {'ner': 2.1080658335552626e-07}\n",
      "Losses {'ner': 0.00019473411562959833}\n",
      "Losses {'ner': 4.524107040512508e-06}\n",
      "Losses {'ner': 1.4352245254437383e-06}\n",
      "Losses {'ner': 1.779939815324921e-07}\n",
      "Losses {'ner': 6.562129132603081e-06}\n",
      "Losses {'ner': 1.8956411730729325e-07}\n",
      "Losses {'ner': 0.0019887764968916344}\n",
      "Losses {'ner': 2.7390563380619795e-06}\n",
      "Losses {'ner': 1.1291129382542758e-05}\n",
      "Losses {'ner': 2.9325536676061896e-05}\n",
      "Losses {'ner': 2.3957595348133773e-06}\n",
      "Losses {'ner': 1.9997057593057226e-07}\n",
      "Losses {'ner': 2.8970263984152393e-08}\n",
      "Losses {'ner': 3.8704321455005543e-07}\n",
      "Losses {'ner': 7.284614043050066e-05}\n",
      "Losses {'ner': 6.395054392247678e-08}\n",
      "Losses {'ner': 7.457250765516165e-05}\n",
      "Losses {'ner': 2.3031005369501002e-08}\n",
      "Losses {'ner': 2.4452454255508714e-07}\n",
      "Losses {'ner': 1.6741502556909137e-07}\n",
      "Losses {'ner': 3.8928019295450476e-07}\n",
      "Losses {'ner': 5.883354932370798e-06}\n",
      "Losses {'ner': 3.2232099508942857e-06}\n",
      "Losses {'ner': 6.054795781927529e-07}\n",
      "Losses {'ner': 8.259272205781449e-08}\n",
      "Losses {'ner': 1.1168568145229832e-06}\n",
      "Losses {'ner': 0.0024556813871511414}\n",
      "Losses {'ner': 0.002805607586731012}\n",
      "Losses {'ner': 2.465290223885781e-06}\n",
      "Losses {'ner': 4.305422503365042e-09}\n",
      "Losses {'ner': 3.341139596695219e-08}\n",
      "Losses {'ner': 2.3066610989160052e-08}\n",
      "Losses {'ner': 2.764131773490227e-07}\n",
      "Losses {'ner': 6.469665020397792e-08}\n",
      "Losses {'ner': 6.987704053708505e-08}\n",
      "Losses {'ner': 8.231487314681912e-06}\n",
      "Losses {'ner': 2.1753968517224624e-07}\n",
      "Losses {'ner': 1.9533638690113887e-06}\n",
      "Losses {'ner': 1.1467184586143807e-05}\n",
      "Losses {'ner': 5.680134138604428e-10}\n",
      "Losses {'ner': 1.5384765668040309e-06}\n",
      "Losses {'ner': 7.525190276571881e-07}\n",
      "Losses {'ner': 3.4375247041488813e-09}\n",
      "Losses {'ner': 0.0005342576304738676}\n",
      "Losses {'ner': 5.388988432783373e-07}\n",
      "Losses {'ner': 2.838483834643835e-09}\n",
      "Losses {'ner': 8.396698013517604e-08}\n",
      "Losses {'ner': 4.112746777682538e-08}\n",
      "Losses {'ner': 1.7986098851375614e-08}\n",
      "Losses {'ner': 0.006251519394414157}\n",
      "Cacofonix 0 9 GAULISH_WARRIOR\n",
      "Asterix 26 33 GAULISH_WARRIOR\n",
      "Obelix 38 44 GAULISH_WARRIOR\n",
      "India 60 65 LOC\n",
      "Orinjade 120 128 PERSON\n",
      "Cacofonix 65 74 GAULISH_WARRIOR\n"
     ]
    }
   ],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "nlp = train_model_new_entity_type(model)\n",
    "test_model(nlp, DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love going to school!\", \"I hate going to school!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob_sentiment(sentence):\n",
    "    result = TextBlob(sentence).sentiment\n",
    "    print(sentence, result.polarity)\n",
    "    return result.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hashtags(tweet):\n",
    "    matches = re.findall(r'#[a-z0-9]+', tweet)\n",
    "    for match in matches:\n",
    "        tweet = re.sub(match, \" \".join(segment(match)[0]), tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nltk_sentiment(sentence):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    print(sentence, ss['compound'])\n",
    "    return ss['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.6696\n",
      "I hate going to school! -0.6114\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_nltk_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love going to school! 0.625\n",
      "I hate going to school! -1.0\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentiment = get_blob_sentiment(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment for short texts using LSTM: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from wordseg import segment\n",
    "import html\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding,SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter_05.ipynb\r\n",
      "DataScientist.csv\r\n",
      "Getting-Started-with-Information-Extraction.ipynb\r\n",
      "\u001b[1m\u001b[36mbert_twitter_model\u001b[m\u001b[m\r\n",
      "data.csv\r\n",
      "\u001b[1m\u001b[36mmodel_output\u001b[m\u001b[m\r\n",
      "training.1600000.processed.noemoticon.csv\r\n",
      "twitter_english.csv\r\n",
      "twitter_tokenizer.pkl\r\n",
      "\u001b[1m\u001b[36mwordseg\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 50000\n",
    "EMBEDDING_DIM = 500\n",
    "twitter_csv = \"./training.1600000.processed.noemoticon.csv\"\n",
    "english_twitter = \"./twitter_english.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred = np.where(y_pred > 0.5, 1,0)\n",
    "    y_pred = [pred[0] for pred in y_pred]\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(text):\n",
    "    lang = \"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"None\"\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(df, save_path):\n",
    "    df['language'] = df['tweet'].progress_apply(lambda t: lang_detect(t))\n",
    "    df = df[df['language'] == 'en']\n",
    "    df.to_csv(save_path, encoding=\"latin1\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename, save_path,num_datapoints=80000):\n",
    "    df = pd.read_csv(filename, encoding=\"latin1\")\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query','username', 'tweet']\n",
    "    df = pd.concat([df.head(num_datapoints),df.tail(num_datapoints)])\n",
    "    df = filter_english(df, save_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hashtags(tweet):\n",
    "    matches = re.findall(r'#[a-z0-9]+', tweet)\n",
    "    for match in matches:\n",
    "        tweet = re.sub(match, ' '.join(segment(match)[0]), tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    #Lowercase all tweets\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: t.lower())\n",
    "    #Decode HTML\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: html.unescape(t))\n",
    "    #Remove @ mentions\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(r'@[A-Za-z0-9]+','',t))\n",
    "    #Remove URLs\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t:re.sub('https?://[A-Za-z0-9./]+','',t))\n",
    "    #Segment hashtags\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: segment_hashtags(t))\n",
    "    #Remove remaining non-alpha characters\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda t: re.sub(\"[^a-zA-Z]\", \" \", t))\n",
    "    #Re-label positive tweets with 1 instead of 4\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda t: 1 if t==4 else t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(df['tweet'].values)\n",
    "    save_tokenizer(tokenizer, 'twitter_tokenizer.pkl')\n",
    "    X = tokenizer.texts_to_sequences(df['tweet'].values)\n",
    "    X = pad_sequences(X)\n",
    "    Y = df['sentiment'].values\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=42, stratify=df['sentiment'])\n",
    "    model = Sequential()\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "    model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    loss='binary_crossentropy' #Binary in this case\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    epochs = 15\n",
    "    batch_size = 32\n",
    "    es = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.3, callbacks=es)\n",
    "    accr = model.evaluate(X_test,Y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "    model.save('twitter_model.h5')\n",
    "    #model = tf.keras.models.load_model(\"twitter_model.h5\")\n",
    "    evaluate(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000/160000 [1:18:03<00:00, 34.16it/s] \n",
      "100%|██████████| 148618/148618 [00:00<00:00, 202220.08it/s]\n",
      "100%|██████████| 148618/148618 [00:00<00:00, 229445.00it/s]\n",
      "100%|██████████| 148618/148618 [00:01<00:00, 128107.03it/s]\n",
      "100%|██████████| 148618/148618 [00:01<00:00, 119867.81it/s]\n",
      "  0%|          | 1/148618 [00:00<36:58, 66.98it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'segment_hashtags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b71637508661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"twitter_english.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d40481c4e13b>\u001b[0m in \u001b[0;36mclean_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https?://[A-Za-z0-9./]+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Segment hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msegment_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Remove remaining non-alpha characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d40481c4e13b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https?://[A-Za-z0-9./]+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#Segment hashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msegment_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Remove remaining non-alpha characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'segment_hashtags' is not defined"
     ]
    }
   ],
   "source": [
    "df = get_data(twitter_csv,\"twitter_english.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148618/148618 [00:00<00:00, 167117.89it/s]\n",
      "100%|██████████| 148618/148618 [00:01<00:00, 135966.84it/s]\n",
      "100%|██████████| 148618/148618 [00:01<00:00, 122246.91it/s]\n",
      "100%|██████████| 148618/148618 [00:01<00:00, 118408.83it/s]\n",
      "100%|██████████| 148618/148618 [00:02<00:00, 63681.86it/s]\n",
      "100%|██████████| 148618/148618 [00:03<00:00, 37882.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 776/2601 [=======>......................] - ETA: 45:04 - loss: 0.6912 - accuracy: 0.5488"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-08796fcdcc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-9437d21b0c56>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0maccr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "train_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"twitter_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.79      0.79     14955\n",
      "    positive       0.79      0.79      0.79     14787\n",
      "\n",
      "    accuracy                           0.79     29742\n",
      "   macro avg       0.79      0.79      0.79     29742\n",
      "weighted avg       0.79      0.79      0.79     29742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_column_name, gold_column_name, test_percent):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[train_column_name], df[gold_column_name], test_size=test_percent, random_state=0)\n",
    "    return (X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "DATASET_SIZE = 4000\n",
    "english_twitter = \"twitter_english.csv\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_inputs_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\"input_ids\": input_ids,\"token_type_ids\": token_type_ids,\"attention_mask\": attention_masks,}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['tweet']\n",
    "        label = row['sentiment']\n",
    "        bert_input = tokenizer.encode_plus(\n",
    "                        tweet,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = max_length, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "            )        \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_inputs_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, size=int(DATASET_SIZE/2)):\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat([df.head(size),df.tail(size)])\n",
    "    df = df.sample(frac = 1)\n",
    "    ds = encode_data(df)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_val_datasets(ds, size=DATASET_SIZE, batch_size=batch_size):\n",
    "    ds.shuffle(32)\n",
    "    train_size = int(0.7 * size)\n",
    "    val_size = int(0.15 * size)\n",
    "    test_size = int(0.15 * size)\n",
    "    train_dataset = ds.take(train_size).batch(batch_size)\n",
    "    test_dataset = ds.skip(train_size)\n",
    "    val_dataset = test_dataset.skip(test_size).batch(batch_size)\n",
    "    test_dataset = test_dataset.take(test_size).batch(batch_size)\n",
    "    return (train_dataset, test_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(ds, export_dir):\n",
    "    (train_dataset, test_dataset, val_dataset) = get_test_train_val_datasets(ds)\n",
    "    learning_rate = 2e-5\n",
    "    number_of_epochs = 1\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    bert_history = model.fit(train_dataset, epochs=number_of_epochs, validation_data=val_dataset)\n",
    "    model.save_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-3af1e5f23f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# df = read_existing_file(english_twitter)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = read_existing_file(english_twitter)\n",
    "dataset = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 00:15:00.112148 20640 filelock.py:274] Lock 1512428473704 acquired on C:\\Users\\AVENGER/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341c06539484f49a5e503d1583e8d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 00:15:01.276500 20640 filelock.py:318] Lock 1512428473704 released on C:\\Users\\AVENGER/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 00:15:02.463517 20640 filelock.py:274] Lock 1512427916032 acquired on C:\\Users\\AVENGER/.cache\\huggingface\\transformers\\775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e36522293e14a698c7702e13d436e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0624 00:15:43.306691 20640 filelock.py:318] Lock 1512427916032 released on C:\\Users\\AVENGER/.cache\\huggingface\\transformers\\775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "W0624 00:15:52.228533 20640 ag_logging.py:146] AutoGraph could not transform <bound method PythonHandler.emit of <PythonHandler stderr (NOTSET)>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpetsqpzp0.py, line 52)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method PythonHandler.emit of <PythonHandler stderr (NOTSET)>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpetsqpzp0.py, line 52)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 00:15:51.466360 20640 control_flow.py:1004] The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "W0624 00:15:52.295768 20640 control_flow.py:1004] The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "W0624 00:15:58.244610 20640 control_flow.py:1004] The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "W0624 00:15:58.257697 20640 control_flow.py:1004] The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - ETA: 0s - loss: 0.5735 - accuracy: 0.6996 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 00:57:24.386456 20640 control_flow.py:1004] The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "W0624 00:57:24.398550 20640 control_flow.py:1004] The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "88/88 [==============================] - 2619s 30s/step - loss: 0.5735 - accuracy: 0.6996 - val_loss: 0.4335 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = fine_tune_model(dataset,'bert_twitter_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_model(export_dir):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(export_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(input_text):\n",
    "    tokenized = tokenizer.tokenize(input_text)\n",
    "    bert_input = tokenizer.encode_plus(\n",
    "                    input_text,                      \n",
    "                    add_special_tokens = True, # add [CLS], [SEP]\n",
    "                    max_length = max_length, # max length of the text that can go to BERT\n",
    "                    pad_to_max_length = True, # add [PAD] tokens\n",
    "                    return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "                    return_tensors='tf'\n",
    "        )\n",
    "    return bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(model_path, tweet):\n",
    "    model = load_existing_model(model_path)\n",
    "    bert_input = encode_example(tweet)\n",
    "    tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "    tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "    new_label = np.argmax(tf_pred, axis=-1)\n",
    "    print(new_label)\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_twitter_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\avenger\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "W0624 20:14:59.366985 17532 ag_logging.py:146] AutoGraph could not transform <bound method PythonHandler.emit of <PythonHandler stderr (NOTSET)>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp5spiwv9e.py, line 52)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method PythonHandler.emit of <PythonHandler stderr (NOTSET)>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp5spiwv9e.py, line 52)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 20:14:58.869765 17532 control_flow.py:1004] The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "W0624 20:14:59.415855 17532 control_flow.py:1004] The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example('bert_twitter_model',\"I hate going to school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = []\n",
    "    for tweet in X_test:\n",
    "        bert_input = encode_example(tweet)\n",
    "        tf_output = model.predict([bert_input['input_ids'], bert_input['token_type_ids'], bert_input['attention_mask']])[0]\n",
    "        tf_pred = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "        new_label = np.argmax(tf_pred, axis=-1)\n",
    "        y_pred.append(new_label)\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_existing_model(export_dir, num_points=200):\n",
    "    model = load_existing_model(export_dir)\n",
    "    df = get_data(twitter_csv,\"twitter_english.csv\")\n",
    "    df = clean_data(df)\n",
    "    df = pd.concat([df.head(num_points),df.tail(num_points)])\n",
    "    (X_train, X_test, y_train, y_test) = split_dataset(df, 'tweet', 'sentiment')\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert_twitter_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert_twitter_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 160000/160000 [14:02<00:00, 189.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 908986.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 818795.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 497028.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 536906.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 287176.33it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 148652/148652 [00:00<00:00, 158901.50it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split_dataset() missing 1 required positional argument: 'test_percent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-705b3ae0f806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_and_evaluate_existing_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert_twitter_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-d8da521fd6fc>\u001b[0m in \u001b[0;36mload_and_evaluate_existing_model\u001b[1;34m(export_dir, num_points)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tweet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: split_dataset() missing 1 required positional argument: 'test_percent'"
     ]
    }
   ],
   "source": [
    "load_and_evaluate_existing_model(\"bert_twitter_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
