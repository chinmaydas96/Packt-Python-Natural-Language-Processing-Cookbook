{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c19b12",
   "metadata": {},
   "source": [
    "# Getting the dataset and evaluation baseline ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c72b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15745fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from Chapter01.tokenization import tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7831067",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "bbc_dataset = 'bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134264ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_token = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_token]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f88574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def get_stopwrods(stop_words):\n",
    "    stemmed_stopwords = [stemmer.stem(word) for word in stop_words]\n",
    "    stop_words = stop_words + stemmed_stopwords\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89146fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwrods(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7fc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',', \n",
    "                            quotechar='\"')\n",
    "        data_read = [row for row in reader]\n",
    "        return data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a08d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = read_in_csv(filename)\n",
    "    data_dict = {}\n",
    "    for row in data[1:]:\n",
    "        category = row[0]\n",
    "        text = row[1]\n",
    "        if category not in data_dict.keys():\n",
    "            data_dict[category] = []\n",
    "        data_dict[category].append(text)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f0c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(text, num_words=200):\n",
    "    word_list = tokenize_nltk(text)\n",
    "    word_list = [word for word in word_list if word not in stop_words and \n",
    "                re.search(\"[A-Za-z]\", word)]\n",
    "    freq_list = FreqDist(word_list)\n",
    "    print(freq_list.most_common(num_words))\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14542f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eabaed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech : 401\n",
      "business : 510\n",
      "sport : 511\n",
      "entertainment : 386\n",
      "politics : 417\n"
     ]
    }
   ],
   "source": [
    "for topic in data_dict.keys():\n",
    "    print(topic, ':', len(data_dict[topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c722a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = data_dict[\"business\"]\n",
    "sports_data = data_dict[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db91b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_string = ' '.join(business_data)\n",
    "sports_string = ' '.join(sports_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b23178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 1680), ('us', 813), ('year', 637), ('mr', 600), ('would', 463), ('also', 440), ('market', 425), ('new', 416), ('company', 415), ('growth', 384), ('last', 365), ('firm', 362), ('economy', 359), ('government', 340), ('bank', 335), ('sales', 316), ('could', 311), ('economic', 310), ('oil', 294), ('shares', 265), ('however', 256), ('world', 252), ('may', 251), ('years', 247), ('prices', 246), ('one', 243), ('chief', 236), ('two', 231), ('china', 223), ('business', 218), ('companies', 212), ('analysts', 209), ('uk', 207), ('deal', 206), ('rise', 203), ('expected', 200), ('group', 199), ('financial', 197), ('yukos', 196), ('firms', 193), ('since', 183), ('dollar', 180), ('december', 173), ('country', 173), ('months', 170), ('people', 170), ('stock', 168), ('first', 165), ('president', 165), ('three', 164), ('still', 164), ('many', 163), ('time', 159), ('european', 159), ('rate', 159), ('state', 158), ('trade', 158), ('told', 155), ('investment', 153), ('demand', 151), ('interest', 151), ('india', 151), ('quarter', 149), ('figures', 149), ('profits', 148), ('rates', 148), ('made', 147), ('countries', 146), ('spending', 146), ('executive', 145), ('news', 143), ('biggest', 142), ('month', 141), ('strong', 139), ('price', 139), ('jobs', 137), ('europe', 136), ('next', 136), ('added', 135), ('foreign', 134), ('tax', 132), ('much', 132), ('back', 131), ('rose', 131), ('offer', 130), ('euros', 130), ('budget', 129), ('according', 129), ('bid', 128), ('costs', 127), ('high', 127), ('set', 126), ('money', 126), ('exchange', 125), ('recent', 123), ('january', 122), ('investors', 122), ('part', 122), ('increase', 121), ('industry', 120), ('share', 120), ('cut', 118), ('fall', 117), ('make', 116), ('million', 116), ('hit', 116), ('week', 115), ('well', 112), ('london', 112), ('russian', 111), ('move', 110), ('japan', 110), ('take', 110), ('court', 109), ('deutsche', 109), ('former', 108), ('higher', 108), ('debt', 108), ('report', 108), ('says', 107), ('united', 106), ('production', 106), ('likely', 106), ('pay', 105), ('fell', 103), ('reported', 102), ('annual', 101), ('deficit', 101), ('minister', 100), ('say', 100), ('consumer', 99), ('russia', 99), ('despite', 98), ('sale', 98), ('earlier', 97), ('global', 95), ('bankruptcy', 95), ('exports', 95), ('markets', 95), ('eu', 95), ('south', 94), ('international', 94), ('number', 93), ('continue', 92), ('club', 92), ('shareholders', 91), ('cost', 90), ('plans', 90), ('record', 90), ('euro', 89), ('seen', 89), ('less', 89), ('public', 89), ('main', 89), ('giant', 88), ('november', 88), ('profit', 88), ('unit', 87), ('trading', 86), ('future', 86), ('end', 86), ('case', 86), ('largest', 85), ('car', 85), ('fraud', 84), ('statement', 84), ('boost', 84), ('need', 83), ('put', 83), ('sector', 83), ('work', 83), ('already', 82), ('lost', 82), ('agreed', 82), ('meeting', 82), ('takeover', 81), ('german', 81), ('buy', 81), ('gm', 80), ('finance', 80), ('low', 79), ('stake', 79), ('lse', 79), ('decision', 78), ('previous', 78), ('domestic', 78), ('value', 78), ('banks', 78), ('warned', 77), ('see', 77), ('including', 77), ('came', 77), ('saying', 77), ('current', 76), ('total', 76), ('board', 76), ('house', 75), ('national', 75), ('help', 75), ('airline', 74)]\n",
      "[('said', 941), ('game', 476), ('england', 459), ('first', 437), ('win', 415), ('would', 396), ('world', 379), ('last', 376), ('one', 355), ('two', 351), ('also', 329), ('time', 327), ('back', 318), ('players', 307), ('play', 292), ('cup', 290), ('new', 285), ('side', 270), ('ireland', 270), ('year', 267), ('team', 265), ('wales', 265), ('good', 258), ('club', 254), ('second', 248), ('six', 246), ('match', 245), ('could', 241), ('three', 230), ('set', 228), ('final', 228), ('coach', 228), ('france', 227), ('season', 223), ('made', 214), ('us', 212), ('get', 212), ('rugby', 210), ('injury', 208), ('think', 204), ('take', 201), ('chelsea', 201), ('added', 200), ('well', 198), ('great', 191), ('going', 187), ('go', 182), ('open', 181), ('victory', 180), ('got', 180), ('best', 178), ('years', 177), ('like', 176), ('next', 174), ('told', 174), ('league', 172), ('games', 171), ('nations', 171), ('make', 168), ('player', 167), ('united', 165), ('minutes', 165), ('way', 163), ('played', 161), ('since', 160), ('start', 160), ('still', 157), ('champion', 157), ('international', 156), ('arsenal', 154), ('scotland', 152), ('playing', 151), ('williams', 151), ('liverpool', 150), ('four', 147), ('want', 142), ('chance', 141), ('come', 140), ('lot', 139), ('home', 139), ('olympic', 139), ('right', 136), ('title', 136), ('five', 134), ('week', 134), ('manager', 134), ('try', 134), ('really', 133), ('squad', 133), ('former', 132), ('know', 132), ('ball', 132), ('end', 131), ('sport', 131), ('beat', 130), ('number', 129), ('top', 129), ('football', 129), ('italy', 127), ('another', 126), ('third', 125), ('race', 125), ('put', 124), ('goal', 124), ('saturday', 122), ('winning', 122), ('european', 122), ('robinson', 121), ('roddick', 120), ('see', 119), ('took', 117), ('jones', 117), ('came', 115), ('v', 114), ('mark', 114), ('grand', 114), ('bbc', 113), ('place', 112), ('return', 110), ('away', 109), ('lost', 108), ('record', 108), ('half', 108), ('champions', 107), ('boss', 106), ('left', 106), ('much', 105), ('captain', 102), ('sunday', 102), ('decision', 102), ('ahead', 101), ('athens', 100), ('andy', 99), ('better', 99), ('break', 99), ('real', 99), ('points', 99), ('australian', 99), ('face', 98), ('round', 96), ('lead', 95), ('madrid', 95), ('even', 94), ('never', 93), ('britain', 91), ('big', 90), ('premiership', 90), ('test', 90), ('newcastle', 89), ('forward', 89), ('run', 89), ('zealand', 89), ('training', 88), ('seed', 87), ('despite', 87), ('early', 87), ('people', 86), ('irish', 86), ('j', 86), ('manchester', 85), ('british', 85), ('indoor', 85), ('defeat', 84), ('championships', 84), ('long', 84), ('given', 83), ('summer', 83), ('move', 82), ('form', 82), ('gerrard', 82), ('give', 81), ('went', 81), ('pressure', 81), ('women', 81), ('slam', 81), ('drugs', 81), ('penalty', 80), ('part', 79), ('career', 79), ('hard', 79), ('days', 78), ('men', 78), ('weeks', 77), ('matches', 77), ('bit', 77), ('referee', 77), ('lions', 77), ('g', 77), ('spain', 76), ('says', 76), ('johnson', 76), ('missed', 75), ('french', 75), ('country', 75), ('fourth', 74), ('admitted', 74), ('american', 74), ('centre', 73), ('day', 73), ('work', 73)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 941, 'game': 476, 'england': 459, 'first': 437, 'win': 415, 'would': 396, 'world': 379, 'last': 376, 'one': 355, 'two': 351, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats(business_string)\n",
    "get_stats(sports_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adec4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=20000,\n",
    "                                      min_df=0.05, stop_words='english',\n",
    "                                      use_idf=True,tokenizer=tokenize_and_stem,\n",
    "                                      ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit_transform(text_list)\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4987c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, train_percentage):\n",
    "    train_test_boarder = math.ceil(train_percentage * len(data))\n",
    "    train_data = data[0:train_test_boarder]\n",
    "    test_data = data[train_test_boarder:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbfab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(names)\n",
    "    return le\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23718dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_train_data, business_test_data = split_train_test(business_data, 0.8)\n",
    "sports_train_data, sports_test_data = split_train_test(sports_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc005ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "train_data = business_train_data + sports_train_data\n",
    "tfidf_vec = create_vectorizer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e43c6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels(['business', 'sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12bead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_matrix(input_data, vectorizer, label, le):\n",
    "    vectors = vectorizer.transform(input_data).todense()\n",
    "    labels = [label] * len(input_data)\n",
    "    enc_labels = le.transform(labels)\n",
    "    return vectors, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47377591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(vectorizer, data_dict, le):\n",
    "    business_news = data_dict['business']\n",
    "    sports_news = data_dict['sports']\n",
    "    \n",
    "    business_vector, business_label = create_data_matrix(business_news, vectorizer,\n",
    "                                                        'business', le)\n",
    "    sports_vector, sports_label = create_data_matrix(sports_news, vectorizer,\n",
    "                                                        'sports', le)\n",
    "    \n",
    "    all_data_matrix = np.vstack((business_vector, sports_vector))\n",
    "    labels = np.concatenate([business_label, sports_label])\n",
    "    return all_data_matrix, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a0f36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'business':business_train_data, \n",
    "                   'sports':sports_train_data}\n",
    "test_data_dict = {'business':business_test_data, \n",
    "                  'sports':sports_test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = create_dataset(tfidf_vec, train_data_dict, le)\n",
    "(X_test, y_test) = create_dataset(tfidf_vec, test_data_dict, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d6647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trivial(X_train, y_train, X_test, y_test, le):\n",
    "    dummy_clf = DummyClassifier(strategy='uniform', random_state=0)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "    print(dummy_clf.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12684310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44607843137254904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.45      0.44      0.44       102\n",
      "      sports       0.45      0.45      0.45       102\n",
      "\n",
      "    accuracy                           0.45       204\n",
      "   macro avg       0.45      0.45      0.45       204\n",
      "weighted avg       0.45      0.45      0.45       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_trivial(X_train, y_train, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79383260",
   "metadata": {},
   "source": [
    "# Performing rule-based text classification using keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f9e1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b75a0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import repeat\n",
    "from nltk.probability import FreqDist\n",
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdfaf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_vocabulary = [\"market\", \"company\", \"growth\", \"firm\", \"economy\", \"government\", \"bank\", \"sales\", \"oil\", \"prices\", \"business\", \"uk\", \"financial\", \"dollar\", \"stock\",\"trade\", \"investment\", \"quarter\", \"profit\", \"jobs\", \"foreign\", \"tax\",\"euro\", \"budget\", \"cost\", \"money\", \"investor\", \"industry\", \"million\", \"debt\"]\n",
    "sports_vocabulary = [\"game\", \"england\", \"win\", \"player\", \"cup\", \"team\", \"club\", \"match\",\"set\", \"final\", \"coach\", \"season\", \"injury\", \"victory\", \"league\", \"play\",\"champion\", \"olympic\", \"title\", \"ball\", \"sport\", \"race\", \"football\", \"rugby\",\"tennis\", \"basketball\", \"hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_vectorizer = CountVectorizer(vocabulary=business_vocabulary)\n",
    "sports_vectorizer = CountVectorizer(vocabulary=sports_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47cdc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_dataset = './bbc-text.csv'\n",
    "stopwords_list = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77b73b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "le = get_labels(['business', 'sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dict, le):\n",
    "    data_matrix = []\n",
    "    classifications = []\n",
    "    gold_labels = []\n",
    "    for text in data_dict[\"business\"]:\n",
    "        gold_labels.append(le.transform([\"business\"]))\n",
    "        text_vector = transform(text)\n",
    "        data_matrix.append(text_vector)\n",
    "    for text in data_dict[\"sport\"]:\n",
    "        gold_labels.append(le.transform([\"sport\"]))\n",
    "        text_vector = transform(text)\n",
    "        data_matrix.append(text_vector)\n",
    "    X = np.array(data_matrix)\n",
    "    y = np.array(gold_labels)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c7ad018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    business_X = business_vectorizer.transform([text])\n",
    "    sports_X = sports_vectorizer.transform([text])\n",
    "    business_sum = sum(business_X.todense().tolist()[0])\n",
    "    sports_sum = sum(sports_X.todense().tolist()[0])\n",
    "    return np.array([business_sum, sports_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4c03671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vector, le):\n",
    "    lebel = ''\n",
    "    if vector[0] > vector[1]:\n",
    "        label = 'business'\n",
    "    else:\n",
    "        label = 'sport'\n",
    "    return le.transform([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d21b8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y):\n",
    "    y_pred = np.array(list(map(classify, X, repeat(le))))\n",
    "    print(classification_report(y, y_pred, labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c74ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       1.00      0.98      0.99       510\n",
      "       sport       0.98      1.00      0.99       511\n",
      "\n",
      "    accuracy                           0.99      1021\n",
      "   macro avg       0.99      0.99      0.99      1021\n",
      "weighted avg       0.99      0.99      0.99      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = get_data(bbc_dataset)\n",
    "(X, y) = create_dataset(data_dict, le)\n",
    "evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1b196",
   "metadata": {},
   "source": [
    "### Automated process for vacubulary selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cefad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(bbc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7445353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def divide_data(data_dict):\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    for topic in data_dict.keys():\n",
    "        text_list = data_dict[topic]\n",
    "        x_train, x_test = train_test_split(text_list, test_size=0.2)\n",
    "        train_dict[topic] = x_train\n",
    "        test_dict[topic] = x_test\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ae2cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, test_dict = divide_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "287833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "822f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizers(data_dict):\n",
    "    topic_list = list(data_dict.keys())\n",
    "    vectorizer_dict = {}\n",
    "    for topic in topic_list:\n",
    "        text_array = data_dict[topic]\n",
    "        text = \" \".join(text_array)\n",
    "        word_list = tokenize_nltk(text)\n",
    "        word_list = [word for word in word_list if \n",
    "                     word not in stop_words]\n",
    "        freq_dist = FreqDist(word_list)\n",
    "        top_200 = freq_dist.most_common(200)\n",
    "        vocab = [wtuple[0] for wtuple in top_200 if \n",
    "                 wtuple[0] not in stop_words and \n",
    "                 wtuple[0] not in string.punctuation]\n",
    "        vectorizer_dict[topic] = CountVectorizer(vocabulary=vocab)\n",
    "    return vectorizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "065fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_auto(text, vect_dict, le):\n",
    "    number_topics = len(list(vect_dict.keys()))\n",
    "    sum_list = [0]*number_topics\n",
    "    for topic in vect_dict.keys():\n",
    "        vectorizer = vect_dict[topic]\n",
    "        this_topic_matrix = vectorizer.transform([text])\n",
    "        this_topic_sum = sum(this_topic_matrix.todense().tolist()[0])\n",
    "        index = le.transform([topic])[0]\n",
    "        sum_list[index] = this_topic_sum\n",
    "    return np.array(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddac0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_auto(data_dict, le, vectorizer_dict):\n",
    "    data_matrix = []\n",
    "    classifications = []\n",
    "    gold_labels = []\n",
    "    for topic in data_dict.keys():\n",
    "        for text in data_dict[topic]:\n",
    "            gold_labels.append(le.transform([topic]))\n",
    "            text_vector = transform_auto(text, vectorizer_dict, le)\n",
    "            data_matrix.append(text_vector)\n",
    "    X = np.array(data_matrix)\n",
    "    y = np.array(gold_labels)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d19e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_auto(vector, le):\n",
    "    result = np.where(vector == np.amax(vector))\n",
    "    label = result[0][0]\n",
    "    return [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ab5f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auto(X, y, le):\n",
    "    y_pred = np.array(list(map(classify_auto, X, repeat(le))))\n",
    "    print(classification_report(y, y_pred, \n",
    "          labels=le.transform(le.classes_), \n",
    "          target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15518bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.98      0.99       102\n",
      "entertainment       0.95      0.99      0.97        78\n",
      "     politics       0.96      0.96      0.96        84\n",
      "        sport       0.98      0.97      0.98       103\n",
      "         tech       0.96      0.95      0.96        81\n",
      "\n",
      "     accuracy                           0.97       448\n",
      "    macro avg       0.97      0.97      0.97       448\n",
      " weighted avg       0.97      0.97      0.97       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = create_vectorizers(train_dict)\n",
    "X, y = create_dataset_auto(test_dict, le, vectorizers)\n",
    "evaluate_auto(X, y, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9dc28",
   "metadata": {},
   "source": [
    "# Clustering sentences using K-means – unsupervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cda75615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from Chapter01.dividing_into_sentences import divide_into_sentences_nltk\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.keyword_classification import divide_data\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb1c88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "bbc_dataset = \"./bbc-text.csv\"\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = get_stopwords(stop_words)\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74e40749",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(bbc_dataset)\n",
    "train_dict, test_dict = divide_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a40a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training = []\n",
    "all_test = []\n",
    "for topic in train_dict.keys():\n",
    "    all_training = all_training + train_dict[topic]\n",
    "for topic in test_dict.keys():\n",
    "    all_test = all_test + test_dict[topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b5cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in \n",
    "                       stop_words and t not in \n",
    "                       string.punctuation and \n",
    "                       re.search('[a-zA-Z]', t)]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e473f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(data):\n",
    "    vec = TfidfVectorizer(max_df=0.90, max_features=200000,\n",
    "                    min_df=0.05, stop_words=stop_words,\n",
    "                    use_idf=True,\n",
    "                    tokenizer=tokenize_and_stem, \n",
    "                    ngram_range=(1,3))\n",
    "    vec.fit(data)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f72de2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'could', 'might', 'must', \"n't\", 'need', 'r', 'sha', 'v', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vectorizer = create_vectorizer(all_training)\n",
    "matrix = vectorizer.transform(all_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3cab959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=5, init='k-means++', random_state=0)\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e47be7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_data, vectorizer, km):\n",
    "    predicted_data = {}\n",
    "    for topic in test_data.keys():\n",
    "        this_topic_list = test_data[topic]\n",
    "        \n",
    "        if topic not in predicted_data.keys():\n",
    "            predicted_data[topic] = {}\n",
    "        \n",
    "        for text in this_topic_list:\n",
    "            prediction = km.predict(vectorizer.transform([text]))[0]\n",
    "            if (prediction not in predicted_data[topic].keys()):\n",
    "                predicted_data[topic][prediction] = []\n",
    "            predicted_data[topic][prediction].append(text)\n",
    "    return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97261c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(predicted_data):\n",
    "    for topic in predicted_data.keys():\n",
    "        print(topic)\n",
    "        for prediction in predicted_data[topic].keys():\n",
    "            print(\"Cluster number: \", prediction, \n",
    "                  \"number of items: \", \n",
    "                  len(predicted_data[topic][prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2caf90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech\n",
      "Cluster number:  4 number of items:  71\n",
      "Cluster number:  3 number of items:  7\n",
      "Cluster number:  2 number of items:  2\n",
      "Cluster number:  0 number of items:  1\n",
      "business\n",
      "Cluster number:  2 number of items:  94\n",
      "Cluster number:  4 number of items:  8\n",
      "sport\n",
      "Cluster number:  3 number of items:  100\n",
      "Cluster number:  4 number of items:  3\n",
      "entertainment\n",
      "Cluster number:  0 number of items:  47\n",
      "Cluster number:  4 number of items:  29\n",
      "Cluster number:  3 number of items:  1\n",
      "Cluster number:  2 number of items:  1\n",
      "politics\n",
      "Cluster number:  1 number of items:  57\n",
      "Cluster number:  4 number of items:  24\n",
      "Cluster number:  2 number of items:  3\n"
     ]
    }
   ],
   "source": [
    "predicted_data = make_predictions(test_dict, vectorizer, km)\n",
    "print_report(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e59f1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_words_by_cluster(all_training, km, num_clusters):\n",
    "    clusters = km.labels_.tolist()\n",
    "    docs = {'text': all_training, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(docs, index = [clusters])\n",
    "    for cluster in range(0, num_clusters):\n",
    "        this_cluster_text = frame[frame['cluster'] == cluster]\n",
    "        all_text = \" \".join(this_cluster_text['text'].astype(str))\n",
    "        top_200 = get_most_frequent_words(all_text)\n",
    "        print(cluster)\n",
    "        print(top_200)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9ee5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(text):\n",
    "    word_list = tokenize_nltk(text)\n",
    "    word_list = [word for word in word_list if word not in stop_words and word not in string.punctuation and re.search('[a-zA-Z]', word)]\n",
    "    freq_dist = FreqDist(word_list)\n",
    "    top_200 = freq_dist.most_common(200)\n",
    "    top_200 = [word[0] for word in top_200]\n",
    "    return top_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "107e501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['film', 'best', 'said', 'awards', 'also', 'year', 'award', 'films', 'director', 'us', 'one', 'actor', 'british', 'new', 'last', 'years', 'star', 'first', 'actress', 'two', 'music', 'show', 'movie', 'people', 'uk', 'prize', 'bbc', 'song', 'comedy', 'tv', 'mr', 'made', 'including', 'time', 'three', 'role', 'nominations', 'ceremony', 'top', 'stars', 'aviator', 'oscar', 'hollywood', 'festival', 'world', 'category', 'named', 'nominated', 'office', 'would', 'million', 'starring', 'well', 'oscars', 'theatre', 'took', 'box', 'london', 'drama', 'win', 'musical', 'band', 'take', 'number', 'series', 'make', 'like', 'winners', 'children', 'book', 'life', 'love', 'could', 'place', 'second', 'think', 'hit', 'screen', 'supporting', 'baby', 'five', 'include', 'academy', 'cinema', 'see', 'story', 'big', 'album', 'winner', 'man', 'play', 'producers', 'vera', 'drake', 'day', 'good', 'international', 'angels', 'told', 'great', 'set', 'golden', 'performance', 'favourite', 'dollar', 'many', 'pop', 'week', 'directed', 'ray', 'february', 'television', 'dvd', 'taking', 'actors', 'get', 'original', 'young', 'added', 'voted', 'work', 'received', 'singer', 'says', 'much', 'nominees', 'included', 'list', 'night', 'martin', 'bafta', 'home', 'went', 'among', 'foxx', 'four', 'released', 'production', 'career', 'played', 'robbie', 'french', 'third', 'part', 'since', 'even', 'success', 'female', 'act', 'group', 'critics', 'next', 'may', 'going', 'little', 'novel', 'debut', 'due', 'american', 'past', 'feature', 'go', 'around', 'video', 'record', 'came', 'nomination', 'really', 'shows', 'game', 'version', 'release', 'held', 'producer', 'categories', 'six', 'another', 'become', 'never', 'movies', 'long', 'winning', 'honour', 'final', 'end', 'cast', 'stage', 'staunton', 'already', 'given', 'title', 'special', 'industry', 'britain', 'opera', 'scorsese', 'dicaprio', 'leigh', 'rock', 'williams']\n",
      "1\n",
      "['mr', 'said', 'would', 'labour', 'blair', 'party', 'election', 'government', 'people', 'brown', 'minister', 'also', 'howard', 'told', 'prime', 'tory', 'public', 'new', 'tax', 'could', 'leader', 'general', 'tories', 'say', 'plans', 'chancellor', 'britain', 'uk', 'one', 'bbc', 'campaign', 'secretary', 'next', 'tony', 'lib', 'says', 'time', 'ukip', 'vote', 'lord', 'last', 'home', 'michael', 'make', 'liberal', 'political', 'spokesman', 'country', 'issue', 'added', 'made', 'get', 'year', 'local', 'saying', 'kilroy-silk', 'parties', 'may', 'two', 'voters', 'kennedy', 'conservative', 'mps', 'eu', 'part', 'take', 'first', 'way', 'think', 'back', 'world', 'years', 'council', 'want', 'gordon', 'former', 'going', 'us', 'iraq', 'expected', 'like', 'blunkett', 'british', 'week', 'claims', 'spending', 'conservatives', 'taxes', 'immigration', 'dems', 'mp', 'war', 'good', 'see', 'politics', 'news', 'change', 'issues', 'much', 'believe', 'asked', 'house', 'work', 'already', 'cabinet', 'police', 'services', 'programme', 'conference', 'even', 'whether', 'bill', 'right', 'economic', 'democrats', 'charles', 'go', 'office', 'still', 'chief', 'dem', 'many', 'meeting', 'budget', 'put', 'european', 'ministers', 'stand', 'plan', 'come', 'clear', 'speech', 'set', 'policy', 'number', 'system', 'health', 'well', 'education', 'months', 'support', 'shadow', 'london', 'give', 'role', 'got', 'david', 'radio', 'cuts', 'report', 'sir', 'choice', 'action', 'elections', 'commons', 'economy', 'trust', 'money', 'members', 'answer', 'politicians', 'foreign', 'future', 'parliament', 'decision', 'today', 'monday', 'argued', 'campbell', 'advice', 'accused', 'ahead', 'denied', 'show', 'women', 'day', 'cut', 'pay', 'proposals', 'poll', 'law', 'given', 'national', 'insisted', 'chairman', 'needed', 'deal', 'warned', 'help', 'schools', 'job', 'john', 'rules', 'milburn', 'europe', 'voting', 'countries', 'later', 'democrat', 'calls']\n",
      "2\n",
      "['said', 'us', 'year', 'mr', 'market', 'would', 'also', 'company', 'new', 'growth', 'last', 'economy', 'firm', 'sales', 'bank', 'government', 'could', 'economic', 'shares', 'prices', 'may', 'oil', 'world', 'deal', 'however', 'one', 'business', 'chief', 'two', 'years', 'companies', 'china', 'uk', 'analysts', 'expected', 'people', 'group', 'since', 'rise', 'financial', 'dollar', 'stock', 'firms', 'december', 'jobs', 'news', 'figures', 'months', 'european', 'many', 'rate', 'first', 'time', 'europe', 'country', 'still', 'increase', 'profits', 'trade', 'biggest', 'strong', 'interest', 'month', 'india', 'countries', 'investment', 'demand', 'costs', 'three', 'price', 'rose', 'made', 'pay', 'executive', 'much', 'bid', 'rates', 'quarter', 'president', 'spending', 'yukos', 'told', 'offer', 'share', 'january', 'according', 'tax', 'added', 'state', 'back', 'cut', 'exchange', 'high', 'budget', 'week', 'money', 'fall', 'set', 'london', 'euros', 'part', 'recent', 'next', 'foreign', 'hit', 'investors', 'debt', 'deutsche', 'eu', 'well', 'industry', 'million', 'likely', 'cost', 'take', 'global', 'reported', 'fell', 'deficit', 'make', 'move', 'annual', 'sale', 'says', 'november', 'say', 'united', 'higher', 'gm', 'trading', 'report', 'markets', 'shareholders', 'former', 'club', 'minister', 'end', 'earlier', 'put', 'takeover', 'meeting', 'number', 'work', 'bankruptcy', 'less', 'production', 'low', 'consumer', 'although', 'despite', 'record', 'continue', 'exports', 'lse', 'main', 'russian', 'fiat', 'euro', 'japan', 'future', 'buy', 'october', 'help', 'boost', 'international', 'lost', 'stake', 'seen', 'national', 'agreed', 'including', 'even', 'friday', 'previous', 'giant', 'get', 'latest', 'businesses', 'statement', 'announced', 'value', 'need', 'plans', 'boerse', 'warned', 'finance', 'public', 'already', 'domestic', 'house', 'german', 'largest', 'general', 'fund', 'talks', 'way', 'board', 'decision', 'inflation', 'cash']\n",
      "3\n",
      "['said', 'game', 'games', 'first', 'would', 'england', 'world', 'time', 'win', 'one', 'last', 'also', 'players', 'play', 'two', 'back', 'new', 'ireland', 'cup', 'year', 'could', 'wales', 'get', 'good', 'side', 'team', 'six', 'final', 'club', 'made', 'match', 'well', 'second', 'set', 'like', 'three', 'season', 'going', 'coach', 'france', 'take', 'next', 'injury', 'added', 'got', 'think', 'playing', 'way', 'us', 'chelsea', 'make', 'played', 'go', 'years', 'best', 'rugby', 'great', 'victory', 'united', 'told', 'minutes', 'start', 'open', 'title', 'player', 'since', 'nations', 'league', 'people', 'really', 'five', 'football', 'arsenal', 'come', 'scotland', 'still', 'top', 'williams', 'right', 'home', 'manager', 'lot', 'liverpool', 'try', 'four', 'number', 'much', 'want', 'v', 'week', 'end', 'chance', 'know', 'champion', 'bbc', 'see', 'squad', 'winning', 'race', 'beat', 'former', 'italy', 'ball', 'put', 'away', 'return', 'goal', 'real', 'international', 'even', 'sport', 'saturday', 'many', 'place', 'came', 'mark', 'another', 'took', 'third', 'grand', 'better', 'european', 'lost', 'record', 'part', 'left', 'olympic', 'sunday', 'champions', 'ahead', 'boss', 'work', 'long', 'robinson', 'half', 'need', 'premiership', 'face', 'early', 'jones', 'bit', 'day', 'manchester', 'move', 'fans', 'decision', 'irish', 'australian', 'newcastle', 'athens', 'points', 'lead', 'days', 'never', 'despite', 'andy', 'british', 'given', 'captain', 'far', 'madrid', 'j', 'big', 'break', 'tournament', 'britain', 'zealand', 'used', 'past', 'however', 'run', 'look', 'forward', 'give', 'matches', 'hard', 'may', 'pressure', 'went', 'gaming', 'titles', 'summer', 'round', 'gerrard', 'say', 'always', 'something', 'g', 'form', 'weeks', 'little', 'training', 'slam', 'future', 'looking', 'test', 'city', 'says', 'seed', 'video']\n",
      "4\n",
      "['said', 'people', 'new', 'would', 'also', 'music', 'one', 'mr', 'could', 'us', 'year', 'uk', 'technology', 'mobile', 'use', 'users', 'make', 'digital', 'like', 'government', 'many', 'software', 'net', 'first', 'tv', 'show', 'service', 'get', 'time', 'phone', 'two', 'used', 'last', 'number', 'world', 'years', 'million', 'says', 'services', 'way', 'system', 'information', 'work', 'bbc', 'computer', 'security', 'internet', 'microsoft', 'broadband', 'using', 'online', 'made', 'access', 'home', 'set', 'company', 'media', 'data', 'told', 'three', 'go', 'next', 'want', 'video', 'phones', 'radio', 'research', 'firm', 'around', 'market', 'added', 'industry', 'help', 'take', 'firms', 'may', 'according', 'report', 'companies', 'well', 'see', 'much', 'part', 'networks', 'still', 'search', 'going', 'law', 'already', 'found', 'public', 'without', 'news', 'network', 'content', 'even', 'top', 'back', 'rights', 'group', 'apple', 'british', 'legal', 'europe', 'say', 'five', 'european', 'players', 'need', 'money', 'months', 'week', 'sites', 'children', 'spokesman', 'four', 'court', 'band', 'likely', 'called', 'web', 'become', 'devices', 'e-mail', 'end', 'currently', 'free', 'since', 'put', 'able', 'good', 'life', 'every', 'day', 'website', 'record', 'plans', 'different', 'site', 'find', 'think', 'consumers', 'sony', 'police', 'including', 'future', 'working', 'available', 'released', 'power', 'bill', 'download', 'come', 'place', 'bt', 'expected', 'big', 'know', 'another', 'human', 'move', 'pc', 'case', 'single', 'control', 'personal', 'month', 'sold', 'chief', 'mobiles', 'business', 'london', 'version', 'far', 'current', 'latest', 'ms', 'country', 'customers', 'making', 'album', 'better', 'attacks', 'high', 'problem', 'sales', 'windows', 'virus', 'right', 'play', 'offer', 'committee', 'however', 'due', 'second', 'messages', 'open', 'seen', 'systems', 'project']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more power to the people says hp the digital r...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam e-mails tempt net shoppers computer users...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam e-mails tempt net shoppers computer users...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china  ripe  for media explosion asia is set t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>consumer concern over rfid tags consumers are ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>labour in constituency race row labour s choic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>concerns at school diploma plan final appeals ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukip candidate suspended in probe eurosceptic ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>england children s tsar appointed the first ch...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>labour battle plan  hides blair  the tories ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1777 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  cluster\n",
       "4   more power to the people says hp the digital r...        4\n",
       "4   spam e-mails tempt net shoppers computer users...        4\n",
       "4   spam e-mails tempt net shoppers computer users...        4\n",
       "4   china  ripe  for media explosion asia is set t...        4\n",
       "4   consumer concern over rfid tags consumers are ...        4\n",
       "..                                                ...      ...\n",
       "1   labour in constituency race row labour s choic...        1\n",
       "1   concerns at school diploma plan final appeals ...        1\n",
       "1   ukip candidate suspended in probe eurosceptic ...        1\n",
       "4   england children s tsar appointed the first ch...        4\n",
       "1   labour battle plan  hides blair  the tories ha...        1\n",
       "\n",
       "[1777 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_most_common_words_by_cluster(all_training, km, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e85e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(km, open('bbc_kmeans.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e3faf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = pickle.load(open(\"bbc_kmeans.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41e2cf",
   "metadata": {},
   "source": [
    "# Using SVMs for supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0da082f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d6c7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db18dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.keyword_classification import get_labels\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fbf9c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words = get_stopwords(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d5c1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_dataset = 'bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7d9251d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dit = get_data(bbc_dataset)\n",
    "le = get_labels(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88cc75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dict, le):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for topic in data_dict:\n",
    "        label = le.transform([topic])\n",
    "        text = text + data_dict[topic]\n",
    "        this_topic_labels = \\\n",
    "        [label[0]]*len(data_dict[topic])\n",
    "        labels = labels + this_topic_labels\n",
    "    docs = {'text':text, 'label':labels}\n",
    "    frame = pd.DataFrame(docs)\n",
    "    return frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7fd3c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, train_column_name,gold_column_name, test_percent):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[train_column_name], df[gold_column_name], test_size=test_percent, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7142423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fit_vectorizer(training_text):\n",
    "    vec = TfidfVectorizer(max_df=0.90, min_df=0.05, \n",
    "                          stop_words=stop_words, use_idf=True,\n",
    "                          tokenizer=tokenize_and_stem, \n",
    "                          ngram_range=(1,3))\n",
    "    return vec.fit(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de52bb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'could', 'might', 'must', \"n't\", 'need', 'r', 'sha', 'v', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "df = create_dataset(data_dict, le)\n",
    "X_train, X_test, y_train, y_test = split_dataset(df, 'text', 'label', 20)\n",
    "vectorizer = create_and_fit_vectorizer(X_train)\n",
    "X_train = vectorizer.transform(X_train).todense()\n",
    "X_test = vectorizer.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "10ec1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_classifier(X_train, y_train):\n",
    "    clf = SVC(C=1, kernel='linear', decision_function_shape='ovo')\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9a4e274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_test, y_test, le):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6eef9c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      1.00      1.00         2\n",
      "entertainment       1.00      1.00      1.00         2\n",
      "     politics       1.00      1.00      1.00         4\n",
      "        sport       1.00      1.00      1.00        10\n",
      "         tech       1.00      1.00      1.00         2\n",
      "\n",
      "     accuracy                           1.00        20\n",
      "    macro avg       1.00      1.00      1.00        20\n",
      " weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = train_svm_classifier(X_train, y_train)\n",
    "pickle.dump(clf, open('bbc_svm.pkl','wb'))\n",
    "evaluate(clf, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "852efab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new(input_string, clf, vectorizer, le):\n",
    "    vector = vectorizer.transform([input_string]).todense()\n",
    "    prediction = clf.predict(vector)\n",
    "    print(prediction)\n",
    "    label = le.inverse_transform(prediction)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "044d22e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "['business']\n"
     ]
    }
   ],
   "source": [
    "new_example = \"\"\"iPhone 12: Apple makes jump to 5G\n",
    "Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks. \n",
    "The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen. \n",
    "The US firm bucked a wider industry downturn by increasing its handset sales over the past year. \n",
    "But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6. \n",
    "\"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, real-time interactivity and so much more,\" said chief executive Tim Cook. \n",
    "…\"\"\"\n",
    "test_new(new_example, clf, vectorizer, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503aa76",
   "metadata": {},
   "source": [
    "# Using LSTMs for supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82a854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7379943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6eeccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.keyword_classification import get_labels\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363a2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) \n",
    "tf.autograph.set_verbosity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd10557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dict, le):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for topic in data_dict:\n",
    "        label = le.transform([topic])\n",
    "        text = text + data_dict[topic]\n",
    "        this_topic_labels = \\\n",
    "        [label[0]]*len(data_dict[topic])\n",
    "        labels = labels + this_topic_labels\n",
    "    docs = {'text':text, 'label':labels}\n",
    "    frame = pd.DataFrame(docs)\n",
    "    return frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43753a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = \"\"\"iPhone 12: Apple makes jump to 5G\n",
    "Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks. \n",
    "The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen. \n",
    "The US firm bucked a wider industry downturn by increasing its handset sales over the past year. \n",
    "But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6. \n",
    "\"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, \n",
    "real-time interactivity and so much more,\" said chief executive Tim Cook. \n",
    "There has also been a cosmetic refresh this time round, with the sides of the devices getting sharper, flatter edges. \n",
    "The higher-end iPhone 12 Pro models also get bigger screens than before and a new sensor to help with low-light photography. \n",
    "However, for the first time none of the devices will be bundled with headphones or a charger. \n",
    "Apple said the move was to help reduce its impact on the environment. \"Tim Cook [has] the stage set for a super-cycle 5G product release,\" \n",
    "commented Dan Ives, an analyst at Wedbush Securities. \n",
    "He added that about 40% of the 950 million iPhones in use had not been upgraded in at least three-and-a-half years, presenting a \"once-in-a-decade\" opportunity. \n",
    "In theory, the Mini could dent Apple's earnings by encouraging the public to buy a product on which it makes a smaller profit than the other phones. \n",
    "But one expert thought that unlikely. \n",
    "\"Apple successfully launched the iPhone SE in April by introducing it at a lower price point without cannibalising sales of the iPhone 11 series,\" noted Marta Pinto from IDC. \n",
    "\"There are customers out there who want a smaller, cheaper phone, so this is a proven formula that takes into account market trends.\" \n",
    "The iPhone is already the bestselling smartphone brand in the UK and the second-most popular in the world in terms of market share. \n",
    "If forecasts of pent up demand are correct, it could prompt a battle between network operators, as customers become more likely to switch. \n",
    "\"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,\" \n",
    "predicted Ben Wood from the consultancy CCS Insight. Apple typically unveils its new iPhones in September, but opted for a later date this year. \n",
    "It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. The firm's shares ended the day 2.7% lower. \n",
    "This has been linked to reports that several Chinese internet platforms opted not to carry the livestream, \n",
    "although it was still widely viewed and commented on via the social media network Sina Weibo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8f0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = get_stopwords(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 300\n",
    "bbc_dataset = \"bbc-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d2feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(input_data, save_path):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                         lower=True)\n",
    "    tokenizer.fit_on_texts(input_data)\n",
    "    save_tokenizer(tokenizer, save_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0351d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_tokenizer(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559c87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01705f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, le):\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_test = y_test.argmax(axis=-1)\n",
    "    y_new_pred = [le.inverse_transform([value]) for \n",
    "                  value in y_pred]\n",
    "    y_new_test = [le.inverse_transform([value]) for \n",
    "                  value in y_test]\n",
    "    print(classification_report(y_new_test, \n",
    "                                y_new_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2238a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(tokenizer, input_text):\n",
    "    if (isinstance(tokenizer, str)):\n",
    "        tokenizer = load_tokenizer(tokenizer)\n",
    "    X_input = tokenizer.texts_to_sequences(input_text)\n",
    "    X_input = pad_sequences(X_input, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f665558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, le):\n",
    "    tokenizer = create_tokenizer(df['text'].values,\n",
    "                                'bbc_tokenizer.pickle')\n",
    "    \n",
    "    X = transform_text(tokenizer, df['text'].values)\n",
    "    y = pd.get_dummies(df['label']).values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size = 0.20,\n",
    "                                                        random_state = 42)\n",
    "    \n",
    "    model = Sequential()\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "    model.add(Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    loss = 'categorical_crossentropy'\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    epochs = 7\n",
    "    batch_size = 64\n",
    "    es = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_split=0.2,\n",
    "                       callbacks=[es])\n",
    "    \n",
    "    accr = model.evaluate(X_test, y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: \\\n",
    "           {:0.3f}'.format(accr[0],accr[1]))\n",
    "    model.save('bbc_model_s1.h5')\n",
    "    evaluate(model, X_test, y_test, le)\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33d9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(bbc_dataset)\n",
    "le = get_labels(list(data_dict.keys()))\n",
    "df = create_dataset(data_dict, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbfb3251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "23/23 [==============================] - 107s 4s/step - loss: 1.6078 - accuracy: 0.2296 - val_loss: 1.6059 - val_accuracy: 0.2472\n",
      "Epoch 2/7\n",
      "23/23 [==============================] - 105s 5s/step - loss: 1.6002 - accuracy: 0.3350 - val_loss: 1.6028 - val_accuracy: 0.2725\n",
      "Epoch 3/7\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.5923 - accuracy: 0.3722 - val_loss: 1.5990 - val_accuracy: 0.2921\n",
      "Epoch 4/7\n",
      "23/23 [==============================] - 101s 4s/step - loss: 1.5823 - accuracy: 0.4052 - val_loss: 1.5932 - val_accuracy: 0.3202\n",
      "Epoch 5/7\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.5656 - accuracy: 0.4136 - val_loss: 1.5820 - val_accuracy: 0.3287\n",
      "Epoch 6/7\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.5069 - accuracy: 0.3926 - val_loss: 1.4602 - val_accuracy: 0.3933\n",
      "Epoch 7/7\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.3217 - accuracy: 0.4867 - val_loss: 1.3062 - val_accuracy: 0.4270\n",
      "14/14 [==============================] - 5s 384ms/step - loss: 1.3254 - accuracy: 0.4067\n",
      "Test set\n",
      "  Loss: 1.325\n",
      "  Accuracy:            0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.63      0.81      0.71       104\n",
      "entertainment       0.00      0.00      0.00        75\n",
      "     politics       1.00      0.01      0.02        82\n",
      "        sport       0.30      0.95      0.46        99\n",
      "         tech       1.00      0.02      0.05        85\n",
      "\n",
      "     accuracy                           0.41       445\n",
      "    macro avg       0.59      0.36      0.25       445\n",
      " weighted avg       0.59      0.41      0.28       445\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(df, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72cb87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(model_path, tokenizer_path, df, le):\n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(tokenizer_path)\n",
    "    X = transform_text(tokenizer, df['text'].values)\n",
    "    y = pd.get_dummies(df['label']).values\n",
    "    evaluate(model, X, y, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "189e0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(model, tokenizer, le, text_input):\n",
    "    X_example = transform_text(tokenizer, [new_example])\n",
    "    label_array = model.predict(X_example)\n",
    "    new_label = np.argmax(label_array, axis=-1)\n",
    "    print(new_label)\n",
    "    print(le.inverse_transform(new_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8f6ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'bbc_model_s1.h5'\n",
    "tokenizer_path = 'bbc_tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a93f7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(tokenizer_path)\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a2b0df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "['business']\n"
     ]
    }
   ],
   "source": [
    "test_new_example(model, tokenizer, le, new_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e6994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
