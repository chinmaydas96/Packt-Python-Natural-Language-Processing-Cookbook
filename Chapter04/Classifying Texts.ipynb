{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c19b12",
   "metadata": {},
   "source": [
    "# Getting the dataset and evaluation baseline ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c72b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15745fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from Chapter01.tokenization import tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7831067",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "bbc_dataset = 'bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134264ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_token = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_token]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f88574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def get_stopwrods(stop_words):\n",
    "    stemmed_stopwords = [stemmer.stem(word) for word in stop_words]\n",
    "    stop_words = stop_words + stemmed_stopwords\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89146fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwrods(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7fc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',', \n",
    "                            quotechar='\"')\n",
    "        data_read = [row for row in reader]\n",
    "        return data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a08d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = read_in_csv(filename)\n",
    "    data_dict = {}\n",
    "    for row in data[1:]:\n",
    "        category = row[0]\n",
    "        text = row[1]\n",
    "        if category not in data_dict.keys():\n",
    "            data_dict[category] = []\n",
    "        data_dict[category].append(text)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f0c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(text, num_words=200):\n",
    "    word_list = tokenize_nltk(text)\n",
    "    word_list = [word for word in word_list if word not in stop_words and \n",
    "                re.search(\"[A-Za-z]\", word)]\n",
    "    freq_list = FreqDist(word_list)\n",
    "    print(freq_list.most_common(num_words))\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14542f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eabaed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech : 401\n",
      "business : 510\n",
      "sport : 511\n",
      "entertainment : 386\n",
      "politics : 417\n"
     ]
    }
   ],
   "source": [
    "for topic in data_dict.keys():\n",
    "    print(topic, ':', len(data_dict[topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c722a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = data_dict[\"business\"]\n",
    "sports_data = data_dict[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db91b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_string = ' '.join(business_data)\n",
    "sports_string = ' '.join(sports_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b23178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 1680), ('us', 813), ('year', 637), ('mr', 600), ('would', 463), ('also', 440), ('market', 425), ('new', 416), ('company', 415), ('growth', 384), ('last', 365), ('firm', 362), ('economy', 359), ('government', 340), ('bank', 335), ('sales', 316), ('could', 311), ('economic', 310), ('oil', 294), ('shares', 265), ('however', 256), ('world', 252), ('may', 251), ('years', 247), ('prices', 246), ('one', 243), ('chief', 236), ('two', 231), ('china', 223), ('business', 218), ('companies', 212), ('analysts', 209), ('uk', 207), ('deal', 206), ('rise', 203), ('expected', 200), ('group', 199), ('financial', 197), ('yukos', 196), ('firms', 193), ('since', 183), ('dollar', 180), ('december', 173), ('country', 173), ('months', 170), ('people', 170), ('stock', 168), ('first', 165), ('president', 165), ('three', 164), ('still', 164), ('many', 163), ('time', 159), ('european', 159), ('rate', 159), ('state', 158), ('trade', 158), ('told', 155), ('investment', 153), ('demand', 151), ('interest', 151), ('india', 151), ('quarter', 149), ('figures', 149), ('profits', 148), ('rates', 148), ('made', 147), ('countries', 146), ('spending', 146), ('executive', 145), ('news', 143), ('biggest', 142), ('month', 141), ('strong', 139), ('price', 139), ('jobs', 137), ('europe', 136), ('next', 136), ('added', 135), ('foreign', 134), ('tax', 132), ('much', 132), ('back', 131), ('rose', 131), ('offer', 130), ('euros', 130), ('budget', 129), ('according', 129), ('bid', 128), ('costs', 127), ('high', 127), ('set', 126), ('money', 126), ('exchange', 125), ('recent', 123), ('january', 122), ('investors', 122), ('part', 122), ('increase', 121), ('industry', 120), ('share', 120), ('cut', 118), ('fall', 117), ('make', 116), ('million', 116), ('hit', 116), ('week', 115), ('well', 112), ('london', 112), ('russian', 111), ('move', 110), ('japan', 110), ('take', 110), ('court', 109), ('deutsche', 109), ('former', 108), ('higher', 108), ('debt', 108), ('report', 108), ('says', 107), ('united', 106), ('production', 106), ('likely', 106), ('pay', 105), ('fell', 103), ('reported', 102), ('annual', 101), ('deficit', 101), ('minister', 100), ('say', 100), ('consumer', 99), ('russia', 99), ('despite', 98), ('sale', 98), ('earlier', 97), ('global', 95), ('bankruptcy', 95), ('exports', 95), ('markets', 95), ('eu', 95), ('south', 94), ('international', 94), ('number', 93), ('continue', 92), ('club', 92), ('shareholders', 91), ('cost', 90), ('plans', 90), ('record', 90), ('euro', 89), ('seen', 89), ('less', 89), ('public', 89), ('main', 89), ('giant', 88), ('november', 88), ('profit', 88), ('unit', 87), ('trading', 86), ('future', 86), ('end', 86), ('case', 86), ('largest', 85), ('car', 85), ('fraud', 84), ('statement', 84), ('boost', 84), ('need', 83), ('put', 83), ('sector', 83), ('work', 83), ('already', 82), ('lost', 82), ('agreed', 82), ('meeting', 82), ('takeover', 81), ('german', 81), ('buy', 81), ('gm', 80), ('finance', 80), ('low', 79), ('stake', 79), ('lse', 79), ('decision', 78), ('previous', 78), ('domestic', 78), ('value', 78), ('banks', 78), ('warned', 77), ('see', 77), ('including', 77), ('came', 77), ('saying', 77), ('current', 76), ('total', 76), ('board', 76), ('house', 75), ('national', 75), ('help', 75), ('airline', 74)]\n",
      "[('said', 941), ('game', 476), ('england', 459), ('first', 437), ('win', 415), ('would', 396), ('world', 379), ('last', 376), ('one', 355), ('two', 351), ('also', 329), ('time', 327), ('back', 318), ('players', 307), ('play', 292), ('cup', 290), ('new', 285), ('side', 270), ('ireland', 270), ('year', 267), ('team', 265), ('wales', 265), ('good', 258), ('club', 254), ('second', 248), ('six', 246), ('match', 245), ('could', 241), ('three', 230), ('set', 228), ('final', 228), ('coach', 228), ('france', 227), ('season', 223), ('made', 214), ('us', 212), ('get', 212), ('rugby', 210), ('injury', 208), ('think', 204), ('take', 201), ('chelsea', 201), ('added', 200), ('well', 198), ('great', 191), ('going', 187), ('go', 182), ('open', 181), ('victory', 180), ('got', 180), ('best', 178), ('years', 177), ('like', 176), ('next', 174), ('told', 174), ('league', 172), ('games', 171), ('nations', 171), ('make', 168), ('player', 167), ('united', 165), ('minutes', 165), ('way', 163), ('played', 161), ('since', 160), ('start', 160), ('still', 157), ('champion', 157), ('international', 156), ('arsenal', 154), ('scotland', 152), ('playing', 151), ('williams', 151), ('liverpool', 150), ('four', 147), ('want', 142), ('chance', 141), ('come', 140), ('lot', 139), ('home', 139), ('olympic', 139), ('right', 136), ('title', 136), ('five', 134), ('week', 134), ('manager', 134), ('try', 134), ('really', 133), ('squad', 133), ('former', 132), ('know', 132), ('ball', 132), ('end', 131), ('sport', 131), ('beat', 130), ('number', 129), ('top', 129), ('football', 129), ('italy', 127), ('another', 126), ('third', 125), ('race', 125), ('put', 124), ('goal', 124), ('saturday', 122), ('winning', 122), ('european', 122), ('robinson', 121), ('roddick', 120), ('see', 119), ('took', 117), ('jones', 117), ('came', 115), ('v', 114), ('mark', 114), ('grand', 114), ('bbc', 113), ('place', 112), ('return', 110), ('away', 109), ('lost', 108), ('record', 108), ('half', 108), ('champions', 107), ('boss', 106), ('left', 106), ('much', 105), ('captain', 102), ('sunday', 102), ('decision', 102), ('ahead', 101), ('athens', 100), ('andy', 99), ('better', 99), ('break', 99), ('real', 99), ('points', 99), ('australian', 99), ('face', 98), ('round', 96), ('lead', 95), ('madrid', 95), ('even', 94), ('never', 93), ('britain', 91), ('big', 90), ('premiership', 90), ('test', 90), ('newcastle', 89), ('forward', 89), ('run', 89), ('zealand', 89), ('training', 88), ('seed', 87), ('despite', 87), ('early', 87), ('people', 86), ('irish', 86), ('j', 86), ('manchester', 85), ('british', 85), ('indoor', 85), ('defeat', 84), ('championships', 84), ('long', 84), ('given', 83), ('summer', 83), ('move', 82), ('form', 82), ('gerrard', 82), ('give', 81), ('went', 81), ('pressure', 81), ('women', 81), ('slam', 81), ('drugs', 81), ('penalty', 80), ('part', 79), ('career', 79), ('hard', 79), ('days', 78), ('men', 78), ('weeks', 77), ('matches', 77), ('bit', 77), ('referee', 77), ('lions', 77), ('g', 77), ('spain', 76), ('says', 76), ('johnson', 76), ('missed', 75), ('french', 75), ('country', 75), ('fourth', 74), ('admitted', 74), ('american', 74), ('centre', 73), ('day', 73), ('work', 73)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 941, 'game': 476, 'england': 459, 'first': 437, 'win': 415, 'would': 396, 'world': 379, 'last': 376, 'one': 355, 'two': 351, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats(business_string)\n",
    "get_stats(sports_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adec4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=20000,\n",
    "                                      min_df=0.05, stop_words='english',\n",
    "                                      use_idf=True,tokenizer=tokenize_and_stem,\n",
    "                                      ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit_transform(text_list)\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4987c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, train_percentage):\n",
    "    train_test_boarder = math.ceil(train_percentage * len(data))\n",
    "    train_data = data[0:train_test_boarder]\n",
    "    test_data = data[train_test_boarder:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(names)\n",
    "    return le\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23718dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_train_data, business_test_data = split_train_test(business_data, 0.8)\n",
    "sports_train_data, sports_test_data = split_train_test(sports_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc005ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "train_data = business_train_data + sports_train_data\n",
    "tfidf_vec = create_vectorizer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43c6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels(['business', 'sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12bead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_matrix(input_data, vectorizer, label, le):\n",
    "    vectors = vectorizer.transform(input_data).todense()\n",
    "    labels = [label] * len(input_data)\n",
    "    enc_labels = le.transform(labels)\n",
    "    return vectors, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47377591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(vectorizer, data_dict, le):\n",
    "    business_news = data_dict['business']\n",
    "    sports_news = data_dict['sports']\n",
    "    \n",
    "    business_vector, business_label = create_data_matrix(business_news, vectorizer,\n",
    "                                                        'business', le)\n",
    "    sports_vector, sports_label = create_data_matrix(sports_news, vectorizer,\n",
    "                                                        'sports', le)\n",
    "    \n",
    "    all_data_matrix = np.vstack((business_vector, sports_vector))\n",
    "    labels = np.concatenate([business_label, sports_label])\n",
    "    return all_data_matrix, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0f36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'business':business_train_data, \n",
    "                   'sports':sports_train_data}\n",
    "test_data_dict = {'business':business_test_data, \n",
    "                  'sports':sports_test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = create_dataset(tfidf_vec, train_data_dict, le)\n",
    "(X_test, y_test) = create_dataset(tfidf_vec, test_data_dict, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d6647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trivial(X_train, y_train, X_test, y_test, le):\n",
    "    dummy_clf = DummyClassifier(strategy='uniform', random_state=0)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "    print(dummy_clf.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12684310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44607843137254904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.45      0.44      0.44       102\n",
      "      sports       0.45      0.45      0.45       102\n",
      "\n",
      "    accuracy                           0.45       204\n",
      "   macro avg       0.45      0.45      0.45       204\n",
      "weighted avg       0.45      0.45      0.45       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_trivial(X_train, y_train, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79383260",
   "metadata": {},
   "source": [
    "# Performing rule-based text classification using keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f9e1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b75a0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import repeat\n",
    "from nltk.probability import FreqDist\n",
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdfaf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_vocabulary = [\"market\", \"company\", \"growth\", \"firm\", \"economy\", \"government\", \"bank\", \"sales\", \"oil\", \"prices\", \"business\", \"uk\", \"financial\", \"dollar\", \"stock\",\"trade\", \"investment\", \"quarter\", \"profit\", \"jobs\", \"foreign\", \"tax\",\"euro\", \"budget\", \"cost\", \"money\", \"investor\", \"industry\", \"million\", \"debt\"]\n",
    "sports_vocabulary = [\"game\", \"england\", \"win\", \"player\", \"cup\", \"team\", \"club\", \"match\",\"set\", \"final\", \"coach\", \"season\", \"injury\", \"victory\", \"league\", \"play\",\"champion\", \"olympic\", \"title\", \"ball\", \"sport\", \"race\", \"football\", \"rugby\",\"tennis\", \"basketball\", \"hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e8bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_vectorizer = CountVectorizer(vocabulary=business_vocabulary)\n",
    "sports_vectorizer = CountVectorizer(vocabulary=sports_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47cdc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_dataset = './bbc-text.csv'\n",
    "stopwords_list = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77b73b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "le = get_labels(['business', 'sport'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c28719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dict, le):\n",
    "    data_matrix = []\n",
    "    classifications = []\n",
    "    gold_labels = []\n",
    "    for text in data_dict[\"business\"]:\n",
    "        gold_labels.append(le.transform([\"business\"]))\n",
    "        text_vector = transform(text)\n",
    "        data_matrix.append(text_vector)\n",
    "    for text in data_dict[\"sport\"]:\n",
    "        gold_labels.append(le.transform([\"sport\"]))\n",
    "        text_vector = transform(text)\n",
    "        data_matrix.append(text_vector)\n",
    "    X = np.array(data_matrix)\n",
    "    y = np.array(gold_labels)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c7ad018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    business_X = business_vectorizer.transform([text])\n",
    "    sports_X = sports_vectorizer.transform([text])\n",
    "    business_sum = sum(business_X.todense().tolist()[0])\n",
    "    sports_sum = sum(sports_X.todense().tolist()[0])\n",
    "    return np.array([business_sum, sports_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4c03671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vector, le):\n",
    "    lebel = ''\n",
    "    if vector[0] > vector[1]:\n",
    "        label = 'business'\n",
    "    else:\n",
    "        label = 'sport'\n",
    "    return le.transform([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d21b8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y):\n",
    "    y_pred = np.array(list(map(classify, X, repeat(le))))\n",
    "    print(classification_report(y, y_pred, labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c74ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       1.00      0.98      0.99       510\n",
      "       sport       0.98      1.00      0.99       511\n",
      "\n",
      "    accuracy                           0.99      1021\n",
      "   macro avg       0.99      0.99      0.99      1021\n",
      "weighted avg       0.99      0.99      0.99      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = get_data(bbc_dataset)\n",
    "(X, y) = create_dataset(data_dict, le)\n",
    "evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1b196",
   "metadata": {},
   "source": [
    "### Automated process for vacubulary selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cefad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(bbc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7445353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def divide_data(data_dict):\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    for topic in data_dict.keys():\n",
    "        text_list = data_dict[topic]\n",
    "        x_train, x_test = train_test_split(text_list, test_size=0.2)\n",
    "        train_dict[topic] = x_train\n",
    "        test_dict[topic] = x_test\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ae2cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict, test_dict = divide_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "287833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "822f2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizers(data_dict):\n",
    "    topic_list = list(data_dict.keys())\n",
    "    vectorizer_dict = {}\n",
    "    for topic in topic_list:\n",
    "        text_array = data_dict[topic]\n",
    "        text = \" \".join(text_array)\n",
    "        word_list = tokenize_nltk(text)\n",
    "        word_list = [word for word in word_list if \n",
    "                     word not in stop_words]\n",
    "        freq_dist = FreqDist(word_list)\n",
    "        top_200 = freq_dist.most_common(200)\n",
    "        vocab = [wtuple[0] for wtuple in top_200 if \n",
    "                 wtuple[0] not in stop_words and \n",
    "                 wtuple[0] not in string.punctuation]\n",
    "        vectorizer_dict[topic] = CountVectorizer(vocabulary=vocab)\n",
    "    return vectorizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "065fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_auto(text, vect_dict, le):\n",
    "    number_topics = len(list(vect_dict.keys()))\n",
    "    sum_list = [0]*number_topics\n",
    "    for topic in vect_dict.keys():\n",
    "        vectorizer = vect_dict[topic]\n",
    "        this_topic_matrix = vectorizer.transform([text])\n",
    "        this_topic_sum = sum(this_topic_matrix.todense().tolist()[0])\n",
    "        index = le.transform([topic])[0]\n",
    "        sum_list[index] = this_topic_sum\n",
    "    return np.array(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ddac0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_auto(data_dict, le, vectorizer_dict):\n",
    "    data_matrix = []\n",
    "    classifications = []\n",
    "    gold_labels = []\n",
    "    for topic in data_dict.keys():\n",
    "        for text in data_dict[topic]:\n",
    "            gold_labels.append(le.transform([topic]))\n",
    "            text_vector = transform_auto(text, vectorizer_dict, le)\n",
    "            data_matrix.append(text_vector)\n",
    "    X = np.array(data_matrix)\n",
    "    y = np.array(gold_labels)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d19e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_auto(vector, le):\n",
    "    result = np.where(vector == np.amax(vector))\n",
    "    label = result[0][0]\n",
    "    return [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ab5f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auto(X, y, le):\n",
    "    y_pred = np.array(list(map(classify_auto, X, repeat(le))))\n",
    "    print(classification_report(y, y_pred, \n",
    "          labels=le.transform(le.classes_), \n",
    "          target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15518bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.93      0.91      0.92       102\n",
      "entertainment       0.96      0.95      0.95        78\n",
      "     politics       0.87      0.98      0.92        84\n",
      "        sport       0.97      0.97      0.97       103\n",
      "         tech       0.96      0.88      0.92        81\n",
      "\n",
      "     accuracy                           0.94       448\n",
      "    macro avg       0.94      0.94      0.94       448\n",
      " weighted avg       0.94      0.94      0.94       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = create_vectorizers(train_dict)\n",
    "X, y = create_dataset_auto(test_dict, le, vectorizers)\n",
    "evaluate_auto(X, y, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9dc28",
   "metadata": {},
   "source": [
    "# Clustering sentences using K-means – unsupervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cda75615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.probability import FreqDist\n",
    "from Chapter01.tokenization import tokenize_nltk\n",
    "from Chapter01.dividing_into_sentences import divide_into_sentences_nltk\n",
    "from Chapter04.preprocess_bbc_dataset import get_data\n",
    "from Chapter04.keyword_classification import divide_data\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb1c88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "bbc_dataset = \"./bbc-text.csv\"\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = get_stopwords(stop_words)\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74e40749",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(bbc_dataset)\n",
    "train_dict, test_dict = divide_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7a40a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training = []\n",
    "all_test = []\n",
    "for topic in train_dict.keys():\n",
    "    all_training = all_training + train_dict[topic]\n",
    "for topic in test_dict.keys():\n",
    "    all_test = all_test + test_dict[topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2b5cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in \n",
    "                       stop_words and t not in \n",
    "                       string.punctuation and \n",
    "                       re.search('[a-zA-Z]', t)]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e473f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(data):\n",
    "    vec = TfidfVectorizer(max_df=0.90, max_features=200000,\n",
    "                    min_df=0.05, stop_words=stop_words,\n",
    "                    use_idf=True,\n",
    "                    tokenizer=tokenize_and_stem, \n",
    "                    ngram_range=(1,3))\n",
    "    vec.fit(data)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f72de2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'could', 'might', 'must', \"n't\", 'need', 'r', 'sha', 'v', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "vectorizer = create_vectorizer(all_training)\n",
    "matrix = vectorizer.transform(all_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3cab959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=5, init='k-means++', random_state=0)\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e47be7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_data, vectorizer, km):\n",
    "    predicted_data = {}\n",
    "    for topic in test_data.keys():\n",
    "        this_topic_list = test_data[topic]\n",
    "        \n",
    "        if topic not in predicted_data.keys():\n",
    "            predicted_data[topic] = {}\n",
    "        \n",
    "        for text in this_topic_list:\n",
    "            prediction = km.predict(vectorizer.transform([text]))[0]\n",
    "            if (prediction not in predicted_data[topic].keys()):\n",
    "                predicted_data[topic][prediction] = []\n",
    "            predicted_data[topic][prediction].append(text)\n",
    "    return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97261c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(predicted_data):\n",
    "    for topic in predicted_data.keys():\n",
    "        print(topic)\n",
    "        for prediction in predicted_data[topic].keys():\n",
    "            print(\"Cluster number: \", prediction, \n",
    "                  \"number of items: \", \n",
    "                  len(predicted_data[topic][prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b2caf90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech\n",
      "Cluster number:  1 number of items:  70\n",
      "Cluster number:  4 number of items:  5\n",
      "Cluster number:  2 number of items:  3\n",
      "Cluster number:  0 number of items:  3\n",
      "business\n",
      "Cluster number:  2 number of items:  101\n",
      "Cluster number:  3 number of items:  1\n",
      "sport\n",
      "Cluster number:  4 number of items:  99\n",
      "Cluster number:  0 number of items:  1\n",
      "Cluster number:  2 number of items:  3\n",
      "entertainment\n",
      "Cluster number:  0 number of items:  51\n",
      "Cluster number:  2 number of items:  23\n",
      "Cluster number:  1 number of items:  3\n",
      "Cluster number:  4 number of items:  1\n",
      "politics\n",
      "Cluster number:  3 number of items:  58\n",
      "Cluster number:  2 number of items:  25\n",
      "Cluster number:  4 number of items:  1\n"
     ]
    }
   ],
   "source": [
    "predicted_data = make_predictions(test_dict, vectorizer, km)\n",
    "print_report(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e59f1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_words_by_cluster(all_training, km, num_clusters):\n",
    "    clusters = km.labels_.tolist()\n",
    "    docs = {'text': all_training, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(docs, index = [clusters])\n",
    "    for cluster in range(0, num_clusters):\n",
    "        this_cluster_text = frame[frame['cluster'] == cluster]\n",
    "        all_text = \" \".join(this_cluster_text['text'].astype(str))\n",
    "        top_200 = get_most_frequent_words(all_text)\n",
    "        print(cluster)\n",
    "        print(top_200)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c9ee5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(text):\n",
    "    word_list = tokenize_nltk(text)\n",
    "    word_list = [word for word in word_list if word not in stop_words and word not in string.punctuation and re.search('[a-zA-Z]', word)]\n",
    "    freq_dist = FreqDist(word_list)\n",
    "    top_200 = freq_dist.most_common(200)\n",
    "    top_200 = [word[0] for word in top_200]\n",
    "    return top_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "107e501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['film', 'best', 'said', 'also', 'year', 'one', 'awards', 'us', 'music', 'new', 'show', 'award', 'director', 'last', 'films', 'uk', 'first', 'number', 'years', 'top', 'actor', 'band', 'british', 'star', 'song', 'two', 'tv', 'album', 'three', 'actress', 'including', 'festival', 'bbc', 'people', 'time', 'would', 'prize', 'stars', 'made', 'movie', 'world', 'oscar', 'aviator', 'mr', 'comedy', 'like', 'rock', 'million', 'musical', 'well', 'nominations', 'win', 'record', 'series', 'role', 'singer', 'hit', 'life', 'ceremony', 'hollywood', 'make', 'week', 'took', 'theatre', 'office', 'oscars', 'five', 'box', 'place', 'named', 'love', 'london', 'book', 'radio', 'starring', 'children', 'set', 'four', 'academy', 'chart', 'play', 'nominated', 'success', 'pop', 'could', 'category', 'second', 'include', 'man', 'take', 'list', 'drama', 'night', 'original', 'day', 'good', 'ray', 'charles', 'think', 'single', 'baby', 'performance', 'third', 'told', 'industry', 'next', 'many', 'songs', 'foxx', 'see', 'go', 'released', 'added', 'big', 'international', 'career', 'group', 'february', 'become', 'among', 'great', 'story', 'get', 'martin', 'act', 'dollar', 'taking', 'work', 'came', 'live', 'debut', 'screen', 'never', 'winners', 'went', 'played', 'young', 'included', 'release', 'american', 'way', 'producer', 'cinema', 'golden', 'producers', 'version', 'around', 'winner', 'john', 'fans', 'much', 'television', 'since', 'part', 'supporting', 'angels', 'may', 'going', 'later', 'shows', 'jamie', 'michael', 'back', 'elvis', 'stage', 'production', 'family', 'critics', 'held', 'swank', 'charts', 'directed', 'former', 'west', 'york', 'actors', 'scorsese', 'eastwood', 'already', 'really', 'end', 'artists', 'robbie', 'says', 'audience', 'vera', 'drake', 'home', 'saw', 'want', 'sunday', 'school', 'special', 'died', 'still', 'making', 'favourite', 'history', 'past', 'angeles']\n",
      "1\n",
      "['said', 'people', 'mr', 'mobile', 'technology', 'also', 'could', 'new', 'music', 'one', 'would', 'digital', 'use', 'users', 'net', 'phone', 'software', 'many', 'games', 'us', 'like', 'make', 'online', 'get', 'used', 'year', 'uk', 'service', 'tv', 'computer', 'video', 'way', 'microsoft', 'services', 'phones', 'world', 'time', 'information', 'internet', 'million', 'using', 'first', 'market', 'media', 'content', 'data', 'system', 'work', 'broadband', 'search', 'firms', 'says', 'security', 'firm', 'research', 'players', 'companies', 'access', 'number', 'game', 'two', 'according', 'much', 'networks', 'news', 'take', 'industry', 'last', 'help', 'apple', 'bbc', 'around', 'home', 'want', 'web', 'see', 'devices', 'company', 'site', 'next', 'even', 'different', 'already', 'consumers', 'website', 'well', 'years', 'going', 'able', 'network', 'show', 'made', 'mobiles', 'go', 'sony', 'pc', 'e-mail', 'report', 'set', 'told', 'found', 'sites', 'play', 'part', 'gaming', 'customers', 'radio', 'technologies', 'download', 'europe', 'may', 'future', 'need', 'windows', 'google', 'end', 'still', 'systems', 'device', 'months', 'added', 'control', 'pcs', 'images', 'dvd', 'hard', 'consumer', 'available', 'likely', 'project', 'move', 'money', 'popular', 'bt', 'player', 'become', 'every', 'wireless', 'gadgets', 'good', 'find', 'three', 'ipod', 'gadget', 'portable', 'currently', 'machines', 'offer', 'files', 'messages', 'camera', 'getting', 'via', 'version', 'free', 'making', 'personal', 'websites', 'put', 'programs', 'called', 'looking', 'although', 'latest', 'program', 'attacks', 'computers', 'big', 'say', 'come', 'better', 'virus', 'without', 'launch', 'important', 'look', 'power', 'sales', 'know', 'group', 'life', 'let', 'working', 'means', 'launched', 'nintendo', 'ways', 'small', 'current', 'month', 'entertainment', 'high-definition', 'others', 'generation', 'several', 'problem', 'huge', 'spam', 'something', 'operators']\n",
      "2\n",
      "['said', 'us', 'would', 'mr', 'year', 'new', 'also', 'government', 'could', 'last', 'company', 'market', 'one', 'firm', 'growth', 'two', 'economy', 'years', 'people', 'uk', 'may', 'bank', 'oil', 'world', 'economic', 'however', 'sales', 'deal', 'chief', 'shares', 'told', 'group', 'law', 'china', 'first', 'european', 'three', 'time', 'since', 'expected', 'made', 'firms', 'financial', 'country', 'court', 'business', 'many', 'added', 'prices', 'companies', 'still', 'rise', 'figures', 'months', 'make', 'eu', 'analysts', 'executive', 'dollar', 'foreign', 'money', 'president', 'trade', 'december', 'says', 'week', 'news', 'state', 'countries', 'million', 'back', 'next', 'part', 'public', 'minister', 'report', 'case', 'interest', 'month', 'say', 'europe', 'set', 'yukos', 'take', 'stock', 'london', 'work', 'high', 'biggest', 'costs', 'offer', 'plans', 'rate', 'profits', 'decision', 'rights', 'according', 'well', 'legal', 'move', 'former', 'jobs', 'industry', 'much', 'bid', 'price', 'quarter', 'tax', 'demand', 'strong', 'india', 'bbc', 'hit', 'rates', 'police', 'increase', 'pay', 'home', 'spokesman', 'spending', 'help', 'investment', 'japan', 'recent', 'share', 'likely', 'cut', 'number', 'put', 'january', 'earlier', 'saying', 'euros', 'higher', 'british', 'office', 'rose', 'get', 'five', 'exchange', 'future', 'show', 'record', 'club', 'budget', 'including', 'action', 'see', 'despite', 'four', 'reported', 'international', 'cost', 'need', 'statement', 'agreed', 'united', 'fall', 'already', 'bill', 'ms', 'commission', 'go', 'national', 'sale', 'seen', 'human', 'current', 'due', 'debt', 'continue', 'secretary', 'production', 'south', 'even', 'like', 'way', 'november', 'investors', 'less', 'major', 'services', 'end', 'warned', 'deficit', 'house', 'local', 'markets', 'russia', 'came', 'exports', 'used', 'going', 'currently', 'lost', 'global', 'annual', 'use', 'ministers', 'wednesday']\n",
      "3\n",
      "['mr', 'said', 'would', 'labour', 'election', 'blair', 'party', 'government', 'brown', 'people', 'minister', 'also', 'prime', 'told', 'howard', 'new', 'chancellor', 'tory', 'leader', 'could', 'plans', 'public', 'tax', 'one', 'general', 'tories', 'uk', 'campaign', 'say', 'bbc', 'tony', 'next', 'britain', 'secretary', 'get', 'lord', 'says', 'time', 'lib', 'political', 'two', 'last', 'make', 'michael', 'added', 'year', 'ukip', 'country', 'mps', 'may', 'issue', 'kennedy', 'liberal', 'world', 'first', 'made', 'parties', 'take', 'vote', 'think', 'home', 'kilroy-silk', 'way', 'local', 'spokesman', 'years', 'former', 'saying', 'house', 'conservative', 'us', 'going', 'part', 'back', 'bill', 'gordon', 'claims', 'mp', 'like', 'british', 'voters', 'want', 'council', 'week', 'iraq', 'expected', 'politics', 'immigration', 'war', 'eu', 'budget', 'commons', 'believe', 'good', 'conservatives', 'dems', 'asylum', 'see', 'london', 'issues', 'many', 'ministers', 'lords', 'spending', 'system', 'asked', 'police', 'go', 'change', 'economic', 'much', 'meeting', 'cabinet', 'news', 'work', 'right', 'charles', 'elections', 'programme', 'stand', 'even', 'conference', 'parliament', 'democrats', 'put', 'clear', 'still', 'already', 'services', 'taxes', 'cut', 'come', 'today', 'well', 'set', 'economy', 'whether', 'speech', 'members', 'policy', 'got', 'european', 'plan', 'office', 'foreign', 'education', 'dem', 'choice', 'politicians', 'away', 'radio', 'later', 'health', 'called', 'support', 'role', 'poll', 'report', 'law', 'end', 'milburn', 'give', 'trust', 'show', 'day', 'insisted', 'accused', 'campbell', 'clarke', 'manifesto', 'need', 'europe', 'use', 'countries', 'without', 'future', 'since', 'national', 'answer', 'number', 'debate', 'three', 'john', 'action', 'monday', 'africa', 'chief', 'key', 'schools', 'money', 'important', 'needed', 'power', 'speaking', 'ahead', 'big', 'place', 'proposals', 'east', 'parliamentary']\n",
      "4\n",
      "['said', 'game', 'england', 'first', 'last', 'world', 'one', 'two', 'win', 'would', 'time', 'back', 'also', 'players', 'new', 'play', 'ireland', 'year', 'good', 'side', 'cup', 'second', 'games', 'wales', 'six', 'set', 'team', 'match', 'three', 'could', 'final', 'made', 'coach', 'club', 'get', 'season', 'take', 'rugby', 'france', 'injury', 'well', 'like', 'added', 'think', 'going', 'us', 'minutes', 'open', 'got', 'chelsea', 'best', 'next', 'go', 'told', 'years', 'victory', 'player', 'nations', 'great', 'league', 'make', 'since', 'playing', 'williams', 'way', 'played', 'still', 'top', 'five', 'number', 'try', 'four', 'chance', 'really', 'come', 'champion', 'title', 'week', 'start', 'international', 'liverpool', 'want', 'right', 'ball', 'goal', 'know', 'lot', 'took', 'end', 'scotland', 'arsenal', 'manager', 'squad', 'home', 'robinson', 'put', 'third', 'bbc', 'another', 'united', 'former', 'football', 'mark', 'sport', 'saturday', 'grand', 'italy', 'winning', 'see', 'place', 'roddick', 'half', 'race', 'beat', 'left', 'points', 'return', 'european', 'much', 'break', 'came', 'real', 'sunday', 'away', 'lost', 'madrid', 'ahead', 'lead', 'record', 'champions', 'australian', 'even', 'fans', 'round', 'boss', 'captain', 'many', 'people', 'andy', 'never', 'part', 'jones', 'irish', 'tour', 'britain', 'olympic', 'despite', 'pressure', 'zealand', 'premiership', 'penalty', 'went', 'decision', 'form', 'seed', 'j', 'british', 'defeat', 'long', 'big', 'training', 'gerrard', 'weeks', 'summer', 'newcastle', 'referee', 'g', 'lions', 'early', 'need', 'forward', 'tournament', 'face', 'better', 'career', 'move', 'missed', 'slam', 'past', 'work', 'matches', 'run', 'women', 'days', 'fourth', 'give', 'championships', 'johnson', 'admitted', 'french', 'tennis', 'always', 'given', 'line', 'test', 'weekend', 'important', 'hard', 'shot', 'indoor']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobiles  not media players yet  mobiles are no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple attacked over sources row civil libertie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>junk e-mails on relentless rise spam traffic i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>broadband set to revolutionise tv bt is starti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disney backs sony dvd technology a next genera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tory candidate quits over remark a conservativ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tory leader  cleared  over work scottish conse...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>observers to monitor uk election ministers wil...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what really divides the parties so what is the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mp s shock at long lost daughter labour mp ste...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1777 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  cluster\n",
       "1   mobiles  not media players yet  mobiles are no...        1\n",
       "2   apple attacked over sources row civil libertie...        2\n",
       "2   junk e-mails on relentless rise spam traffic i...        2\n",
       "1   broadband set to revolutionise tv bt is starti...        1\n",
       "1   disney backs sony dvd technology a next genera...        1\n",
       "..                                                ...      ...\n",
       "3   tory candidate quits over remark a conservativ...        3\n",
       "3   tory leader  cleared  over work scottish conse...        3\n",
       "3   observers to monitor uk election ministers wil...        3\n",
       "3   what really divides the parties so what is the...        3\n",
       "3   mp s shock at long lost daughter labour mp ste...        3\n",
       "\n",
       "[1777 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_most_common_words_by_cluster(all_training, km, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6e85e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(km, open('bbc_kmeans.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e3faf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = pickle.load(open(\"bbc_kmeans.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41e2cf",
   "metadata": {},
   "source": [
    "# Using SVMs for supervised text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8d8d2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3, 6, -2, -5, 7, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a717714",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3480437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 5\n",
      "5 3\n",
      "5 5\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    initial = sequence[0]\n",
    "    block = 0\n",
    "    for i in range(1,len(sequence)):\n",
    "        print(initial, sequence[i])\n",
    "        if initial < sequence[i]:\n",
    "            initial = sequence[i]\n",
    "        elif i==1:\n",
    "            initial = sequence[i]\n",
    "            block += 1\n",
    "        else: \n",
    "            block += 1\n",
    "    print(block)\n",
    "    return True if block <=1 else False    \n",
    "almostIncreasingSequence([1, 2, 5, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572c833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37028d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 5, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "20953d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 3, 5]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6cda7f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 3, 5]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2-1]+a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "90c92cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1, 2, 2] [2, 1, 2]\n",
      "2\n",
      "[1, 1, 2] [1, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    #status = False\n",
    "    for i in range(1,len(sequence)+1):\n",
    "\n",
    "        temp = sequence[:]\n",
    "        temp.pop(i-1)\n",
    "        temp.sort()\n",
    "        print(i)\n",
    "        print(temp, sequence[:i-1]+sequence[i:])\n",
    "        \n",
    "        if temp == sequence[:i-1]+sequence[i:]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "almostIncreasingSequence([1, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f06c4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, -1]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a - b for a,b in zip([1, 2, 1],[2, 1, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fccf86f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 2], [1, 2])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[1:], sequence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b08b6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    new_s = [ a - b > 0 for a,b in zip(sequence[1:], sequence[:-1])]\n",
    "    print([ a - b for a,b in zip(sequence[1:], sequence[:-1])])\n",
    "    print(new_s)\n",
    "    return  False if new_s.count(False) > 1 else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9ffdba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    start = 0\n",
    "    end = 1\n",
    "    status = 0\n",
    "    block_index = []\n",
    "    while end <= len(sequence)-1:\n",
    "        \n",
    "        print(sequence[start] , sequence[end])\n",
    "        if sequence[start] >= sequence[end] and status <= 1 :\n",
    "            status += 1\n",
    "            block_index.append(start)\n",
    "            if start !=0:\n",
    "                start -= 1\n",
    "            else:\n",
    "                start += 1\n",
    "                end += 1\n",
    "            \n",
    "            \n",
    "                   \n",
    "        else:\n",
    "            \n",
    "            start += 1\n",
    "            if start in block_index:\n",
    "                start +=1\n",
    "            end += 1\n",
    "    print(status)\n",
    "    return True if status <= 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "83035440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 3\n",
      "3 3\n",
      "2 3\n",
      "4 6\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [1, 2, 3, 4, 3, 6]\n",
    "almostIncreasingSequence(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b887d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    temp = sequence[0]\n",
    "    status = 0\n",
    "    for i in range(1,len(sequence)):\n",
    "        print(temp , sequence[i])\n",
    "        if temp >= sequence[i] and status <= 1 :\n",
    "            status += 1\n",
    "            if i ==1:\n",
    "                temp = sequence[1]\n",
    "            else:\n",
    "                temp = sequence[i-2]\n",
    "                \n",
    "        else:\n",
    "            temp = sequence[i]\n",
    "    print(status)\n",
    "    return True if status <= 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f091ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 5\n",
      "5 3\n",
      "2 5\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [1, 2, 5, 3, 5]\n",
    "almostIncreasingSequence(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8b1a1a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.count(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f9a3c924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i = 1\n",
    "sequence[:i-1] + sequence[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd691ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b28652cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "sequence = [1, 3, 2, 4]\n",
    "print(sequence[:2-1])\n",
    "print(sequence[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a5a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    initial = sequence[0]\n",
    "    block = 0\n",
    "    for i in range(1,len(sequence)):\n",
    "        print(initial, sequence[i])\n",
    "        if initial < sequence[i]:\n",
    "            initial = sequence[i]\n",
    "        elif i==1:\n",
    "            initial = sequence[i]\n",
    "            block += 1\n",
    "        else: \n",
    "            block += 1\n",
    "    print(block)\n",
    "    return True if block <=1 else False    \n",
    "almostIncreasingSequence([1, 2, 5, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9b20a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2]\n",
      "***\n",
      "1 2\n",
      "2 1\n",
      "[1, 1, 2]\n",
      "***\n",
      "1 1\n",
      "2 1\n",
      "[1, 2, 2]\n",
      "***\n",
      "2 1\n",
      "2 2\n",
      "[1, 2, 1]\n",
      "***\n",
      "2 1\n",
      "1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def almostIncreasingSequence(sequence):\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        status = True\n",
    "        #print(i)\n",
    "        temp = sequence[:]\n",
    "        temp.pop(i)\n",
    "        \n",
    "        item = temp[0]\n",
    "        print(temp)\n",
    "        print('***')\n",
    "        for j in temp[1:]:\n",
    "            print(j, item)\n",
    "            if j > item:\n",
    "                item = j\n",
    "                continue\n",
    "            else:\n",
    "                status = False\n",
    "                item = j\n",
    "        if status:\n",
    "            return True\n",
    "            \n",
    "        \n",
    "    \n",
    "    return False\n",
    "            \n",
    "            \n",
    "almostIncreasingSequence([1, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec1c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
