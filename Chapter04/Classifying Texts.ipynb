{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c19b12",
   "metadata": {},
   "source": [
    "# Getting the dataset and evaluation baseline ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c72b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15745fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from Chapter01.tokenization import tokenize_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7831067",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "bbc_dataset = 'bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134264ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_token = [t for t in tokens if t not in string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_token]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12cf4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f88574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwrods(stop_words):\n",
    "    stemmed_stopwords = [stemmer.stem(word) for word in stop_words]\n",
    "    stop_words = stop_words + stemmed_stopwords\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89146fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwrods(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7fc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',', \n",
    "                            quotechar='\"')\n",
    "        data_read = [row for row in reader]\n",
    "        return data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a08d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = read_in_csv(filename)\n",
    "    data_dict = {}\n",
    "    for row in data[1:]:\n",
    "        category = row[0]\n",
    "        text = row[1]\n",
    "        if category not in data_dict.keys():\n",
    "            data_dict[category] = []\n",
    "        data_dict[category].append(text)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f0c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(text, num_words=200):\n",
    "    word_list = tokenize_nltk(text)\n",
    "    word_list = [word for word in word_list if word not in stop_words and \n",
    "                re.search(\"[A-Za-z]\", word)]\n",
    "    freq_list = FreqDist(word_list)\n",
    "    print(freq_list.most_common(num_words))\n",
    "    return freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14542f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eabaed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech : 401\n",
      "business : 510\n",
      "sport : 511\n",
      "entertainment : 386\n",
      "politics : 417\n"
     ]
    }
   ],
   "source": [
    "for topic in data_dict.keys():\n",
    "    print(topic, ':', len(data_dict[topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c722a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = data_dict[\"business\"]\n",
    "sports_data = data_dict[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db91b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_string = ' '.join(business_data)\n",
    "sports_string = ' '.join(sports_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b23178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 1680), ('us', 813), ('year', 637), ('mr', 600), ('would', 463), ('also', 440), ('market', 425), ('new', 416), ('company', 415), ('growth', 384), ('last', 365), ('firm', 362), ('economy', 359), ('government', 340), ('bank', 335), ('sales', 316), ('could', 311), ('economic', 310), ('oil', 294), ('shares', 265), ('however', 256), ('world', 252), ('may', 251), ('years', 247), ('prices', 246), ('one', 243), ('chief', 236), ('two', 231), ('china', 223), ('business', 218), ('companies', 212), ('analysts', 209), ('uk', 207), ('deal', 206), ('rise', 203), ('expected', 200), ('group', 199), ('financial', 197), ('yukos', 196), ('firms', 193), ('since', 183), ('dollar', 180), ('december', 173), ('country', 173), ('months', 170), ('people', 170), ('stock', 168), ('first', 165), ('president', 165), ('three', 164), ('still', 164), ('many', 163), ('time', 159), ('european', 159), ('rate', 159), ('state', 158), ('trade', 158), ('told', 155), ('investment', 153), ('demand', 151), ('interest', 151), ('india', 151), ('quarter', 149), ('figures', 149), ('profits', 148), ('rates', 148), ('made', 147), ('countries', 146), ('spending', 146), ('executive', 145), ('news', 143), ('biggest', 142), ('month', 141), ('strong', 139), ('price', 139), ('jobs', 137), ('europe', 136), ('next', 136), ('added', 135), ('foreign', 134), ('tax', 132), ('much', 132), ('back', 131), ('rose', 131), ('offer', 130), ('euros', 130), ('budget', 129), ('according', 129), ('bid', 128), ('costs', 127), ('high', 127), ('set', 126), ('money', 126), ('exchange', 125), ('recent', 123), ('january', 122), ('investors', 122), ('part', 122), ('increase', 121), ('industry', 120), ('share', 120), ('cut', 118), ('fall', 117), ('make', 116), ('million', 116), ('hit', 116), ('week', 115), ('well', 112), ('london', 112), ('russian', 111), ('move', 110), ('japan', 110), ('take', 110), ('court', 109), ('deutsche', 109), ('former', 108), ('higher', 108), ('debt', 108), ('report', 108), ('says', 107), ('united', 106), ('production', 106), ('likely', 106), ('pay', 105), ('fell', 103), ('reported', 102), ('annual', 101), ('deficit', 101), ('minister', 100), ('say', 100), ('consumer', 99), ('russia', 99), ('despite', 98), ('sale', 98), ('earlier', 97), ('global', 95), ('bankruptcy', 95), ('exports', 95), ('markets', 95), ('eu', 95), ('south', 94), ('international', 94), ('number', 93), ('continue', 92), ('club', 92), ('shareholders', 91), ('cost', 90), ('plans', 90), ('record', 90), ('euro', 89), ('seen', 89), ('less', 89), ('public', 89), ('main', 89), ('giant', 88), ('november', 88), ('profit', 88), ('unit', 87), ('trading', 86), ('future', 86), ('end', 86), ('case', 86), ('largest', 85), ('car', 85), ('fraud', 84), ('statement', 84), ('boost', 84), ('need', 83), ('put', 83), ('sector', 83), ('work', 83), ('already', 82), ('lost', 82), ('agreed', 82), ('meeting', 82), ('takeover', 81), ('german', 81), ('buy', 81), ('gm', 80), ('finance', 80), ('low', 79), ('stake', 79), ('lse', 79), ('decision', 78), ('previous', 78), ('domestic', 78), ('value', 78), ('banks', 78), ('warned', 77), ('see', 77), ('including', 77), ('came', 77), ('saying', 77), ('current', 76), ('total', 76), ('board', 76), ('house', 75), ('national', 75), ('help', 75), ('airline', 74)]\n",
      "[('said', 941), ('game', 476), ('england', 459), ('first', 437), ('win', 415), ('would', 396), ('world', 379), ('last', 376), ('one', 355), ('two', 351), ('also', 329), ('time', 327), ('back', 318), ('players', 307), ('play', 292), ('cup', 290), ('new', 285), ('side', 270), ('ireland', 270), ('year', 267), ('team', 265), ('wales', 265), ('good', 258), ('club', 254), ('second', 248), ('six', 246), ('match', 245), ('could', 241), ('three', 230), ('set', 228), ('final', 228), ('coach', 228), ('france', 227), ('season', 223), ('made', 214), ('us', 212), ('get', 212), ('rugby', 210), ('injury', 208), ('think', 204), ('take', 201), ('chelsea', 201), ('added', 200), ('well', 198), ('great', 191), ('going', 187), ('go', 182), ('open', 181), ('victory', 180), ('got', 180), ('best', 178), ('years', 177), ('like', 176), ('next', 174), ('told', 174), ('league', 172), ('games', 171), ('nations', 171), ('make', 168), ('player', 167), ('united', 165), ('minutes', 165), ('way', 163), ('played', 161), ('since', 160), ('start', 160), ('still', 157), ('champion', 157), ('international', 156), ('arsenal', 154), ('scotland', 152), ('playing', 151), ('williams', 151), ('liverpool', 150), ('four', 147), ('want', 142), ('chance', 141), ('come', 140), ('lot', 139), ('home', 139), ('olympic', 139), ('right', 136), ('title', 136), ('five', 134), ('week', 134), ('manager', 134), ('try', 134), ('really', 133), ('squad', 133), ('former', 132), ('know', 132), ('ball', 132), ('end', 131), ('sport', 131), ('beat', 130), ('number', 129), ('top', 129), ('football', 129), ('italy', 127), ('another', 126), ('third', 125), ('race', 125), ('put', 124), ('goal', 124), ('saturday', 122), ('winning', 122), ('european', 122), ('robinson', 121), ('roddick', 120), ('see', 119), ('took', 117), ('jones', 117), ('came', 115), ('v', 114), ('mark', 114), ('grand', 114), ('bbc', 113), ('place', 112), ('return', 110), ('away', 109), ('lost', 108), ('record', 108), ('half', 108), ('champions', 107), ('boss', 106), ('left', 106), ('much', 105), ('captain', 102), ('sunday', 102), ('decision', 102), ('ahead', 101), ('athens', 100), ('andy', 99), ('better', 99), ('break', 99), ('real', 99), ('points', 99), ('australian', 99), ('face', 98), ('round', 96), ('lead', 95), ('madrid', 95), ('even', 94), ('never', 93), ('britain', 91), ('big', 90), ('premiership', 90), ('test', 90), ('newcastle', 89), ('forward', 89), ('run', 89), ('zealand', 89), ('training', 88), ('seed', 87), ('despite', 87), ('early', 87), ('people', 86), ('irish', 86), ('j', 86), ('manchester', 85), ('british', 85), ('indoor', 85), ('defeat', 84), ('championships', 84), ('long', 84), ('given', 83), ('summer', 83), ('move', 82), ('form', 82), ('gerrard', 82), ('give', 81), ('went', 81), ('pressure', 81), ('women', 81), ('slam', 81), ('drugs', 81), ('penalty', 80), ('part', 79), ('career', 79), ('hard', 79), ('days', 78), ('men', 78), ('weeks', 77), ('matches', 77), ('bit', 77), ('referee', 77), ('lions', 77), ('g', 77), ('spain', 76), ('says', 76), ('johnson', 76), ('missed', 75), ('french', 75), ('country', 75), ('fourth', 74), ('admitted', 74), ('american', 74), ('centre', 73), ('day', 73), ('work', 73)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 941, 'game': 476, 'england': 459, 'first': 437, 'win': 415, 'would': 396, 'world': 379, 'last': 376, 'one': 355, 'two': 351, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats(business_string)\n",
    "get_stats(sports_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adec4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text_list):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=20000,\n",
    "                                      min_df=0.05, stop_words='english',\n",
    "                                      use_idf=True,tokenizer=tokenize_and_stem,\n",
    "                                      ngram_range=(1, 3))\n",
    "    tfidf_vectorizer.fit_transform(text_list)\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4987c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, train_percentage):\n",
    "    train_test_boarder = math.ceil(train_percentage * len(data))\n",
    "    train_data = data[0:train_test_boarder]\n",
    "    test_data = data[train_test_boarder:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(names)\n",
    "    return le\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23718dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_train_data, business_test_data = split_train_test(business_data, 0.8)\n",
    "sports_train_data, sports_test_data = split_train_test(sports_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc005ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bat/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "train_data = business_train_data + sports_train_data\n",
    "tfidf_vec = create_vectorizer(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e43c6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels(['business', 'sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12bead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_matrix(input_data, vectorizer, label, le):\n",
    "    vectors = vectorizer.transform(input_data).todense()\n",
    "    labels = [label] * len(input_data)\n",
    "    enc_labels = le.transform(labels)\n",
    "    return vectors, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47377591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(vectorizer, data_dict, le):\n",
    "    business_news = data_dict['business']\n",
    "    sports_news = data_dict['sports']\n",
    "    \n",
    "    business_vector, business_label = create_data_matrix(business_news, vectorizer,\n",
    "                                                        'business', le)\n",
    "    sports_vector, sports_label = create_data_matrix(sports_news, vectorizer,\n",
    "                                                        'sports', le)\n",
    "    \n",
    "    all_data_matrix = np.vstack((business_vector, sports_vector))\n",
    "    labels = np.concatenate([business_label, sports_label])\n",
    "    return all_data_matrix, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0f36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'business':business_train_data, \n",
    "                   'sports':sports_train_data}\n",
    "test_data_dict = {'business':business_test_data, \n",
    "                  'sports':sports_test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train) = create_dataset(tfidf_vec, train_data_dict, le)\n",
    "(X_test, y_test) = create_dataset(tfidf_vec, test_data_dict, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d6647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trivial(X_train, y_train, X_test, y_test, le):\n",
    "    dummy_clf = DummyClassifier(strategy='uniform', random_state=0)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "    print(dummy_clf.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                labels=le.transform(le.classes_),\n",
    "                               target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12684310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44607843137254904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.45      0.44      0.44       102\n",
      "      sports       0.45      0.45      0.45       102\n",
      "\n",
      "    accuracy                           0.45       204\n",
      "   macro avg       0.45      0.45      0.45       204\n",
      "weighted avg       0.45      0.45      0.45       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_trivial(X_train, y_train, X_test, y_test, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79383260",
   "metadata": {},
   "source": [
    "# Performing rule-based text classification using keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a0cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
